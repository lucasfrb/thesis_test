<INDIVIDUAL>
  <ID>subject4005</ID>
  <WRITING>
    <TITLE>We #8217;re doubling down on our efforts to live decentralised! Read how and how you can contribute.</TITLE>
    <DATE>2018-11-14 11:13:52</DATE>
    <TEXT> #8203; https://preview.redd.it/p0undgj0vby11.png?width=2000 amp;format=png amp;auto=webp amp;s=8c1c3cca8bc3fd2d4d1cc98cb97878b25cbfa192 These are the steps we have taken, and will take, to create a greater opensource culture, and collaborate with other projects. See how you can get involved with our new community fund. Tell us what you think! [https://medium.com/streamrblog/how-were-doubling-our-efforts-to-live-decentralized-a61245eed191](https://medium.com/streamrblog/how-were-doubling-our-efforts-to-live-decentralized-a61245eed191)  #8203; \---  #8203; Back at the start of the Autumn, Henri, myself and a few others in the Streamr team received an email from one of our most trusted advisors, the enigmatic Mr X (he likes his privacy). He was in the shower, he said, when it struck him what had been bothering him about the project.  #8203;  #8220;Stop acting like a company, #8221; he wrote.  #8220;Companies are lame, obsolete concepts  #8230; We #8217;ve all seen this movie before and all it produced was a world where everyone has less autonomy and freedom. #8221;  #8203; Mr X is known for being blunt. That #8217;s what makes him a great advisor. Not just for Streamr but for many an excellent project out there. In a series of emails, he went on to detail practical steps which he thought we should be taking to shift direction.  #8203; Around the same time, Gnosis founder Martin K #246;ppelmann published this #8202; #8212; #8202;an outline for a Web3 first strategy #8202; #8212; #8202;which I and a few other members of the project spotted and started passing around to the rest of the team (ironically on our internal Slack).  #8203; What subsequently erupted amongst the core Streamr team was a detailed, passionate and lengthy debate over the project #8217;s identity; who we are in terms of mission and structures, what we #8217;ve been doing and how that might fit with our end aims, and whether our actions around partnerships and communications also fit with our ideals and goals.  #8203; Was Mr X right? Has Streamr been too eager to delve into the enterprise field? Have we adopted too much of a startup culture as opposed to an open source decentralized one? How could we better utilise other decentralized applications in the everyday life of the project, and extend a technical hand to our brothers and sisters in the rest of the space?  #8203; Of course, decentralized means a number of things to a number of people. There #8217;s no need to rehash those discussions here but it #8217;s plenty clear that the term is more than just a descriptor of the technical architecture of nodes, users, incentives etc. At its core, the term also encompasses normative stipulations; beliefs around how humanity should and can act.  #8203; Many of these other aspects of decentralization were discussed with passion in multiple sessions during the Web3 Summit and Devcon4. Having attended a number of those, one useful summation might be this, that we Ethereans (h/t Lane Rettig) are building tools to help humanity rebalance and reshape the often competing interests of individual liberty and communal cooperation.  #8203; If high-libertarianism seems to leave no room for common endeavour and socialism #8217;s centralised methods are inherently threatening to the individual, then Ethereans are trying to build a new, technologically backed, Cooperativism.  #8203; [Can Blockchains Solve Ten Years of Standardization Failure? by Harry Halpin at Web3 Summit 2018](https://www.youtube.com/watch?list=PLxVihxZC42nHlc7J-UqeS-asagbehBeh3 amp;v=6PPv3W0InUM)  #8203;  #8203; |||| |:-|:-|:-| ||||  #8203; Streamr #8217;s mission to build a p2p pub/sub Network for realtime data exchange fits directly into that definition. People and organisations should be sovereign over their data. And yet information sharing has been a crucial building block of modernity. So how do we share data for common benefit without giving up our property and personal information to someone else to manage and ultimately control? We believe our decentralized Network will help reconcile those interests. It allows entities to share their data for common good, whilst retaining all rights to their property [including financial benefits](https://marketplace.streamr.com/).  #8203; However that #8217;s not at all what Mr X was getting at. It is not enough that Streamr simply build decentralized tools. A centralized multinational listed on a stock exchange like say Microsoft could do that. We must also live our ideals. We must live decentralized as far as is possible. So how do we do that?  #8203; The Platonic ideal might have been this: once crowdfunded, the vision contained in the Streamr whitepaper would be executed by a DAO. (According to Gavin Wood, [the Ethereum Foundation once intended](https://youtu.be/eO3fG_1YrE4?t=2946) to do just this). But such governance structures aren #8217;t yet practicable. So in the very least, full time contributors to Streamr should be seeking at every stage to empower those around them who want to develop Streamr, its vision, and the wider decentralized ecosystem on which Streamr and others will rely.  #8203; Following that internal debate, we emerged with a series of tangible actions. If we #8217;re brutally honest, none is wildly radical for the space but the list represents a commitment to a direction. And in fact since that list was devised, we took a conscious choice to wait a month or so to prove to the Streamr and wider Ethereum/ decentralizing community that we can cross off a few of these action items.  #8203; [Crypto powered Kickback #8202; #8212; #8202;use it instead of eventbrite. It #8217;s pretty #160;awesome.](https://preview.redd.it/c62069fnxby11.png?width=800 amp;format=png amp;auto=webp amp;s=2d57a7eb6dc8940154fcd0f5266d132f5b14dab7)  #8203; **Let #8217;s get practical**  #8203; So here #8217;s what we #8217;ve done so far: 1. [We now Peep](https://peepeth.com/streamr)! Peepth is the decentralized version of Twitter. There aren #8217;t many Streamr community users on Peepth as yet but as this picks up (and when we sort out our MetaMask permissions) we will shift to a Peepth first strategy, meaning we #8217;ll Peep before we Tweet. 2. Staying with social media, we changed our Twitter handle. The old one @Streamrinc was way too corporate sounding. There was an option to go for @StreamrNetwork but this was pretty cumbersome (@StreamrNet sounded no better). Instead we reached out to Todd Stabley, s[enior media engineer at Duke University](https://www.linkedin.com/in/toddstabley), and the owner of the handle @streamr. Todd [turned out to be a real lovely guy](https://twitter.com/streamr/status/1049345078282608640) and was happy to gift us the handle as he hadn #8217;t used it for several years. May the gods of karma shine upon you Todd. 3. We #8217;ve [started using Kickback](https://kickback.events/event/0xf6912d7ab160aa1645206f6725a0f57840839799) for events. The platform is pretty awesome in ensuring attendance at free events. Event participants stake ETH for entrance, and if they don #8217;t turn up their stake is handed back as a kickback (gedit!) to those who did attend. Attendees basically make a profit off of no shows. Thank you to Golem #8217;s community manager and all round hero [MP](https://twitter.com/MPtherealMVP), for organising our [Cryptoween party in Prague](https://kickback.events/event/0xf6912d7ab160aa1645206f6725a0f57840839799) through the platform with Kickback founder [Makoto Inoue](https://twitter.com/@makoto_inoue). 4. Getting more involved in community hackathons. One of our proudest moments in the later half of this year was participating in[ Mexico](https://medium.com/streamrblog/streamrs-hardcore-hackathon-in-mexico-use-cases-to-fend-off-bus-robbers-and-feed-meat-eating-31ed87bb913e) in September. But during these last few weeks, we also got involved in the Web3 Summit in Berlin and the Status hackathon in Prague. We #8217;re ramping up this effort both in participating and educating others and ourselves. Here #8217;s what we #8217;re doing today: This is something small (more ambitious announcements below) but it helps kickstart our first official bounty programme. We #8217;re looking to incentivise biz dev introductions to our Marketplace and so if you are interested in learning more, and helping to increase the numbers of Marketplace buyers and sellers, contact us at [marketplace@streamr.com](mailto:marketplace@streamr.com).  #8203; [It worked! A packed Cryptoween party due to #160;Kickback](https://preview.redd.it/uu4785svxby11.jpg?width=1067 amp;format=pjpg amp;auto=webp amp;s=105b1c7ebf9a86a5305a7c9446abcee094c4f15f) This what we definitely plan to do in the coming months: * Change our url to streamr.network. Streamr.com is a layover from pre-crowdfunded Streamr as it was a few years ago. It #8217;s now time to change it and signal what we #8217;re really about. * We #8217;re also currently updating the website so it can better relay to people that Streamr is an open-source, network-oriented, decentralization project. * We will also open up our design materials on the new website to give the community more resources. * Alongside this we plan to ensure that our merchandise is much easier to obtain (currently you can only get it through us) by uploading our designs on Teespring. We recently had [a community competition to come up with inventive new merchandise](https://www.reddit.com/r/streamr/comments/9fkwp8/think_up_some_decent_community_goodies_and_win_5k/) and received some excellent and inventive entries. We still need to act on that and we will. * We will contribute to space-wide research projects. In fact we #8217;re already doing a little of this for Plasma (more soon). Devcon4 was a real inspiration for those working on the Network. So where possible, we will always try and cooperate within the space. And of course this follows previous promises to disseminate our learning and research. *  #8203; #8203;The partnerships team will do more to identify crypto partners to team up with for decentralized pilots. * The Dev team will ensure that users will soon be able to do away with accounts and emails in the sign up process on the platform and just use an Ethereum address as their form of ID. * Also where useful, the team will do more to integrate User flow into other decentralized platforms such as Bancor. * Add editor features to help with building Oracles for Smart Contracts * Open source the Marketplace. Okay hands up #8202; #8212; #8202;we #8217;ve snuck this one in because we already promised to do it before. But Henri and the dev team haven #8217;t forgotten. And overall, we #8217;ll try and be less self-critical about our work-in-progress and open source brand new things earlier on. Ideally everything, even highly exploratory bits and pieces doomed to fail, would be open source from the moment the repository is created. #8221; * Organise further community meetups. * Look into[ Akasha](https://akasha.world/) and[ Ghost](https://ghost.org/) for blog posting platform to replace Medium and will wait for live streaming solutions to emerge from Livepeer and other projects. * We #8217;ve given out a few rewards for technical/developer tasks before but we will commit to setting up a formal bounty system for implementing technical tasks. [Imbibing decentralization \(and obligatory doge memes\) as a way of life at #160;Devcon4](https://preview.redd.it/uzt51bq1yby11.jpg?width=1334 amp;format=pjpg amp;auto=webp amp;s=a451d37cd2a986cc9b5db4d79885a9b6e0e8dac6)  #8203; **Beyond bounties: A properly funded Community** Now as you #8217;ll notice, this is a list that the roughly 35 or so full time contributors to Streamr have promised to implement. However we are only the custodians of this technology. There are others out there who have also contributed to the development of the project. And it #8217;s time that fact was recognised with something substantial. So perhaps most importantly of all, and mirroring the steps taken by many other projects, we are going to set up a community fund backed up by a significant amount of DATA (the upper bounds of six figures in DATA). However unlike most other projects, this fund will be directly run by community. After discussions with a number of leading community members, we have begun etching out the rough structure for the fund in order to make its administration open, fair and effective. In that regard, we #8217;d like the community fund members to be elected by the wider community. And although the specific purposes of the fund will largely be left to the community, to ensure it isn #8217;t subverted, the fund panel will likely have one full-time Streamr contributor on it with the power of veto. And of course we hope all of this management can take place using blockchain governance as much as possible. So this is a fairly long list of actions but there #8217;s something more we #8217;d like you to do. Firstly, please do give us your ideas. If you #8217;re reading this in Medium then head to our subreddit where this is also posted, and comment there. The second action everyone in the community should take part in is to hold us to account! We might get sloppy with some of this or let it fade. Your job is to ensure we stick to our direction or at least explain why certain actions above weren #8217;t taken. Thanks for reading!  #8203; By Shiv Malik  #8203; Tell us what you think and please ask any questions you might have!  #8203;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Our community hero for November is ...</TITLE>
    <DATE>2018-11-15 12:18:32</DATE>
    <TEXT>Mark Milton, who proudly modeled our limited edition Streamr tees in Prague during cryptolife hackathon and Devcon4!  #127881;. Thank you Mark! https://preview.redd.it/62h38c6clhy11.png?width=1012 amp;format=png amp;auto=webp amp;s=d4fb1e7c0936dba8b339b0bd2cd18156812d934e</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Hear the latest Streamr news on Peepeth before any centralised platforms</TITLE>
    <DATE>2018-11-15 15:30:59</DATE>
    <TEXT>In line with our doubling commitment to live the decentralized life outlined in our latest blog post, we're making all new announcements on Peepeth before Twitter (and other centralised platforms where appropriate).  #8203; Join us there! [https://peepeth.com/streamr](https://peepeth.com/streamr)  #8203; Yesterday's blog doubling down on our commitment to live decentralised. [https://medium.com/streamrblog/how-were-doubling-our-efforts-to-live-decentralized-a61245eed191](https://medium.com/streamrblog/how-were-doubling-our-efforts-to-live-decentralized-a61245eed191)  #8203; https://preview.redd.it/uxqhle2vjiy11.png?width=2000 amp;format=png amp;auto=webp amp;s=cc44db3c95195b0ae70916a249f3cc8a5710d380  #8203;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128225; Streamr Live Webinar Ahead of the IATA Hackathon - 27th November</TITLE>
    <DATE>2018-11-16 11:50:26</DATE>
    <TEXT>We #8217;re hosting a live webinar to introduce Streamr #8217;s platform and API usage to attendees and airline members on the 27th November, two days before the IATA hackathon.  #8203; Read more and save the date! [https://medium.com/streamrblog/streamr-invited-to-contribute-in-iata-hackathon-116a90dccd96](https://medium.com/streamrblog/streamr-invited-to-contribute-in-iata-hackathon-116a90dccd96) https://preview.redd.it/grkp9nueloy11.png?width=743 amp;format=png amp;auto=webp amp;s=71a57a5183523a834a2fa7558e5f76b156ccc69d  #8203;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;re doubling down on our efforts to live decentralised! Read how and how you can contribute.</TITLE>
    <DATE>2018-11-16 13:32:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;ve Updated the #Streamr Public Trello</TITLE>
    <DATE>2018-11-19 13:34:06</DATE>
    <TEXT>You can find all the development progress we #8217;ve made in the last few weeks here. [https://trello.com/b/j24hxvjg/streamr-milestone-1](https://trello.com/b/j24hxvjg/streamr-milestone-1)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The 20th Anniversary of Open Source. What #8217;s in it For Me/Them/Us?</TITLE>
    <DATE>2018-11-20 11:31:13</DATE>
    <TEXT>It #8217;s been 20 years since the term "open source" was first heard. Streamr's Wei and Mikhael share their thoughts on the advantages it brings and where the movement is heading. [https://medium.com/streamrblog/20-years-of-open-source-what-s-in-it-for-me-them-us-9387daf6cbe2](https://medium.com/streamrblog/20-years-of-open-source-what-s-in-it-for-me-them-us-9387daf6cbe2)  #8203;  #8203; https://preview.redd.it/3h5narvi1hz11.jpg?width=2000 amp;format=pjpg amp;auto=webp amp;s=fe105a3d0400bd5f0fbbe806d38cbf21b2f7bb36  #8203;  #8203;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The 20th Anniversary of Open Source. What #8217;s in it For Me/Them/Us?</TITLE>
    <DATE>2018-11-20 11:42:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The 20th Anniversary of Open Source. What #8217;s in it For Me/Them/Us?</TITLE>
    <DATE>2018-11-20 15:06:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The 20th Anniversary of Open Source. What #8217;s in it For Me/Them/Us?</TITLE>
    <DATE>2018-11-20 15:47:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;ve been invited to join the IATA hackathon in Lapland on the 29th November. Streamr will serve as the event #8217;s sole blockchain solution provider.</TITLE>
    <DATE>2018-11-21 14:16:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Github Outline on how to Replicate the Cold Chain Monitoring Demo Before our live Webinar for the IATA Hackathon</TITLE>
    <DATE>2018-11-23 13:01:18</DATE>
    <TEXT>We #8217;ve outlined on Github how you can replicate the Cold Chain Monitoring demo we are demonstrating during our live webinar on the 27th Nov, 14:00 CET, before the IATA hackathon. Read how this use case is relevant to the aviation/transport industry. [https://github.com/streamr-dev/iata-hackathon-2018/blob/master/README.md](https://github.com/streamr-dev/iata-hackathon-2018/blob/master/README.md)  #8203; https://preview.redd.it/6m2bn7s7w2021.png?width=743 amp;format=png amp;auto=webp amp;s=9d3a6587fc57704b7608e100982c3532090e95a1</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Github Outline on how to Replicate the Cold Chain Monitoring Demo Before our live Webinar for the IATA Hackathon</TITLE>
    <DATE>2018-11-23 16:13:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is LIVE - Learn about the Platform and See Our Cold Chain Monitoring Demo Ahead of the IATA Hackathon</TITLE>
    <DATE>2018-11-27 12:52:59</DATE>
    <TEXT>We have a live webinar today from 14:00 CET to 15:30 CET to present the Streamr platform and Cold Chain Monitoring demo to hackathon attendees.  #8203; You can join using the link below: [https://zoom.us/j/125096940](https://zoom.us/j/125096940)  #8203; More info on the webinar and demo: [https://github.com/streamr-dev/iata-hackathon-2018](https://github.com/streamr-dev/iata-hackathon-2018)  #8203; More info in the hackathon: [https://medium.com/streamrblog/streamr-invited-to-contribute-in-iata-hackathon-116a90dccd96](https://medium.com/streamrblog/streamr-invited-to-contribute-in-iata-hackathon-116a90dccd96)  #8203; Recording to follow :)  #8203; https://preview.redd.it/s1k16bhxev021.png?width=2000 amp;format=png amp;auto=webp amp;s=d75d92a83ea9d7f914969cdcab7b5e4233c9beae</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr Marketplace code is now fully open source!</TITLE>
    <DATE>2018-11-28 15:20:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr Marketplace code is now fully open source!</TITLE>
    <DATE>2018-11-28 15:22:32</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>STREAMR Webinar: Cold chain monitoring</TITLE>
    <DATE>2018-11-30 09:30:50</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128736;  #128187; Streamr is in #Lapland for today's #IATAhackathon. Here's all you need to know.</TITLE>
    <DATE>2018-11-30 11:36:40</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-03 16:17:23</DATE>
    <TEXT>Join us in London as we recap the year and look ahead to 2019. Henri Pihkala will host a live streamed #StreamrAMA so please send us your questions. [Register here](https://kickback.events/event/0x7fd21553aa5948cc5e416b0bdb32d7eda95f3120) with Kickback! [Send us your questions](https://np.www.reddit.com/r/streamr/comments/a2qaj6/streamrama_live_send_us_your_questions_for_henri/) for Henri and we'll add them to the AMA.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>#StreamrAMA LIVE - Send us your Questions for Henri - 18th December 18:30 GMT</TITLE>
    <DATE>2018-12-03 16:36:32</DATE>
    <TEXT>https://preview.redd.it/imnhsv43c3221.png?width=1920 amp;format=png amp;auto=webp amp;s=a9277249a824d71c3095a6488e121208a719ee9e Henri Pihkala will host a live streamed #StreamrAMA at our London meetup on the 18th December 18:30 GMT. Please send us your questions here for him here. More info on the event and register here with Kickback! [https://kickback.events/event/0x7fd21553aa5948cc5e416b0bdb32d7eda95f3120](https://kickback.events/event/0x7fd21553aa5948cc5e416b0bdb32d7eda95f3120)  #8203;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #9992; #65039; Round up of IATA #8217;s #AirHack Lapland Blockchain Hackathon - News From Streamr</TITLE>
    <DATE>2018-12-05 10:34:27</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2018-12-05 10:45:10</DATE>
    <TEXT>Thanks! All the latest info on the marketplace is here for now [https://medium.com/streamrblog/market-makers-a-retrospective-on-streamrs-realtime-data-marketplace-e1519d23e293](https://medium.com/streamrblog/market-makers-a-retrospective-on-streamrs-realtime-data-marketplace-e1519d23e293). We're having a meetup and live AMA with Henri on the 18th in London. I'm sure they'll be an update on the marketplace then. You can share any questions here and we'll include them in the event. [https://www.reddit.com/r/streamr/comments/a2qaj6/streamrama\_live\_send\_us\_your\_questions\_for\_henri/](https://www.reddit.com/r/streamr/comments/a2qaj6/streamrama_live_send_us_your_questions_for_henri/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-05 10:48:56</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>This year we grew the Streamr team from 10 to over 35. And this November we made four new hires. Meet our latest additions!</TITLE>
    <DATE>2018-12-05 15:28:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>#StreamrAMA LIVE - Send us your Questions for Henri - 18th December 18:30 GMT</TITLE>
    <DATE>2018-12-06 10:33:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-06 13:38:00</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>#StreamrAMA LIVE - Send us your Questions for Henri - 18th December 18:30 GMT</TITLE>
    <DATE>2018-12-06 14:05:23</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Three new integrations into the Streamr Marketplace. Atmotrack, Cryptoquote,  amp; Tradermade are using our #realtimedata platform!</TITLE>
    <DATE>2018-12-06 17:01:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Demonstration of the new Nantes Pollution Data [ + guide on filtering for particular data points]</TITLE>
    <DATE>2018-12-07 12:44:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>#StreamrAMA LIVE - Send us your Questions for Henri - 18th December 18:30 GMT</TITLE>
    <DATE>2018-12-07 16:04:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128085; Streamr merchandise is now available on Teespring! We'll have hoodies  amp; other swag to give away at our LDN meetup on the 18th.</TITLE>
    <DATE>2018-12-10 15:13:26</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-10 15:15:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to deploy smartcontracts on the Ethereum blockchain  amp; use Streamr to manage real time data streams  amp; create dapps</TITLE>
    <DATE>2018-12-11 12:10:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to deploy smartcontracts on the Ethereum blockchain  amp; use Streamr to manage real time data streams  amp; create dapps</TITLE>
    <DATE>2018-12-11 12:11:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-11 13:31:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-11 14:05:14</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to deploy smartcontracts on the Ethereum blockchain  amp; use Streamr to manage real time data streams  amp; create dapps</TITLE>
    <DATE>2018-12-11 14:29:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2018-12-11 14:35:27</DATE>
    <TEXT>Thanks for your question! We'll pass on to Henri on the 18th and see if he can explain in more detail then.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2018-12-11 14:36:25</DATE>
    <TEXT>Thanks, Remy! I'm sure Henri will be happy to elaborate</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-12 09:17:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to deploy smartcontracts on the Ethereum blockchain  amp; use Streamr to manage real time data streams  amp; create dapps</TITLE>
    <DATE>2018-12-12 09:28:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-12 10:34:23</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-12 12:48:16</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to deploy smartcontracts on the Ethereum blockchain  amp; use Streamr to manage real time data streams  amp; create dapps</TITLE>
    <DATE>2018-12-12 13:04:35</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Exclusive look at Streamr's unified platform for Q1 2019</TITLE>
    <DATE>2018-12-12 15:06:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Exclusive look at Streamr's unified platform for Q1 2019</TITLE>
    <DATE>2018-12-12 15:07:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2018-12-13 09:43:13</DATE>
    <TEXT>Thanks, us too :)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2018-12-13 09:43:43</DATE>
    <TEXT>Tutorials is something we're keen to explore soon</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>New look and feel Streamr marketplace, editor and tools coming early 2019!</TITLE>
    <DATE>2018-12-13 13:42:49</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128170; Our community hero for the month of December is ...  #127881; Amandine Flachs!</TITLE>
    <DATE>2018-12-13 16:07:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Hourly Air Pollution Data from 102 Locations around London added to Streamr #8217;s platform.</TITLE>
    <DATE>2018-12-14 09:18:37</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; We're having a #StreamrXmasLDN meetup  amp; live #StreamrAMA! Join us 18th December 18:30</TITLE>
    <DATE>2018-12-17 17:11:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; Join us TODAY for our Streamr Christmas meetup and AMA in East London. We're live from 19:00 GMT!</TITLE>
    <DATE>2018-12-18 12:31:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; Join us TODAY for our Streamr Christmas meetup and AMA in East London. We're live from 19:00 GMT!</TITLE>
    <DATE>2018-12-18 12:32:28</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; Join us TODAY for our Streamr Christmas meetup and AMA in East London. We're live from 19:00 GMT!</TITLE>
    <DATE>2018-12-18 12:42:41</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; Join us TODAY for our Streamr Christmas meetup and AMA in East London. We're live from 19:00 GMT!</TITLE>
    <DATE>2018-12-18 12:44:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127876; Join us TODAY for our Streamr Christmas meetup and AMA in East London. We're live from 19:00 GMT!</TITLE>
    <DATE>2018-12-18 15:53:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're LIVE for our end of year meetup and AMA!</TITLE>
    <DATE>2018-12-18 19:06:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're LIVE for our end of year meetup and AMA!</TITLE>
    <DATE>2018-12-18 19:24:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're LIVE for our end of year meetup and AMA!</TITLE>
    <DATE>2018-12-18 19:53:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Thanks for tuning in to our live #StreamrAMA and for sending your questions. You can find the full stream here.</TITLE>
    <DATE>2018-12-18 20:41:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr's head of partnerships, Ben Sheppard, highlights some of the relationships we're building with other projects</TITLE>
    <DATE>2018-12-19 11:45:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're excited to announce the #Streamr Community Fund! Propose and allocate tokens to initiatives that promote our tech or grow our reach.</TITLE>
    <DATE>2018-12-19 15:51:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to monetise your wearables' data in this simple tutorial!</TITLE>
    <DATE>2018-12-20 14:08:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to monetise your wearables' data in this simple tutorial!</TITLE>
    <DATE>2018-12-20 14:08:40</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Watch Henri Pihkala talk about the future of #Streamr and answer questions about on Network and platform use cases</TITLE>
    <DATE>2018-12-26 11:30:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127947; Learn how to monetise your fitness data with Streamr in this easy to follow tutorial</TITLE>
    <DATE>2018-12-28 13:03:32</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Meet our new Senior Digital Marketer and find out what she #8217;s been up to since joining the Streamr team in November!</TITLE>
    <DATE>2019-01-04 12:33:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-07 15:42:21</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-07 15:42:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-08 11:46:13</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>[OpenAQ Integration] Air quality data streams from 10,210 locations in 68 countries available via Streamr #8217;s platform!</TITLE>
    <DATE>2019-01-08 12:14:21</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Getting started with Streamr? We've got you covered with these easy to follow tutorials!</TITLE>
    <DATE>2019-01-08 12:52:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>[OpenAQ Integration] Air quality data streams from 10,210 locations in 68 countries available via Streamr #8217;s platform!</TITLE>
    <DATE>2019-01-08 14:27:43</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Carl presented the Streamr platform and asked 'How a real time data economy can solve city challenges?' at the the Smart City Alliance today</TITLE>
    <DATE>2019-01-09 15:13:01</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-10 11:54:43</DATE>
    <TEXT>A little bit more detail from Carl in Partnerships on yesterday's Smart City Alliance event. "It was very useful for networking and generated some new leads. The organisers will provide Streamr with a members list so we can select potential attendees for the team to follow up on and invite to future Streamr hackathons and events."</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-10 13:56:27</DATE>
    <TEXT>Hi there! Thanks for your message and words of support. There are partnerships and projects in progress behinds the scenes that are certainly connected to those areas and we're excited to announce more in due course as they develop. For now, you can see our list of current partnerships at the foot of the Streamr website [https://www.streamr.com/](https://www.streamr.com/) and watch Ben, head of partnerships at Streamr, give the latest update at our live meetup in December [https://youtu.be/Hr6q2c6riTs?t=490](https://youtu.be/Hr6q2c6riTs?t=490).  #8203; Hope that helps!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-10 15:53:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-11 09:34:51</DATE>
    <TEXT>Thanks, I understand the desire to hear about all the projects in progress, but they do take a bit of time and it makes sense for both parties to hold off on announcing publicly until it is in the best form possible.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-11 09:36:39</DATE>
    <TEXT>Thanks for the kind words, that's certainly what we're working towards. Much appreciated by the team.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Electrify Asia just integrated an initial solar powerpod generator with Streamr. Check out the canvas and see when it is generating electricity in realtime</TITLE>
    <DATE>2019-01-11 10:02:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-11 10:44:02</DATE>
    <TEXT>Thanks for sharing. Helping to create a fairer data sharing economy and return control of data to the individuals who produce it is what we're working towards. It's a big challenge and good to see the problem is gaining wider public attention.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-11 11:09:30</DATE>
    <TEXT>Yes, the consumer fairness model being adopted by the insurance companies rather than demanding the data sharing outright is certainly a challenge. Offering rewards as incentives for compliance and sharing rather than insisting is likely the best case and fairest to the user.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Getting started with Streamr? We've got you covered with these easy to follow tutorials!</TITLE>
    <DATE>2019-01-11 12:48:37</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>[OpenAQ Integration] Air quality data streams from 10,210 locations in 68 countries available via Streamr #8217;s platform!</TITLE>
    <DATE>2019-01-11 13:58:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #129345; Our community hero for January is  #8230;  #127878; Remy Konings! Congratulations Remy and thank you for administering the Streamr community groups!</TITLE>
    <DATE>2019-01-14 14:50:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr explained in under 2min - video</TITLE>
    <DATE>2019-01-14 14:51:03</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128499; Community fund administrator nominations!</TITLE>
    <DATE>2019-01-15 10:14:34</DATE>
    <TEXT>Our [community fund](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674) is launching soon and we're looking for two more administrators to join Sason and Remy in managing the fund. You can nominate yourself to become a community administrator here. Just comment on this thread and introduce why you #8217;d make a good candidate, any ideas for growing the project, and your Telegram handle. After a week, we #8217;ll add the eight candidates with the most thumbs up to a polling bot and share it on the [Streamr Telegram](https://t.me/streamrdata) for a final week of voting. At the end of the week, the two candidates with the most votes are chosen as fund administrators for the next six months. Good luck and don't forget to vote for your favourite!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128161; Community Fund project suggestions</TITLE>
    <DATE>2019-01-15 10:47:36</DATE>
    <TEXT>Got an idea or suggestion for the Streamr [Community Fund?](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674) Please share it here! How the fund is allocated is up to the wider community, but here are a few possible examples of actions and tasks the fund could undertake: * Community Competitions * Producing interesting Merchandise (Streamr core funds may be used to pay for production) * Marketplace integrations of open source data * Bounty prizes for development * Technical integrations with IoT devices and educational content creation * Local events organisation #8202; #8212; #8202;from meetups to hackathons * Original content and other marketing initiatives * Use case demos and prototypes These are simply suggestions and we encourage admins to invite the community to submit ideas and proposals of their own #8202; #8212; #8202;we #8217;re sure there #8217;s many more we haven #8217;t thought of and those are the ones we #8217;re really excited about! Interested in becoming a fund administrator? You can nominate yourself [here](https://www.reddit.com/r/streamr/comments/ag7290/community_fund_administrator_nominations/)!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-15 13:11:40</DATE>
    <TEXT>Ah i see you already started the process! Let's try and keep them all here if possible for ease of reading/voting. I'll ask if they wouldn't mind reposting.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-15 13:13:20</DATE>
    <TEXT>Hi @KauaiFe, would you mind posting again in in this thread to nominate yourself? Apologies, but we're trying to keep it one place. Thanks [https://www.reddit.com/r/streamr/comments/ag7290/community\_fund\_administrator\_nominations/](https://www.reddit.com/r/streamr/comments/ag7290/community_fund_administrator_nominations/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-15 13:14:06</DATE>
    <TEXT>Hi @jackfriends, would you mind posting again in in this thread to nominate yourself? Apologies, but we're trying to keep it one place. Thanks [https://www.reddit.com/r/streamr/comments/ag7290/community\_fund\_administrator\_nominations/](https://www.reddit.com/r/streamr/comments/ag7290/community_fund_administrator_nominations/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-15 13:14:43</DATE>
    <TEXT>Hi Chenrong, would you mind posting again in in this thread to nominate yourself? Apologies, but we're trying to keep it one place. Thanks [https://www.reddit.com/r/streamr/comments/ag7290/community\_fund\_administrator\_nominations/](https://www.reddit.com/r/streamr/comments/ag7290/community_fund_administrator_nominations/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-15 13:15:06</DATE>
    <TEXT>Hi Ulrich, would you mind posting again in in this thread to nominate yourself? Apologies, but we're trying to keep it one place. Thanks [https://www.reddit.com/r/streamr/comments/ag7290/community\_fund\_administrator\_nominations/](https://www.reddit.com/r/streamr/comments/ag7290/community_fund_administrator_nominations/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-16 12:31:35</DATE>
    <TEXT>Sounds great, Fredy! As long as it's legal we can't wait to see the results. Feel free to Tweet to @streamr and we'll give it a share.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr explained in under 2min - video</TITLE>
    <DATE>2019-01-16 13:31:08</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-16 13:56:14</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr explained in under 2min - video</TITLE>
    <DATE>2019-01-17 12:23:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!</TITLE>
    <DATE>2019-01-17 13:17:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're testing the upcoming Streamr P2P Network version on US Navel Research Laboratory's CORE emulator. 150 nodes in 15 cities with realistic latencies modeled after ping statistics from Wonder Network.</TITLE>
    <DATE>2019-01-17 14:52:11</DATE>
    <TEXT> #8203; https://i.redd.it/ahkwf9cdyza21.jpg</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-18 15:44:53</DATE>
    <TEXT>You can read more about the network here: https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Create a pyhton client for Streamr and get paid for</TITLE>
    <DATE>2019-01-18 15:46:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-21 11:58:44</DATE>
    <TEXT>Developer rates would depend on the nature of the task so it's difficult to give estimates. You can use this as a benchmark though [https://gitcoin.co/issue/streamr-dev/streamr-client-python/1/2150](https://gitcoin.co/issue/streamr-dev/streamr-client-python/1/2150). The fund it designed to be community run on community initiatives so involvement from the core team will be kept at a minimum as much as possible (with final veto rights held to ensure brand coherence). And yes, as outlined in the whitepaper, ideally the whole Streamr project will transition into a fully open source platform with decentralized governance as much as possible. We've got a way to go yet before we're at that stage but this is the aim and we're always exploring tools and models to achieve this.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-21 12:00:54</DATE>
    <TEXT>Interesting! Thanks for sharing that could be one to keep an eye on in line with our air pollution data integration [https://www.reddit.com/r/streamr/comments/a5ue37/hourly\_air\_pollution\_data\_from\_102\_locations/](https://www.reddit.com/r/streamr/comments/a5ue37/hourly_air_pollution_data_from_102_locations/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 15:10:00</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 15:24:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 15:38:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 16:00:47</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 16:14:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-22 17:00:27</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-23 08:57:28</DATE>
    <TEXT> gt;Ulrich Thanks for applying, Ulrich, can you send me your telegram handle for the week of voting please?</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-23 08:59:10</DATE>
    <TEXT>Thanks for applying, can you send me your telegram handle for the week of voting please?</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-23 11:07:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-23 11:39:43</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128241;  #128506;  #128722; Watch Riku push his phone #8217;s GPS location data to Streamr's Marketplace in this demo of the Streamr Trackr app, coming summer 2019</TITLE>
    <DATE>2019-01-23 12:03:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-23 12:18:39</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-23 12:59:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-23 13:19:25</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128467; We #8217;ve got a lot of exciting things coming up in 2019!</TITLE>
    <DATE>2019-01-23 14:56:26</DATE>
    <TEXT> #8226; New platform user pages  #127881;  #8226; The new look Editor  #128562;  #8226; Our new website with dev docs and a user education programme  #10024;  #8226; New integrations and libraries  #128170;  #8226; The Community Fund launch  #128184;  #8226; The new community newsletter  #128226;  #8226; A Trackr app MVP allowing anyone to sell GPS location data on Marketplace  #128506;  #8226; A new API explorer  #128269;  #8226; Community Products  #127757;  #8230; and more! Keep an eye out for further updates!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-23 16:13:55</DATE>
    <TEXT> #128499; The polls are open! Head over to the Streamr Announcement channel and cast your vote: [https://t.me/streamrofficial](https://t.me/streamrofficial). The two with the most votes at 16:00 CET 30/01/19 will be appointed for 6 months. Good luck!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We've started 2019 as we mean to go on! Read what the technical team have been working on in January's packed dev update.</TITLE>
    <DATE>2019-01-24 11:43:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-24 13:23:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Community Products: making crowdselling your data a reality from any application or gadget</TITLE>
    <DATE>2019-01-24 17:35:21</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128499; The polls are open until 16:00 CET 30/01 for our Community Fund administrator positions.</TITLE>
    <DATE>2019-01-25 10:02:42</DATE>
    <TEXT>Head over to the Streamr announcement channel to vote  #128073; [https://t.me/streamrofficial](https://t.me/streamrofficial)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Following yesterday's discussion on Telegram, Maverick Chow publishes his thoughts on how to enhance the value of the DATA token</TITLE>
    <DATE>2019-01-25 11:59:25</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-25 12:32:19</DATE>
    <TEXT>Wei and Shiv added their thoughts to the discussion. You can see their responses here: [https://www.reddit.com/r/streamr/comments/ajgbst/short\_update\_about\_datatokeneconomics/](https://www.reddit.com/r/streamr/comments/ajgbst/short_update_about_datatokeneconomics/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Recap of the Streamr hackathon focused on real time data insights with reps from the UK transport sector</TITLE>
    <DATE>2019-01-28 12:14:16</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 12:39:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 12:47:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 12:48:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 13:04:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 13:31:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-01-29 13:52:49</DATE>
    <TEXT>Hi there, this is a piece developed by our community member, Mavercik Chow. It's an idea that was discussed as a team and hosted it on the Streamr blog to generate further discussion - as we'd like to do with all considered suggestions related to the project. At present, there are no plans to follow through with the idea of using ETH to access the marketplace instead of DATA. You can see a further response to the idea from Wei and Shiv here [https://www.reddit.com/r/streamr/comments/ajgbst/short\_update\_about\_datatokeneconomics/](https://www.reddit.com/r/streamr/comments/ajgbst/short_update_about_datatokeneconomics/). Hope that helps!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is proud to be supporting the amazing work Ethereum Cat Herders is doing with a full year's grant provided in DATA</TITLE>
    <DATE>2019-01-29 15:17:32</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're pleased to launch the Streamr API Explorer as part of our ongoing drive to improve the developer experience on our platform</TITLE>
    <DATE>2019-01-30 15:54:45</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're pleased to launch the Streamr API Explorer as part of our ongoing drive to improve the developer experience on our platform</TITLE>
    <DATE>2019-01-30 15:55:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're getting close to finishing Milestone 1 ... https://trello.com/b/j24hxvjg/streamr-milestone-1</TITLE>
    <DATE>2019-01-31 15:01:38</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>[Youtube] Streamrs new API Editor</TITLE>
    <DATE>2019-02-04 09:36:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Login to Streamr with Ethereum in two clicks. Out now via our API  amp; coming soon to the front-end!</TITLE>
    <DATE>2019-02-04 15:37:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Login to Streamr with Ethereum in two clicks. Out now via our API  amp; coming soon to the front-end!</TITLE>
    <DATE>2019-02-05 11:00:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Take a look at the our new API Explorer as part of our drive to improve the developer experience on the Streamr platform.</TITLE>
    <DATE>2019-02-05 12:31:37</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Take a look at the our new API Explorer as part of our drive to improve the developer experience on the Streamr platform.</TITLE>
    <DATE>2019-02-05 12:31:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;re speaking at ETHCC in Paris on 6  amp; 7 March. We'll host a talk and workshop to introduce our off-chain scaling solution, Monoplasma, to the Ethereum community.</TITLE>
    <DATE>2019-02-06 10:56:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Look out for our demo of Monoplasma - a one-to-many, off-chain payment solution - live from the mainstage at Ethereum Denver</TITLE>
    <DATE>2019-02-07 11:29:03</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Great panel discussion on decentralising the #transportation industry at MOVE in London. Ties in with our hackathon in Birmingham to explore how realtime data can provide the missing component in this transformation.</TITLE>
    <DATE>2019-02-12 14:55:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128654; The Streamr team is on the road this week with FOUR events across three continents. Here #8217;s where to find us:</TITLE>
    <DATE>2019-02-12 14:59:04</DATE>
    <TEXT>\- [GDST hackathon](https://seafood-trackathon.devpost.com/) \- Bangkok \- [MOVE](https://www.terrapinn.com/exhibition/move/index.stm) \- London \- [MOBI](https://dlt.mobi/) \- Munich \- [ETH Denver](https://www.ethdenver.com/) (re posting this as it seems to have been deleted)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>ETH Denver this weekend! Shiv  amp; Henri will participate in panel talks,  amp; we'll demo our newest development  #8212; Monoplasma  #8212;live on stage!</TITLE>
    <DATE>2019-02-14 09:38:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr explained in under 2min - video</TITLE>
    <DATE>2019-02-14 09:39:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #129345; Our community hero for February is  #8230; John Doe  #127881;. Thanks for your help getting the Community Fund up  amp; running! We #8217;re looking forward to seeing the proposed projects develop!</TITLE>
    <DATE>2019-02-14 13:49:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We can't wait to share more about Monoplasma, our off-chain scaling solution for one-to-many payments, today at 15:00 (MST) from the #ETHDenver main stage. But why did we build it?</TITLE>
    <DATE>2019-02-15 16:26:44</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>If you wanna catch the launch of Monoplasma at 3pm Denver time (MST) but you're not in Denver, you can sign in here:</TITLE>
    <DATE>2019-02-15 16:40:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>In ~30 mins, we'll be introducing Monoplasma. Watch the livestream here:</TITLE>
    <DATE>2019-02-15 21:42:44</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're proud to release Monoplasma, our scalable one-to-many payment solution, at EthereumDenver. Watch the demo here:</TITLE>
    <DATE>2019-02-18 15:35:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Monoplasma code is now open and ready for anyone to deploy so pls send to projects you think might need this!</TITLE>
    <DATE>2019-02-18 15:36:44</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Monoplasma code is now open and ready for anyone to deploy so pls send to projects you think might need this!</TITLE>
    <DATE>2019-02-18 15:41:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Monoplasma allows multiple data producers to receive payments when their bundled data is sold on our Marketplace. See why this brings us closer to crowdselling:</TITLE>
    <DATE>2019-02-19 14:36:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127897; Henri speaks with Ryan Selkis on why you should monetise your data, who can benefit from decentralised realtime data streams, and how we're building the tools to make this a reality.</TITLE>
    <DATE>2019-02-20 10:09:37</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We support this call for an Ethereum community that is healthier and stronger when united. Decentralised Networks cannot work without a community.</TITLE>
    <DATE>2019-02-20 15:01:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Who will own the infrastructure and therefore control your data when our cities become "smart"?</TITLE>
    <DATE>2019-02-21 12:29:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Here are eight ways you might use the Streamr Community Fund to help us grow</TITLE>
    <DATE>2019-02-21 15:02:28</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We built Monoplasma as a scaling solution for one-to-many payments in our upcoming community products feature. But we think it can be useful for other #buidlers out there too!</TITLE>
    <DATE>2019-02-22 10:42:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We built Monoplasma as a scaling solution for one-to-many payments in our upcoming community products feature. But we think it can be useful for other #buidlers out there too!</TITLE>
    <DATE>2019-02-22 10:43:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We built Monoplasma as a scaling solution for one-to-many payments in our upcoming community products feature. But we think it can be useful for other #buidlers out there too!</TITLE>
    <DATE>2019-02-22 12:15:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-02-27 13:45:58</DATE>
    <TEXT>Thanks for flagging this to us. We'll have a response on it soon.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-02-28 10:33:05</DATE>
    <TEXT>Ben Sheppard from partnerships shared this on the press release: "Hi everyone, Ben Sheppard from Partnerships here. Sorry for not writing earlier, I #8217;ve been busy in meetings at Mobile World Conference in Barcelona.  #8203; The update from HPE and Continental is a really fantastic development within the mobility industry. Seeing two major enterprises taking significant steps around data marketplaces can only be good news for projects such as ours.  #8203; Let me also add that it is not unusual for organisations of HPE size to have a relationships with multiple partners working with similar technologies. We continue to have a very strong relationship with HPE which has developed since we first announced we were partnering with them in May last year. That work to explore multiple use cases involving the Streamr technology stack continues." You can see the full transcript and other responses from Shiv regarding partnership news on our Telegram [https://t.me/streamrdata](https://t.me/streamrdata)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128239; How will we tell the world about our mission and attract the right users to our platform? Here #8217;s what #8217;s on the road ahead for marketing Streamr in 2019:</TITLE>
    <DATE>2019-02-28 12:51:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There is now an integration between Node-RED and Streamr! Read more here:</TITLE>
    <DATE>2019-03-01 12:28:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There is now an integration between Node-RED and Streamr! Read more here:</TITLE>
    <DATE>2019-03-01 12:29:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There is now an integration between Node-RED and Streamr! Read more here:</TITLE>
    <DATE>2019-03-01 15:53:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Excited to see our Monoplasma repository has already been forked five times in under a week.</TITLE>
    <DATE>2019-03-04 11:15:01</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128187;  #128736; Our developer forum is the place to workshop pilots, integrations, the Community Fund projects, and more. Join us there!</TITLE>
    <DATE>2019-03-05 12:42:07</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Today we're at ETHCC sharing the ways Monoplasma can be deployed by the Ethereum scaling space. Henri Pihkala will be on stage at 10:35 Paris time.</TITLE>
    <DATE>2019-03-06 09:04:18</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128225;  #8220;Data Crowdsourcing will become a huge thing #8221; - Henri features on the Disruptor Daily podcast and looks ahead to the future of decentralised data sharing.</TITLE>
    <DATE>2019-03-06 12:08:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Juuso will give a workshop today on Monoplasma examining its use for dividends, airdrops,  amp; revenue sharing at 13:10 Paris time. Livestream here:</TITLE>
    <DATE>2019-03-07 09:12:35</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-03-07 17:54:47</DATE>
    <TEXT>Hold on I #8217;ll repost</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Network topology simulations, protocol upgrade, analytics tools, release of Monoplasma and integration with IBM #8217;s Node RED, February has been a frantic month alright! Here #8217;s what the devs have been working on:</TITLE>
    <DATE>2019-03-07 17:58:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-03-07 17:59:09</DATE>
    <TEXT>Reposting as the last had a link issue.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Nice quote from Henri speaking on the Disruptor Daily podcast</TITLE>
    <DATE>2019-03-08 10:18:50</DATE>
    <TEXT> #8220;In the current business model of the whole internet: The internet giants are collecting your data, selling it off to advertisers, you #8217;re not in control, it #8217;s not transparent, you #8217;re not earning a penny from your data. I would be really disappointed in mankind if this was the only business model we can actually have on the internet for free services or personal data." - Henri Pihkala. Full podcast here: [https://soundcloud.com/disruptordaily/ep58-taking-data-marketplaces-open-source-with-blockchain-henri-pihkala](https://soundcloud.com/disruptordaily/ep58-taking-data-marketplaces-open-source-with-blockchain-henri-pihkala)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127897;Shiv speaks with George Spasov from Lime Chain at ETHCC about the issues Monoplasma helped them solve around airdrops, dividends, and bounties</TITLE>
    <DATE>2019-03-08 14:18:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Design updates are on the way and we #8217;d love your help getting the UX as smooth as possible. Please sign up on the forum to be among the first to test the new look Streamr:</TITLE>
    <DATE>2019-03-12 14:10:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Thanks for all your questions in yesterday's Telegram AMA with Henri Pihkala! Topics covered design updates, partnerships, tokenomics, marketing, long term goals, and more. You can read the full transcript here:</TITLE>
    <DATE>2019-03-14 11:03:48</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Our community hero for March is  #129345;  #8230; George Spasov! Thanks to George for sharing how Monoplasma has helped the Limechain development process and for modelling the Streamr t-shirt during his vlog.</TITLE>
    <DATE>2019-03-15 15:30:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can the Ethereum scaling space benefit from Monoplasma? Henri shares some of its one-to-many payment use cases from pensions to airdrops</TITLE>
    <DATE>2019-03-18 15:54:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can the Ethereum scaling space benefit from Monoplasma? Henri shares some of its one-to-many payment use cases from pensions to airdrops</TITLE>
    <DATE>2019-03-18 15:54:46</DATE>
    <TEXT> #8203; https://reddit.com/link/b2kcc4/video/j4na4y30gwm21/player</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The new API explorer is just one of the ways we're working to make it simple for developers to build and integrate with Streamr products. Check out this short overview:</TITLE>
    <DATE>2019-03-19 13:39:12</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The new API explorer is just one of the ways we're working to make it simple for developers to build and integrate with Streamr products. Check out this short overview:</TITLE>
    <DATE>2019-03-19 13:40:07</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"In the future, open sourcing a project will be synonymous with credibility  amp; transparency, similar to how companies nowadays employ auditors to certify their financial statements"</TITLE>
    <DATE>2019-03-20 13:07:13</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>You'll find many useful tutorials saved to our Medium page, including how to:</TITLE>
    <DATE>2019-03-21 14:51:58</DATE>
    <TEXT>\- Connect your data to the Streamr Marketplace  #128225; \- Build air quality based smart contract apps  #127981; \- Use Monoplasma for one to many payments  #129534; Take a look! [https://medium.com/streamrblog/tutorial/home](https://medium.com/streamrblog/tutorial/home)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Devs - share a stream proposal or product extension that allows people to retail their digital info and get backing from the Community Fund:</TITLE>
    <DATE>2019-03-22 15:16:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>One of our favourite integrations from last year: easily sell your real time data produced by the sensor nodes from our partners Ruuvi to monetise and decentralise your data with Streamr.</TITLE>
    <DATE>2019-03-25 17:42:01</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We're really pleased that when you strip away the fake volumes, (which we've always refused to engage with) Streamr is in the top 25.</TITLE>
    <DATE>2019-03-27 12:09:00</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is now on Blockfolio Signal. Follow us for the latest project updates.</TITLE>
    <DATE>2019-03-28 10:47:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Check out the Node-RED integration tutorial using Helsinki tram data to help you get started:</TITLE>
    <DATE>2019-03-29 15:27:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Lower insurance, better roads, less congestion: last year's blog examining some of the use cases for how Streamr can unlock the benefits of vehicle data in the transport industry</TITLE>
    <DATE>2019-04-01 14:30:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Join us at #PBCW on the 16th and 17th April. Henri will feature on the main stage tech track panel discussing  #8216;Wallets for People vs. Wallets for Objects #8217; at 14:20 local time (17th)</TITLE>
    <DATE>2019-04-02 16:25:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-03 08:40:10</DATE>
    <TEXT>Great write up. Thanks a lot for posting!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Really pleased to get some strong and independent project reviews at the start of the month from Coin Bureau and Binance.</TITLE>
    <DATE>2019-04-03 09:05:03</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #9881; #65039; Take a look at the Streamr dev update for March. This is what has been happening behind the scenes on the tech side:</TITLE>
    <DATE>2019-04-04 13:08:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127903;  #127467; #127479; We have a ticket for Paris Blockchain Week Summit to give away to our community</TITLE>
    <DATE>2019-04-04 14:19:35</DATE>
    <TEXT>Simply share our explainer video on Twitter or LinkedIn and tag @Streamr in your post to enter. The winner will be selected at random next week. See you there! #PBCW  #8203; Vid to share: [https://youtu.be/nbWIPVE8zFE](https://youtu.be/nbWIPVE8zFE)  #8203; The ticket does not include travel or accommodation. Please let us know if you're already attending and look out for Henri on stage at 14:20 on the 17th.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;re delighted to be competing in the Trusted IoT Alliance Smart E-Mobility Challenge. Building infrastructure to unlock the value of vehicle data is an exciting prospect!  #128654;  #128225;  #128752;  #127760;  #128182;</TITLE>
    <DATE>2019-04-09 11:17:47</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-09 11:48:52</DATE>
    <TEXT>You might find these two answers useful from the recent [AMA](https://medium.com/streamrblog/transcript-telegram-ama-with-henri-pihkala-13th-march-2019-6133985144e3) with Henri: ***"What is the use of tokens and what ways to spend it in the future?*** ***Henri #8202;:*** *So far the tokens are only a means of payment on the Marketplace. They will also have a broad range of functionality on the Network level, and this will be shipped in stages starting from simple token stake checking to a more complicated incentive mechanism enabling node rewards. On the Marketplace the role of the token will also be extended to influence a product reputation system (for example, products with more volume or with larger DATA holdings by the seller can be shown higher up in product listings).* *Perhaps one more thing to point out, although a sub-use case of payments on Marketplace, will be Community Products, which will enable individual end users to earn DATA for contributing data to collaborative data products on the Marketplace.* ***Hi Henri, what is your view on the tokeneconomics of Streamr? I like to know how you look at this and how you can assure the community about DATAcoin and it relevence.***  #8203; ***Henri #8202;:*** *So obviously the token will have various roles both on the Network and on the Marketplace, but on the Marketplace specifically: yes we #8217;re looking to enable data buyers to make purchases using various assets, not just DATA, to improve usability. The first solution to this will be to use one of the smart contract -based DEXes to swap these incoming assets to DATA when making purchases. So what will happen is this:* *- Buyer comes in, clicks on Purchase button on Marketplace* *- We can inspect what tokens they have in their wallet, and enable them to choose the one they want to spend on the purchase* *- When they make the purchase, we atomically swap that asset to the required amount of DATA using either Bancor or Uniswap* *- The seller will receive DATA, just like currently* *So we #8217;re kind of adding a conversion layer to enable people to purchase products even if they don #8217;t have DATA but have other crypto assets.* *This combined with a proper fiat on-ramp would be amazing, but this is something people are still trying to solve. Imagine being able to come in with a credit card, use that to easily buy ETH, and then use that ETH to automatically swap to DATA in order to buy a product. That could be made really, really easy for the user if the fiat-to-crypto didn #8217;t require all the KYC hurdles."*  #8203; We also have a bit of info on the [FAQ](https://www.streamr.com/faq#datacoin):  #8203; ***"Why is DATAcoin necessary and what does it do?***  #8203; *The token will be needed for at least two purposes:* 1. *Streamr Network usage.* 2. *Payment for data licenses in case of non-free data on the Streamr Marketplace.* *The Streamr Network is a decentralised peer-to-peer (P2P) network. It is used to transport real-time data from data producers to data consumers. Using the services of the Network - publishing and subscribing to data - requires a small amount of DATA tokens, consumed as fees. The fees will be earned by nodes which form the Network and provide the data transport service. This incentivises people to run nodes in the Network, offering them a way to monetise their idle bandwidth.* *The token also represents value in the data content. The Streamr Marketplace allows data producers to receive payments in DATAcoin for a data license and permissions to access and use the data. Access to data can be traded on the Marketplace by companies, organisations, individuals, and machines, offering a novel monetisation mechanism for data content. The token will probably have more complicated uses too, related to staking and reputation mechanisms at play within the P2P network. The details regarding these will only be discovered much further into the project."*  #8203; Ultimately, as others have mentioned, communicating more on the tokenomics and DATA's use in the ecosystem (without token "hype" practices which we do not believe in) is something we can (and will) do more of - especially as we near the completion of the Network where its use will be focused.  #8203; Thanks for an interesting discussion and your feedback. The help and support from this community is an essential trait that can set us apart from the traditional tech world.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-10 08:13:19</DATE>
    <TEXT>You might find this post useful on some of the transportation use cases we see: https://medium.com/streamrblog/unlocking-the-benefits-of-vehicle-data-solving-data-limitations-in-the-transport-industry-a3e7b7f78355</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-11 08:29:24</DATE>
    <TEXT>Thanks for sharing your experience there and thoughts. The primary focus for the time being is on getting the tech stack as good as possible so there is limited marketing activity in Asia. This may change in the future but those markets will require a lot of attention and a solid foundation first to penetrate.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Meet the Streamr team and hear what motivates our core contributors to work towards a decentralised realtime data economy:</TITLE>
    <DATE>2019-04-11 10:46:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-11 10:47:18</DATE>
    <TEXT>Full playlist here: [https://www.youtube.com/playlist?list=PLz4GDOpDozwUxATM8yWHLt5uDO953nyZ2](https://www.youtube.com/playlist?list=PLz4GDOpDozwUxATM8yWHLt5uDO953nyZ2)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Fighting tech with tech - how crowdselling is the solution to the politics of data ownership.</TITLE>
    <DATE>2019-04-11 12:47:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Fighting tech with tech - how crowdselling is the solution to the politics of data ownership.</TITLE>
    <DATE>2019-04-11 12:50:47</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127925;  #128225;  #128722; Have you tried our Community Products demo using your Spotify data? Scan the QR code, click the link, and start sending your playlist info into a data union. In the near future you can receive automatic payment for a similar data exchange: https://bit.ly/2v6eNPv</TITLE>
    <DATE>2019-04-12 09:12:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>For the Trusted IoT Smart E-Mobility Challenge, we're showcasing how a P2P realtime data feed that shares events, such as accidents, with other subscribing cars  amp; infrastructure managers can be used to enhance road saftey. Pics of the Streamr team  amp; TIoTA members at the Bosch facility in Ludwigsburg</TITLE>
    <DATE>2019-04-12 12:17:57</DATE>
    <TEXT> #8203; https://preview.redd.it/2bd3oz5cstr21.jpg?width=4032 amp;format=pjpg amp;auto=webp amp;s=721de18071be4c36fb6fc490ab9d4ab8d00184bf https://preview.redd.it/tsne3p5cstr21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=4d7a20c12e3c4fdb6ea7491d221e425b866bece4 https://preview.redd.it/kj2uxq5cstr21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=9286feb20ea3c7d6bcbfa2437a472969b8a69042 https://preview.redd.it/yo6za06cstr21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=4b616c7c7f74c95cdbfeddf713167e7dbd4a86ba</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Taking data from a Jaguar electric car and visualising it as it flows through the Streamr stack. Drivers can share data with anyone or anything in the world for a safer and more optimised road network. Great to see the tech from our explainer video come to life!  #128665;  #128202;  #128752; #TIOTAchallenge</TITLE>
    <DATE>2019-04-15 11:52:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Curious to learn more about the Streamr decentralised Network for realtime data exchange? Join Lead Developer, Eric Andrews, for a Telegram AMA today at 11:00 CET</TITLE>
    <DATE>2019-04-16 08:33:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>We #8217;re at PBWSummit for the next two days. Look out for Henri Pihkala on the Master Stage panel tomorrow (17th) at 14:20 local time discussing 'how blockchain will transform IOT'. Here #8217;s the lineup!</TITLE>
    <DATE>2019-04-16 11:18:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127897; Crypto 101 spoke with Henri Pihkala and Shiv Malik at ETH Denver. Hear about the latest Streamr news and project developments (from 21:30):</TITLE>
    <DATE>2019-04-16 13:41:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Q amp;A transcript about the Streamr Network with Lead Developer, Eric Andrews</TITLE>
    <DATE>2019-04-16 14:41:31</DATE>
    <TEXT> #8203; https://preview.redd.it/v47eanyf0ns21.png?width=3200 amp;format=png amp;auto=webp amp;s=bea2c1594a8ecc60ffcd05d0743e046d790d8ab7 *Streamr lead Developer, Eric Andrews, joined us for a* [*Telegram*](https://t.me/streamrdata) *Q amp;A today (11:00 CET 16/04/19) to discuss the Network development and answer community questions. Here is what was said:*  #8203; Some background to the topic. Currently me and Miroslav are focusing our efforts on the software development part of the network. We also have Henri overseeing/guiding the project and have consulting help from Petri Savolainen from University of Helsinki, who was has studied p2p networks in depth.  #8203; The next milestone for the Streamr Network is to replace our current centralized system ([https://github.com/streamr-dev/streamr-docker-dev/blob/master/high-level.png](https://github.com/streamr-dev/streamr-docker-dev/blob/master/high-level.png)) with a peer-to-peer system. This means getting rid of a lot of services and replacing them with capabilities provided by Streamr Network. This milestone is mostly concerned about the p2p networking capabilities as well signing and possibly encrypting data. Incentivization is not yet a part of this milestone, that will have to come later.  #8203; At the moment we have working peer-to-peer pub-sub system built on top of WebSockets. We can run nodes, they connect to a tracker that oversees the structure of the network and assigns nodes to connect to other nodes. Nodes can subscribe to streams and receive as well as propagate messages. Duplicate detection is in place, and currently we are working on storage nodes and re-send request / historical requests for persistent data.  #8203;  #8203; ***Q: I see on trello that testing at scale is done. Can you tell something more about the results?***  #8203; **A:** We are currently using a Network emulator tool called CORE [https://www.nrl.navy.mil/itd/ncs/products/core](https://www.nrl.navy.mil/itd/ncs/products/core) to test our network at scale.  #8203; In the tool we can create virtual network interfaces in Linux and run Streamr Network nodes behind them. This allows us to run large amounts of nodes whilst simulating realistic network conditions. For example, we can introduce latency as well as disruptions between links. We can also simulate a car driving around a city/highway changing base stations for a mobile link.  #8203; What we have currently been measuring is certain properties of the emerging network topology, or graph if you will. We have been measuring for example, how many hops on average or at worst case a message will have to go through before arriving at a node. Also we've measured the diameter of the emerging graph to see if we're on the right track.  #8203; Another thing we've been measuring is how much of duplication of messages there are. Message duplication is unavoidable in our network since messages get propagated to all/most neighbors of node, and then to the neighbors of the neighbors etc. etc.. Message redundancy is not necessarily bad though, it can be a good property for censorship-resistance.  #8203; We've also been measuring just general statistics (number of messages received, latency etc.)  #8203; The main goal of the measurements at this point has been to ensure that we are on the right track and nothing particularly alarming is happening in the observed metrics.  #8203;  #8203; **Q: Can you tell something about the difference between a normal node and a storage node? Will the community be able to run both in the future?**  #8203; **A:** In the current code base, a storage node is basically a node with the following additional properties.  #8203; \- It stores messages received into a persistent data store (e.g. Cassandra). \- Clients and nodes can request stored messages from it using short-lived WebSocket connections. \- It has a distinct peer-type that lets other nodes and the tracker know that is a storage node.  #8203; A storage node's main added responsibility will be to store incoming messages and then be able to serve them when requested.  #8203; What is clear is that it will be very important for the storage node to receive all messages in the network so nothing goes missing. It may even have a more active role in asking neighbors for missing messages compared to regular nodes.  #8203; The idea is that in the future, anyone could run a node (non-storage or store). We are still not fully decided whether all nodes will be storage nodes, or if the storage is an additional capability that can be enabled for some extra $DATA in change for storage space on your disk. It may also be beneficial to not server regular clients from a storage node but save the bandwidth for those resend / historical data fetches.  #8203;  #8203; ***Q: If 0% was start and 100% is the version you envision, where is the current version in your opinion?***  #8203; **A:** For the next milestone, which is replacing the central system, I'd say we're at least 70% there. I think we're very very close to being feature-complete w.r.t. the current system. The next big hoops will be putting this thing running on a staging environment and integrating as well as testing it against the existing services. There is some work that needs to be done to one of the existing services (engine-and-editor) for this to happen.  #8203; One thing about developing the network that we have learned is that sometimes it is very hard to diagnose and fix bugs. Some bugs only appear in large networks, and debugging the root cause of a misbehavior in a network of say 100 nodes can require some serious detective work. We've ironed out several of such bugs, hopefully we're more or less done on that front .:)  #8203;  #8203; ***Q: Is there at this stage any collaboration with Gollem to test/develop the network?***  #8203; **A:** At this stage, not that much. We've been really focused on getting this forward with a small but effective task force. I believe after we get something tangible out (the 1st version that will replace the centralized version), we will have more to talk with Golem. Certainly lessons we could learn from them, I'd imagine. One thing I've been eyeing is their network browser that shows the state and nodes of their network. Something similar would be nice for the Streamr Network as well. :)  #8203;  #8203; ***Q: Will message duplication at a large scale not lead to a lot of wasted bandwidth and possibly a congested network?***  #8203; **A:** There are two systems in place to alleviate this.  #8203; 1. We partition the Streamr network based on streams. Each node is responsible for a subset of all the streams. This means that when a message is produced to a stream, it only gets propagated to those nodes that are responsible for the stream.  #8203; 2. Duplicate detection. A node will only propagate a message once. If the same message arrives on the node again (from another source), it will notice that it is a duplicate, and not propagate it itself.  #8203; There is still unavoidable duplication. Say I am running node N0 and am propagating to 5 nodes N1, N2, N3, N4, and N5. Now say N1 and N4 have already received the message. I cannot know as N0 which of 5 nodes have already received the message so inevitably some duplication will occur. But at least N1 and N4 will not further propagate the already seen message.  #8203; Of course with control messages we could let N0 know that, hey, N1 and N4 already know about the message. But control messages have a cost associated as well, because they have to use the bandwidth, so it becomes a sort of a trade-off. At this point it is our assumption that messages will most likely often be small. Thus we might as well send them instead of sync around with control messages. If messages are large, however, the value of control messages increase. This is something we need more data on to make a conclusive decision on though.  #8203; ***Q: Do you have any exciting or ideal use cases in mind for how you #8217;d like to see others use and connect to the Network in the future?***  #8203; **A:** I find the community product and its possibilities and use cases to be very powerful.  #8203; But I am a rather technical guy so I will give you some maybe more under-the-radar technical use cases I could envision:  #8203; \- In the network code we will allow people to extend the network into their own codebases. I.e. running a node in their own project. It will be interesting to see what sort of extended behavior people could come up with. E.g., you could build a system that provides an interface to some existing protocol X and then handles the conversion of messages between it and Streamr Network.  #8203; \- Somewhat related to the above: MQTT integration! Being able to, with little to no effort, write a MQTT url somewhere and just hit "submit" and then have that data be forwarded into the Streamr Network.  #8203;  #8203; ***Q:*** ***regarding running a node ourselves. Do you have any knowledge already if it will be necessary to open certain ports on a router in order to make it work for people who want to run nodes? As i recall, in running a golem node, one first needs to open some of these ports to make it work. I'm asking of course because opening ports is not very mainstream for most people :)***  #8203; **A:** For the first milestone version opening ports will be necessary for node operators. Clients subscribing to and publishing messages don't need to open ports.  #8203; That being said we are aware of the Firewall/NAT issue and of suggested ideas to get around it. Ideally we wouldn't want our users to have to play around with firewall/NAT settings, but we haven't still looked deeply into how solvable the problem is. One project to keep an eye on in this front is MetaMask Lab's Mustekala. They are looking at having "light client" on the browser that seed data [https://medium.com/metamask/metamask-labs-presents-mustekala-the-light-client-that-seeds-data-full-nodes-vs-light-clients-3bc785307ef5](https://medium.com/metamask/metamask-labs-presents-mustekala-the-light-client-that-seeds-data-full-nodes-vs-light-clients-3bc785307ef5)  #8203; Alright people, thanks for questions!  #8203; Time for me to wrap this up and get back to developing the network! I hope you found the answers informative and/or useful and that you got a picture of where we are on technical side of the network and where we are headed. ...  #8203; *Eric has agreed to return for another Q amp;A further along the line as the Network progresses. Thanks for your questions and until then!*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"If you place your data in a silo you're placing a cap on the value your data can have ... If these silos can't communicate you can say goodbye to things like Smart Cities ... IoT and the machine economy cannot happen with centralised players" - Quotes from Henri Pihkala on today's panel at #PBWS</TITLE>
    <DATE>2019-04-17 13:47:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Our community hero for March is  #129345;  #8230; Milan. Thank you for your communication suggestions that led to us joining Blockfolio Signal to expand our reach.</TITLE>
    <DATE>2019-04-18 08:08:48</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-18 08:09:45</DATE>
    <TEXT>Unfortunately not. Don't believe it was filmed. If one appears we'll be sure to share here of course.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>This week we're presenting our Smart E-Mobility Challenge use case at the IBM Research HQ in Zurich. You can read about the challenge and PACE tour here:</TITLE>
    <DATE>2019-04-23 08:00:08</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Here's Carl Rodrigues at IBM Research campus in Zurich explaining our latest pilot to employees. Instead of cars mining crypto, they'll be 'mining' data: getting paid for contributing capacity to a new web of real-time data which everyone runs.</TITLE>
    <DATE>2019-04-24 13:20:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Today the explainer video comes to life filming our #tiotachallenges pilot. Real-time vehicle data is sent to the Network. Others subscribe: fellow road users, road operators,  amp; companies like Fetch AI who turn it into intelligence. Drivers are paid when aggregated data is sold.</TITLE>
    <DATE>2019-04-25 12:51:46</DATE>
    <TEXT> #8203; https://preview.redd.it/6cudmy48qeu21.jpg?width=4032 amp;format=pjpg amp;auto=webp amp;s=88919445d31fd4d32b93f8adc9a744943555ac49</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128665;  #128225;  #128663;  #127760; It was a lot of fun testing our #TIOTAchallenges pilot in the Swiss mountains yesterday. Here, the second car avoids obstacles in the road, flagged by the first's data shared in real-time! Full video to come soon!</TITLE>
    <DATE>2019-04-26 11:21:01</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There are no small challenges involved in building a robust Oracle system which is why we're announcing that we're working with the experts Chainlink. Read more here:</TITLE>
    <DATE>2019-04-29 09:02:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Last week in Zurich was wonderful working with our pilot partner #8217;s Bosch, Fetch AI, and Riddle  amp; Code to create a more networked future in transport. Here #8217;s a clip of Carl presenting the pilot at IBM Research HQ. #tiotachallenges</TITLE>
    <DATE>2019-04-30 08:47:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-04-30 14:16:21</DATE>
    <TEXT>Correction: I should add we #8217;re currently working on some integrations with companies in the Philippines to monietise fisheries related data and Ben, our Head of Partnerships, has been in Asia recently and will be there again next month probably visiting Thailand, Singapore and the Philippines again for meetings with various interested parties</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>'IoT Is Broken  #8211; This Is How Peer-to-Peer Networks  amp; Blockchains Will Fix It' is the new topic of Henri #8217;s talk at Bosch Connected World on the 15th of May. Hear how Streamr can upend the market  amp; usher in a new era of data ownership.</TITLE>
    <DATE>2019-05-01 12:59:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There are no small challenges involved in building a robust Oracle system which is why we're announcing that we're working with the experts Chainlink. Read more here:</TITLE>
    <DATE>2019-05-02 06:58:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-02 06:58:38</DATE>
    <TEXT>Thanks!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is named as a "disruptor" in the mobility space and will pitch at the Plug and Play Innovation Day on the 28th May. Read more:</TITLE>
    <DATE>2019-05-03 11:11:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-07 09:33:55</DATE>
    <TEXT>Thanks for the suggestions, i'll pass on to the wider team.  #8203; Some points discussed before you might find relevant:  #8203; Henri regarding DATA tokenomics, use cases, and usability (from the [Telegram AMA](https://medium.com/streamrblog/transcript-telegram-ama-with-henri-pihkala-13th-march-2019-6133985144e3)):  #8203; ***What is your view on the tokeneconomics of Streamr? I like to know how you look at this and how you can assure the community about DATAcoin and it relevence.*** "*So obviously the token will have various roles both on the Network and on the Marketplace, but on the Marketplace specifically: yes we #8217;re looking to enable data buyers to make purchases using various assets, not just DATA, to improve usability. The first solution to this will be to use one of the smart contract -based DEXes to swap these incoming assets to DATA when making purchases. So what will happen is this:* *- Buyer comes in, clicks on Purchase button on Marketplace* *- We can inspect what tokens they have in their wallet, and enable them to choose the one they want to spend on the purchase* *- When they make the purchase, we atomically swap that asset to the required amount of DATA using either Bancor or Uniswap* *- The seller will receive DATA, just like currently* *So we #8217;re kind of adding a conversion layer to enable people to purchase products even if they don #8217;t have DATA but have other crypto assets.* *This combined with a proper fiat on-ramp would be amazing, but this is something people are still trying to solve. Imagine being able to come in with a credit card, use that to easily buy ETH, and then use that ETH to automatically swap to DATA in order to buy a product. That could be made really, really easy for the user if the fiat-to-crypto didn #8217;t require all the KYC hurdles."* ***What is the use of tokens and what ways to spend it in the future?*** "*So far the tokens are only a means of payment on the Marketplace. They will also have a broad range of functionality on the Network level, and this will be shipped in stages starting from simple token stake checking to a more complicated incentive mechanism enabling node rewards. On the Marketplace the role of the token will also be extended to influence a product reputation system (for example, products with more volume or with larger DATA holdings by the seller can be shown higher up in product listings).*  #8203; *Perhaps one more thing to point out, although a sub-use case of payments on Marketplace, will be Community Products, which will enable individual end users to earn DATA for contributing data to collaborative data products on the Marketplace."*  #8203; In terms of decentralising, this will take time but is on the roadmap to be implemented at all levels possible. This is related from our [FAQ](https://www.streamr.com/faq#tech):  #8203; ***Streamr already has a working product. Will DATAcoin be used on this platform before you fully complete the roadmap?*** *Definitely. DATAcoin can be implemented swiftly in the application layers (Marketplace and Engine), even if the* [*underlying infrastructure*](https://www.streamr.com/register/signup) *remains centralised. The hardest part of our project is the decentralisation of the needed infrastructure, which enables people to earn DATAcoin by running the broker node on their own machines. Building our fully decentralised infrastructure will take time but along the way, the token can be put to good use in all the layers.* DATA use cases were also covered on [this thread](https://www.reddit.com/r/streamr/comments/b9hyft/please_stop_being_just_another_non_crypto_tech/).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Our dev community has built prototypes to share your Fitbit, Spotify, Ruuvi Tag, and Firefox data (coming soon!). If you have a proposal for a data union, let us know  amp; get funding to make it:</TITLE>
    <DATE>2019-05-07 12:33:59</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-07 12:34:23</DATE>
    <TEXT>You can submit or pick up a proposal here: [https://forum.streamr.dev/t/community-fund](https://forum.streamr.dev/t/community-fund)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"What does it take to turn a flood of data into authoritative and useful information?" Our head of Partnerships, Ben Sheppard, will speak at SeaWeb Seafood Summit to answer this question and present our work on the 12th June.</TITLE>
    <DATE>2019-05-07 15:45:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-07 16:08:54</DATE>
    <TEXT>Ultimately the aim is to have the payment system and tokens as backended as possible so those not familiar to crypto can come with a credit card to transact and earn for their data as simply as possible. For incentivising producers, at first, as is common for new products, Streamr will subsidise data producers so users are paid for their data right away. This should give the products its best chance of growing into something useful others will want to pay for. Of course money is not the incentive for all - many of our community feel a decentralised data sharing model is right in the current climate, which is showing the dangers of the reverse. Other products, such as [this air quality one](https://medium.com/streamrblog/air-quality-data-streams-from-10-210-locations-in-68-countries-available-via-streamrs-platform-981d52b0ecb3), are intended to attract users with motivation to contribute for a greater good. A similar example, later down the line but demoed recently for our TIOTA pilot, will see drivers benefit from the network of data they contribute along with other cars on the road.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #8220;Community Products creates a win win win situation for developers, data producers,  amp; data consumers. #8221; Quote from Ebrahim, one of our amazing community devs working on a plugin for you to crowdsell your browser, social media,  amp; shopping data!</TITLE>
    <DATE>2019-05-08 13:25:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127937; The finish line for milestone one is in sight! Take a look at our public Trello and see what's completed for the Engine, Marketplace, and Network: https://trello.com/b/j24hxvjg/streamr-milestone-1</TITLE>
    <DATE>2019-05-09 12:43:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A design refresh of Streamr's Core App, both Network layer and broker nodes in-progress, and alpha testing underway on Community Products. April's dev update is packed out:</TITLE>
    <DATE>2019-05-09 13:54:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The data sharing economy is one sided and broken. It #8217;s time for a new model that puts you in control of your data. This is how we start down that path.</TITLE>
    <DATE>2019-05-13 12:39:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The data sharing economy is one sided and broken. It #8217;s time for a new model that puts you in control of your data. This is how we start down that path.</TITLE>
    <DATE>2019-05-13 12:39:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Firefox plugin to sell your browser data and earn tokens.</TITLE>
    <DATE>2019-05-13 15:17:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Middle way between withholding data access and the current hand it all over model?</TITLE>
    <DATE>2019-05-13 15:28:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Plugin to let users aggregate their browser data for purchase as a bundle.</TITLE>
    <DATE>2019-05-13 16:48:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The data sharing economy is one sided and broken. It #8217;s time for a new model that puts you in control of your data. This is how we start down that path.</TITLE>
    <DATE>2019-05-14 07:21:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Really pleased to have had Streamr devs partnering with Bosch and RIDDLE amp;CODE  #8207;on this in depth pilot for the #tiotachallenges competition. Watch how the team is helping to develop the internet of cars for the future.</TITLE>
    <DATE>2019-05-14 13:21:29</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Today we #8217;re at Bosch Connected World speaking with attendees about our TIOTA pilot and finding new projects for community products and more</TITLE>
    <DATE>2019-05-15 07:57:56</DATE>
    <TEXT>\- Henri is up at 12:15 local time with the topic  #8216;IoT Is Broken  #8211; This Is How Peer-to-Peer Networks  amp; Blockchains Will Fix It #8217;. \- The team is at booth S27 for the BCX Hackathon \- The TIOTA Challenge E-Mobility Award winners will be announced at 1:35pm on the Lab Stage</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>lostcivilisation has been created</TITLE>
    <DATE>2019-05-15 10:10:51</DATE>
    <TEXT>A community to discuss the hypothesis, and bring together the complex and fragmented evidence, that suggests we are not the first advanced civilisation on this planet.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"Once data is as open and permissionless for machines as the World Wide Web is for us humans, we can turn the Internet of Things from a dream into a reality." Henri Pihkala on stage today at #BCW19</TITLE>
    <DATE>2019-05-15 12:22:57</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Why does blockchain and IoT fit so well together? Henri on the need for a decentralised approach in machine to machine value exchange at #BCW19 today</TITLE>
    <DATE>2019-05-15 12:34:16</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"Millions of machines happily talking to each other with low latency, trading information, and utilising mircopayments in this new data economy". Final pics from Henri #8217;s talk at Bosch Connected World today</TITLE>
    <DATE>2019-05-15 12:50:25</DATE>
    <TEXT> #8203; https://preview.redd.it/6brmp4tufdy21.jpg?width=4032 amp;format=pjpg amp;auto=webp amp;s=54df7deaaf63fdf2be46fda6a72eae0838a66022  #8203; https://preview.redd.it/fz1d07gxfdy21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=d8e1eadf17dccdacb6857fae2ef123ea00a4f7be  #8203; https://preview.redd.it/npqe8v64gdy21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=4fcd69857f2a62ec2b3f0ddc81fd867439085607  #8203; https://preview.redd.it/ud3tvuwzfdy21.jpg?width=4032 amp;format=pjpg amp;auto=webp amp;s=9f8214a4dffc83b4d22e078f510215a2d1eb88a7  #8203; https://preview.redd.it/orgp1n92gdy21.jpg?width=1600 amp;format=pjpg amp;auto=webp amp;s=3f669ed37eaaaa37c973a2b8a6579d4a5648815d</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128665;  #128225;  #128663; Cars will become the second most important IoT device after the mobile phone. For cities to be truly  #8216;smart #8217;, we need a decentralised layer for real-time data dialog between machines. Welcome to the 'Internet of Cars'.</TITLE>
    <DATE>2019-05-16 13:37:49</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128665;  #128225;  #128663; Cars will become the second most important IoT device after the mobile phone. For cities to be truly  #8216;smart #8217;, we need a decentralised layer for real-time data dialog between machines. Welcome to the 'Internet of Cars'.</TITLE>
    <DATE>2019-05-16 13:38:18</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-16 13:39:47</DATE>
    <TEXT>Great to hear you liked it! I've asked one of the developers who worked on it for more detail to help with your questions and will let you know when i hear back.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-17 09:11:57</DATE>
    <TEXT>Martin, who worked on the pilot directly, shared this in response to your questions: 1. Streamr is developing the underlying technology, the way it is used remains on the people that build on it. In the Machine Witness use case, car data would be sold on Streamr via a community product. Every community product needs and engineer that develops the integration between the data sources and Streamr. It is responsibility of the developer/community product manager either to use a localization anonymisation techniques, or to clearly communicate to the users that the location is not anonymised. Once the ecosystem will emerge, you can even have multiple community products for the same device type and users could decide which one they want to join based on the conditions. Besides that, as the data is being sold on a Streamr, it is always transparent to the users which data is being offered for sale. 2. Buyers of the raw vehicle data in this use case are multiple independent companies applying machine learning algorithms to extract the information from raw data, one use case is the obstacle on road detection. Buyer of the processed data are cars and road side units/road authorities. Prices are yet to be found by offer and demand.  #8203; Henri also left a note that data related to the driving itself might include: highways agencies, governments, car manufacturers, insurance companies ... Cars can also measure things unrelated to driving, and valuable to someone: pollution, mobile network, etc. Hope that helps!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #129352; We're proud to pick up the silver medal for our E-Mobility #tiotachallenges pilot. It was a privilege to work with our partners, Bosch and RIDDLE amp;CODE, to unlock the value of individual vehicle data for a smarter future of transport.</TITLE>
    <DATE>2019-05-17 11:59:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Our community hero for April is  #129345;  #8230; Eric Margay! Well done to Eric for hosting a workshop to spread the word about open source tech and the value in real-time data streams.</TITLE>
    <DATE>2019-05-20 09:17:37</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-20 09:17:57</DATE>
    <TEXT>Pic from the workshop: [https://twitter.com/neowton/status/1122657023592730624](https://twitter.com/neowton/status/1122657023592730624)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Streamr Core. Your toolkit to unlock the potential of real-time data. Try the beta app! https://streamr.com/core</TITLE>
    <DATE>2019-05-20 14:45:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Streamr Core. Your toolkit to unlock the potential of real-time data. Try the beta app! https://streamr.com/core</TITLE>
    <DATE>2019-05-20 14:46:10</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-20 14:53:35</DATE>
    <TEXT>Some context on the launch: [https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c](https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has updated the Editor, unifying data streams, canvases, dashboards, Marketplace products, and purchases into one central location called Streamr Core. See what's new in the app:</TITLE>
    <DATE>2019-05-21 13:03:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Introducing Streamr Core. Your toolkit to unlock the potential of real-time data. Try the beta app! https://streamr.com/core</TITLE>
    <DATE>2019-05-21 13:04:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>People need 3 things to efficiently monetise their own data</TITLE>
    <DATE>2019-05-22 13:40:25</DATE>
    <TEXT>1. People need to be able to **transfer ownership** of their data to someone else. If, for example, you own your house but couldn #8217;t ever transfer the title to anyone else, then the notion of  #8216;ownership #8217; becomes more than a little meaningless. 2. That asset needs to be **discoverable**. To use the house example again, if no one could ever know your house was for sale, then again, ownership loses its power. That #8217;s exactly what [marketplaces](https://marketplace.streamr.com/) are for. But in the case of individually generated data, there #8217;s a twist. This type of data is only really valuable in aggregate form. So individuals need to be able to bundle/unionise/aggregate that data so they can sell it as a crowd. They need their data to be discoverable *en masse*. 3. Data owners also need to **get paid** for their data. This turns out to be the last and perhaps the most technologically complex piece of the puzzle because if your data is only worth 20 cents, the regular fiat banking system isn #8217;t going help solve this problem. And that #8217;s where micro-payments facilitated by cryptocurrencies come in.  #8203; We believe we've developed all 3: [https://medium.com/p/ec032289a51c](https://medium.com/p/ec032289a51c)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-23 13:17:08</DATE>
    <TEXT>Very interesting points. It #8217;s a big topic and something to explore in a blog post in the future I feel. One point to keep in mind though is that the focus is on real-time data, and the value that is created by opening it up for multiple vendors rather than just one at a time. It seems this is an important step for IoT to work and may help push a paradigm shift around how data flows.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-23 13:41:08</DATE>
    <TEXT>Time well tell of course, but we think so, and I don #8217;t know many others who are offering a similar platform that #8217;s decentralised.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build, visualise, and execute your real-time data processes in one place with Streamr Core. Try the app!</TITLE>
    <DATE>2019-05-23 15:15:25</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>New tool for managing and creating real-time data streams from a decentralised platform</TITLE>
    <DATE>2019-05-24 10:25:48</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-24 10:27:02</DATE>
    <TEXT>https://www.streamr.com/core</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Embedded trust is essential for a decentralised data sharing economy to work. That's why we integrated with oracle providers, Chainlink to ensure critical data streams can be verified on demand by smart contract blockchains such as Ethereum.</TITLE>
    <DATE>2019-05-24 15:06:32</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A Firefox plugin to crowdsell your browser data will be the first project backed by the Community Fund. This is what we meant by 'the most exciting use cases will be built by YOU'. Congrats to Ebrahim  amp; his team. See the demo:</TITLE>
    <DATE>2019-05-28 14:56:35</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>What if there was a way to get paid for the data you produce? Thanks Slate Magazine for raising the issues and showcasing our platform for creating data unions.</TITLE>
    <DATE>2019-05-29 12:45:00</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Consolidated crypto market data from Digital Assets Power Play is now available on the Streamr Marketplace! DataStreams provides live data for real-time trades, order books, technical indicators, and more across 5 crypto exchanges.</TITLE>
    <DATE>2019-05-29 13:37:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-30 07:12:30</DATE>
    <TEXT>Here #8217;s the correct link, thanks. https://medium.com/streamrblog/dpp-datastreams-is-available-now-on-streamr-6398b0ded3c</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #10004; #65039; We #8217;ve crossed off all but the new website from our last list,  amp; that #8217;s close! Next community goals ahead for 2019:</TITLE>
    <DATE>2019-05-30 09:40:25</DATE>
    <TEXT>\- Dev education materials \- Core App out of beta \- First version of Community Products \- Integrated Community Product Apps \- First version of the decentralised Network \- An overhaul of the Marketplace features \- Full marketing push for user adoption \- A few surprises! For context, here was the first list from January:  #128467; We #8217;ve got a lot of exciting things coming in 2019:  #8226; New platform user pages  #10004; #65039;  #8226; Editor redesign  #10004; #65039;  #8226; New website with dev docs  amp; user ed [Soon!]  #8226; Community fund  #10004; #65039;  #8226; New integrations  amp; libraries  #10004; #65039;  #8226; Newsletter  #10004; #65039;  #8226; The Trackr app MVP  #10004; #65039;  #8226; A new API explorer  #10004; #65039;  #8226; Community Products  #128064; [on the horizon!]</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128241;  #128225;  #128722; Coming soon to an internet near you  #8230; a way to crowdsell your personal data while retaining control of your privacy. We hope you #8217;re excited for Community Products 2019!</TITLE>
    <DATE>2019-05-31 14:04:01</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-05-31 14:06:02</DATE>
    <TEXT>More info on Community Products:  #8203; * [Introducing Community Products: Making Crowdselling a Reality](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) * [How to Crowdsell Your Information Through a Data #160;Union](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c) * [Crowdselling your personal data through Mozilla #160;Firefox](https://medium.com/streamrblog/crowdselling-your-personal-data-through-firefox-c4f8bf9b8a96)  #8203; Here are a few existing examples of data integrated to the Streamr marketplace:  #8203; * [Consolidated air position streams from over 10,000 locations on the Marketplace](https://medium.com/streamrblog/air-quality-data-streams-from-10-210-locations-in-68-countries-available-via-streamrs-platform-981d52b0ecb3). * [Easily sell Fitbit data](https://medium.com/streamrblog/personal-fitbit-data-sell-streamr-marketplace-blockchain-ethereum-3b32c215660c). * [Send your Spotify playlist information into a data union in real-time](https://spotify.streamr.dev/). * [Sell your Firefox and Chrome browser data demo (coming soon!)](https://www.youtube.com/watch?v=r6mmiMGdyfY)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Ensuring data authenticity and integrity on the Streamr Network</TITLE>
    <DATE>2019-06-03 10:18:16</DATE>
    <TEXT>With data signing, subscribers don #8217;t need to trust the network anymore to be convinced of the authenticity and integrity of the data points they consume. As intermediate nodes cannot be trusted, this signing scheme will be especially important in the Streamr P2P Network as it starts to decentralize.  #8203; [Visual of the Streamr Network ](https://preview.redd.it/k7b60st494231.png?width=2000 amp;format=png amp;auto=webp amp;s=4410f9e4bda569b04da010636a873c4c6350877d)  #8203; Read more from Streamr Security Engineer, Melchior Thambipillai: [https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b](https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Ensuring data authenticity and integrity on the Streamr Network</TITLE>
    <DATE>2019-06-03 10:19:28</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Out designers made a few phone wallpapers for the Streamr teams, we thought you might like them too</TITLE>
    <DATE>2019-06-03 15:17:52</DATE>
    <TEXT>Wallpapers made from designs by our illustrator, Stuart Wade.  #8203; [Brand short](https://i.redd.it/clj4b24cr5231.png) [Brand tall](https://i.redd.it/uvfl8a4cr5231.png) [Comms short](https://i.redd.it/a5dtja4cr5231.png) [Comms tall](https://i.redd.it/mun0wc4cr5231.png) [Dev short](https://i.redd.it/tthbde4cr5231.png) [Dev tall](https://i.redd.it/r1g0cd4cr5231.png) [Products short](https://i.redd.it/o424514cr5231.png) [Product tall](https://i.redd.it/k86imd4cr5231.png)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-04 07:34:32</DATE>
    <TEXT>Stuart is great! Glad you like the illustrations.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>In this talk from last year, Henri Pihkala explains how we incrementally transition from a centralised tech stack to a decentralised platform for p2p real-time data transfer.</TITLE>
    <DATE>2019-06-04 15:07:41</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How best to manage the huge volumes of IoT real-time data predicted to increase in the next few years? We think a decentralised pub/sub network might be the answer</TITLE>
    <DATE>2019-06-05 15:01:42</DATE>
    <TEXT>IoT connected devices are predicted to [increase fivefold](https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/) in the next 10 years, each generating a [huge volume of real-time data](https://www.information-age.com/data-management-in-the-internet-of-things-123480691/). How and by whom this data is managed and used is an important consideration. In the current model, developers of applications can either roll their own infrastructure or resort to centralised cloud services providing the needed messaging and storage functionality. This makes it expensive, limiting innovation, and means most of the data in the world ends up passing through a handful of internet giants. We believe a decentralised pub/sub layer is needed to remove these barriers and ensure that the world #8217;s real-time data is used and controlled by those who produce it. The Streamr Network is a scalable realtime messaging system, which enables applications and devices such as IoT sensors, connected cars, and basically all  #8220;smart #8221; gadgets to make available the data they are producing, as well as listen to incoming data from other applications and devices. The Network employs the publish/subscribe pattern. Messages produced to a stream (sometimes called a topic) get delivered to all subscribers listening on that stream in realtime. It #8217;s a bit like an instant messenger for machines, which #8202; #8212; #8202;in IM lingo #8202; #8212; #8202;supports  #8220;group chats #8221; (many-to-many),  #8220;channels #8221; (one-to-many), as well as  #8220;private #8221; (one-to-one) messaging patterns. Such a publish/subscribe network has the following great properties for application developers: * Data producers simply  #8220;fire and forget #8221;. No need to set up APIs, data silos, or open ports that could compromise security. Just send new data points to the Network and you #8217;re done. This makes integration very easy. * Data consumers simply listen. They don #8217;t need to know where the data sources are, which IP address to reach them on, or how to interface with them. They don #8217;t need to open server ports either. They just connect to the network, subscribe to what they need, and react to incoming messages. Read more about the Streamr Network: [https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Thanks to ConsenSys for describing us as a "remarkable team"  #128591;. We've taken no prisoners in May. Expect much the same in August, Sept and Oct as we deliver the first versions of our Network  amp; our tools for creating data unions - Community Products.</TITLE>
    <DATE>2019-06-06 16:11:18</DATE>
    <TEXT>Web 3 teams ranked in terms of dev activity on Github in the past month:  #8220;... Below are the top 20 projects on State of the Dapps by developer activity over the past 30 days as of June 4th, 2019  #8230; 15. STREAMR an open-source platform for data marketplaces, with a flagship market of its own, Streamr is taking on a huge pain point. Its platform, already in Core Beta, offers data on everything from trams in Helsinki, air quality in Nantes, to personal Google Fit data for sale. And with a 42% jump to 1788 commits on Github last month, expect more from this remarkable team. #8221; Check out the full post: https://media.consensys.net/20-blockchain-projects-with-the-most-dev-activity-on-github-may-2019-6ce07e3c866f</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-06 20:28:04</DATE>
    <TEXT>Yes I meant  #8220;our #8220; haha</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Community Beta testing for Streamr Core completed, MQTT broker data bridge created, and end-to-end encryption in testing and ahead of schedule for the Network. A productive May for the dev community!</TITLE>
    <DATE>2019-06-07 11:47:59</DATE>
    <TEXT> #8203; [After many months of development, the Core App \(beta\) has been launched to support data analytics and prototyping](https://preview.redd.it/vtkz3afn8x231.png?width=2000 amp;format=png amp;auto=webp amp;s=5792f9531d1a973091598e9077d3c0377d6df492) Welcome to Streamr Core Developers Update for May. It has been an exciting and hectic month in equal measure. You see that from the long changelog at the end of this blog post. But it #8217;s also something which, going by the number of Github commits we racked up, [ConsenSys](https://media.consensys.net/20-blockchain-projects-with-the-most-dev-activity-on-github-may-2019-6ce07e3c866f) and [Blockcast Labs  amp; Research](https://twitter.com/BlockcastLR/status/1135400523882885120)also made note of. Our first beta testing event for Streamr Core went well. We gathered some decent feedback from users and they helped uncover remaining bugs. It was heartwarming to see community members volunteering their time and effort to improve the platform for everyone. If you haven #8217;t tried the now deployed version, do [go and experience it firsthand](http://www.streamr.com/). You can also read this [blog post](https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c) by Chief Design Officer [Matt Innes](https://medium.com/@matt_innes), which gives a run down of the major feature updates. Just a reminder Core (a merger of the old Editor, other parts of the Streamr stack and entirely new elements) is still in beta as there are a few remaining bugs to be ironed out and a number of suggested features the Core team would like to implement. Any feedback for improvement or [bug reports](https://github.com/streamr-dev/streamr-platform/issues)are always welcome.  #8203; [Fluid drag and drop experience for modules on Streamr #160;Canvas](https://preview.redd.it/debbx9uaax231.png?width=1628 amp;format=png amp;auto=webp amp;s=1dba25c53e4475e773eef9d75bca729dc8f38089) On the Network side the core team has implemented a light adapter to bridge data from a MQTT broker to Streamr. The adapter supports stream auto-creation based on MQTT topics. With this it #8217;s very simple to bridge data from, for example, HSL MQTT servers or Ubikampus MQTT servers. This means any company or developer already familiar with, or having running MQTT brokers, can now easily integrate with Streamr. In case you don #8217;t know what MQTT is, here is a [short overview](https://www.ably.io/blog/why-is-everyone-talking-about-mqtt):  gt;*Message Queuing Telemetry Transport (MQTT), is a network protocol especially built to enable efficient communication to and from Internet of Things (IoT) devices.*  gt;  gt;*Additionally, unlike the most popular networking protocol HTTP, MQTT does not follow the request/response mechanism for communication. Instead, it uses the publish/subscribe paradigm (popularly known as Pub/Sub).*  #8203; [MQTT broker flow](https://preview.redd.it/4cs95c6w8x231.png?width=1520 amp;format=png amp;auto=webp amp;s=f7cf1c70b6c13061af369d3f5aec0246d0e46c81) Source: [Why Everyone is Talking about #160;MQTT:](https://www.ably.io/blog/why-is-everyone-talking-about-mqtt) The good news gets better. Following the [introduction of data signing](https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b) to ensure chains of messages can be made tamper-proof, Streamr core developers were also able to implement an initial version of end-to-end encryption for the data transferred over the Streamr Network! It is currently undergoing internal testing and the tentative plan is to now roll it out with the Milestone One release, instead of M2 as initially planned. This means that as soon as the first version of decentralized Streamr Network is out, streams will be encrypted end-to-end, preventing man-in-the-middle attacks or data tampering. That brings us us ever closer to the world #8217;s first p2p pub/sub service Network for real-time optimised for machine data. As always, thanks for reading. Here #8217;s our regular list of updates: ## Network * Add missing field msgChainIdto resend requests * For resend responses, return Node.js (Readable) streams instead of relying on listening to events * Migrate auto-detect fields functionality from data-api * Use NetworkNode for resend requests in HTTP adapter * Verify that resend requests via websocket adapter and HTTP adapter work * Skeleton for MQTT adapter * Starting to refactor the protocol in the p2p network ## Ethereum * Community product server with functional API * Community product admin fee logic initial implementation * Testing with Etherlime integration ## API  amp; #160;Clients * End-to-end encryption with symmetric group key in JS and Java clients (still in testing) * Streamr-client-protocol-js: Fix bug with not working in unauthenticated mode * Streamr-client-protocol-js: use null instead of empty string as an indicator of lack of value in resend requests * Add /api/v1/canvases/downloadCSV API to allow CSV download * Expose streamService.deleteAllData/deleteDataUpTo/deleteDataRange to StreamApiController * Add PUT endpoints for Stream Resource Key and User Api Key * Fix NPE in Stream status endpoint * Domain object changes for Community Product * CommunityJoinRequest endpoints and secrets API * Published first official version of the Java client to Maven Central * Version 31 of the StreamMessage (support for encryption) * JS implementation of the v31 of the protocol published to npm * Java implementation of the v31 protocol published to maven (snapshot) * Usage of the v31 protocol in data-api, cloud-broker and engine-and-editor * Finished resize constraint implementation for modules. Modules have min. widths and heights now. User can #8217;t make them smaller than the limit * Updated core app navigation for more consistency and readability * Fixed select input (module port control) placeholder to be visible even if the value is blank * Fixed redirection from the old routes for reset password and register pages * Added custom styling to Map #8217;s zoom in/out controls * Allowed \`redirect\` GET param to link to a page that doesn #8217;t require a logged in user * Fixed login page flashing on every refresh when the user is actually logged in * Added  #8220;Give feedback #8221; link to the Onboarding menu * Fixed ethereum login. The browser, instead of failing silently, asks the user for proper permissions now * Fixed Map module resizing. The Map component respects the dynamic size of its parent now and gets redrawn when necessary * Improved canvas/dashboard previews. This includes nicer previews for Table, Chart and Comment components and simplified code in general * Made the logout wipe out user #8217;s information to avoid data spills between sessions ## Core App (Engine, Editor, Marketplace) * Quick fix SolidityCompileDeploy module * Put in a year limit to /api/v1/streams/:id/status endpoint to prevent timeouts * Update Cassandra driver to fix problem with dropping connections * Write up quick script to calculate module usage statistics * Publish common eslint-config for Streamr #8217;s vanilla node.js projects and start using it in Broker * Deploy new version of ISS Space Streamr hackathon project * Allow saving and loading of canvases even if they are in erroneous state * Fix module search visibility bug and center SearchPanel vertically * Add support for retina tiles in Map component * Add autoZoom (still work in progress) to better canvas view/management * Toolbar styling for shared canvases * Stream edit view permissions * Gracefully handle embed urls as initial Core Beta didn #8217;t handle it properly * Fix ExportCSV module to use new API endpoint * Optimize Map rendering performance * Add correct texts for mobile empty states in the userpages * Fix  #8220;Create #8221; buttons to handle links better * Improve module search result prioritization * Fix async and state handling issues in stream selector * Fix async  amp; state handling issues in Java module * Fix module exception handling so crashed modules can be deleted * Frontend workaround for moduleErrors/initialValue null issue * Show streamr app version + branch in sidebar * Allow clicking reset and start button even if it does nothing * Fix empty sidebar visible after module deleted * Add hash to bundle asset file names * Significantly improve module drag  amp; drop performance * Redirect to 404 on canvas/dashboard not found * Fix trying to autosave deleted dashboards or canvases * Prevent  #8220;cannot read id on undefined #8221; exception on bad canvas/dashboard load * Investigate Unpublish popup comes up randomly when trying to publish a product * Finalise stream list page * Fixing bug regarding uploading CSV file does not work * Permanently delete item modal improvements * Refactor code editor to be resizable * Fix login for disabled users ## DevOps * Reduce memory footprint of Cassandra in streamr-docker-dev environment * New deployment flow for Streamr Core beta deployment * New Nginx configuration for production * Create self-deployment mechanism for production *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community run* [*dev forum here*](http://forum.streamr.dev/)*. Be sure to give the original* [post](https://medium.com/streamrblog/dev-update-may-2019-83992de3975d) *a clap on Medium too.*  #8203; Thanks as always for your support!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Ben Sheppard, Head of Partnerships at Streamr, is at Sea Web Seafood Summit this Wednesday discussing how to turn a flood of supply chain data into reliable information for seafood traceability.</TITLE>
    <DATE>2019-06-10 12:15:22</DATE>
    <TEXT>Here's the topic of the [conference](https://www.seafoodsummit.org/) panel [talk](https://www.seafoodsummit.org/session/true-facts-or-fake-news-verification-challenges-for-a-data-rich-future/):  #8203; "With the rapid emergence of powerful new technologies, the availability of data about how fish products are made is poised to grow explosively. But is technology really the silver bullet we #8217;ve all been waiting for? How quickly (and equitably) will reliable eFM and IT solutions become available across the wide variety of fisheries globally? What does it take to turn a flood of data into authoritative, useful information? How can NGOs, industry stakeholders, and governments help ensure that human systems are ready to promote the reliable and fair digitization of seafood? These questions are especially relevant as more than five dozen leading seafood companies intensify their work through the Global Dialogue on Seafood Traceability (GDST) to adopt voluntary industry norms for the verification of  #8220;key data elements #8221; in interoperable traceability systems. This interactive expert panel will offer SWSS attendees an opportunity to get updated, give input, and raise questions as the GDST seeks to describe best practices for verification of digital supply chain data in the rapidly evolving seafood sector."  #8203; Ben's [profile](https://www.seafoodsummit.org/speaker/ben-sheppard/):  #8203; "Ben Sheppard is a serial entrepreneur and start-up specialist. He previously founded two successful companies specialising within infrastructure, agriculture and fisheries and acted on behalf of clients to lead public-private investment deals worth up to USD 9BN. He currently has a dual role in Streamr AG Group. He is both Head of Partnerships within the Streamr Project, where he leads a team accountable for gaining adoption of the technology stack and, MD of Streamr Business Unit, which offers private instances of the Streamr Stack and wider software solutions."  #8203; Event hashtag: [\#swss19](https://twitter.com/hashtag/swss19?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Ehashtag)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Official Streamr Java library version 1.0 is out! It lays the groundwork for connecting data to all the Java-based analytics frameworks including Apache Spark, NiFi, Storm, Druid, and others. Available now via Maven Central.</TITLE>
    <DATE>2019-06-10 14:25:41</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Official Streamr Java library version 1.0 is out! It lays the groundwork for connecting data to all the Java-based analytics frameworks including Apache Spark, NiFi, Storm, Druid, and others. Available now via Maven Central.</TITLE>
    <DATE>2019-06-10 14:26:03</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Imagine a world where your car sells the data it produces and purchases the data it uses, p2p, in real-time, without a centralized third party.</TITLE>
    <DATE>2019-06-11 14:20:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Imagine a world where your car sells the data it produces and purchases the data it uses, p2p, in real-time, without a centralized third party.</TITLE>
    <DATE>2019-06-11 14:21:49</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-11 14:22:36</DATE>
    <TEXT>Thanks for sharing this. Passed it on to the team.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Here's a shot of Ben Sheppard, Head of Partnerships at Streamr, delivering a talk on the data-driven future of seafood at the Sea Web Seafood Sustainability conference in Thailand today</TITLE>
    <DATE>2019-06-12 11:12:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Labs has created publish and subscribe processors for Apache NiFi. Connect your streams and integrate real-time data to multiple systems at scale without the typical data flow challenges.</TITLE>
    <DATE>2019-06-12 14:33:32</DATE>
    <TEXT> #8203; https://preview.redd.it/0ykyht34sx331.png?width=2600 amp;format=png amp;auto=webp amp;s=866e40f74d11e9f497f7235a56810054ecf98be0 When building larger architectures, especially those focused on data flowing from system to system such as IoT, developers are likely to encounter issues with maintainability and scalability. You may even need to write a script to connect and integrate different systems. This problem is only exacerbated in architectures where data formats differ between systems, devices and applications. Apache NiFi is a tool built specifically to address these and many more issues concerning data flows. Here at Streamr Labs, we have created publish and subscribe processors for Apache NiFi. Connecting your streams to NiFi allows you to connect and integrate real-time data to different systems with little to no effort. One great use case for these processors is data analysis with Apache Spark. The ExecuteSparkInteractive processor provided out of the box in NiFi allows you to inject code to PySpark to refine your data. #### What is Apache #160;NiFi? [Apache NiFi](https://nifi.apache.org/) was built to ease the automation of data flows between systems. NiFi makes understanding and modifying data flows in even larger architectures easier through a drag and drop user interface; NiFi #8217;s design closely relates to the main ideas of Flow-Based Programming. However, NiFi doesn #8217;t just offer a UI to create connections between systems, it has multiple features that make it a viable option as a backbone of a data flow architecture. Some of its most important features are: * Data buffering with back pressure and pressure release * Encryption with 2-Way SSL * Horizontal scalability (clustering) * And (most importantly for Streamr Labs) extendability. This is what allows anyone to build a custom processor for NiFi. You can read more about NiFi and its features in its official [documentation](https://nifi.apache.org/docs.html). NiFi also has a subproject called MiNiFi that is more suited to be used in the edge of an architecture. For example, MiNiFi can be set up in a Raspberry Pi to create a light data flow that processes and pushes forward sensor data in an IoT architecture. Where NiFi is built on Java, MiNiFi has a Java version and a C++ version for lighter data flows. Streamr has also [integrated with Node-Red](https://medium.com/streamrblog/streamr-node-red-integration-tutorial-b0b410496354) #### How does Apache NiFi differ from the Streamr stack #160;? Comparing Apache NiFi to the Streamr stack is like comparing apples to oranges. There are similarities: they are both open source and utilise flow-based programming models. They also operate on top of Java and ingest data. But as solutions, they serve different purposes. Apache NiFi is an open source enterprise-grade data flow automation or ETL tool designed to automate the flow of data between software systems. The Streamr stack could be seen in this context as a holistic data processing stack that has generalised data ingestion capabilities, a data transportation layer, [data analysis tools](https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c) and a [data Marketplace](https://marketplace.streamr.com/). One could say that these two solutions operate on different markets and segments but the differences between Apache NiFi and Streamr can be beneficial. With the Streamr processor, Apache NiFi users can easily integrate their existing information systems and data to be monetised on the Streamr data marketplace. #### Getting started with the Streamr processors on #160;NiFi If you haven #8217;t installed Apache NiFi yet, you can find the guides and downloads for it [here](https://nifi.apache.org/docs.html). After you have Apache NiFi installed, the easiest way to get started with the Streamr processors is to: 1. [Download](https://github.com/streamr-dev/streamr-nifi/blob/master/nifi-StreamrNifi-nar/target/nifi-StreamrNifi-nar-1.0-SNAPSHOT.nar) the #160;.nar file for the processors 2. Copy the #160;.nar file to NiFi #8217;s /lib directory. The lib directory is where all the processors #8217; #160;.nar files are stored. The lib directory is most likely at NIFI\_HOME/lib or NIFI\_HOME/libexec/lib. 3. Restart NiFi 4. See if the professors were correctly added to NiFi by dragging the add processor icon to the flow, and checking if StreamrLabs were added to the Source drag down menu. 5. The processors should be found under the StreamrLabs source. If you cannot see the processors make sure that you added the #160;.nar file to the correct directory. The StreamrSubscribe processor allows you to subscribe to real-time data in Streamr and transfer the data onwards to a NiFi flow. The StreamrPublish processor allows you to publish data to Streamr from a NiFi flow. Here is an example flow showing how to use the processors. You can try out the example flow yourself by importing [this](https://github.com/streamr-dev/streamr-nifi/blob/master/docs/Example.xml) #160;.xml file to your NiFi flow. You can also see how the processors and controllers are configured in the example.  #8203; https://preview.redd.it/j1rbcb91sx331.png?width=1600 amp;format=png amp;auto=webp amp;s=96b60e2b005b00ff003db3cbaf3ab78725868079 The processors use a single line, single entry JSON string as their data format. { #8220;string #8221;:  #8220;string #8221;,  #8220;number #8221;: 123123,  #8220;list #8221;: \[123, 123\]} The JSON #8217;s can be easily converted to / from other formats using NiFi #8217;s out of the box processor ConvertRecord. Simply configure the processor to have a JsonTreeReader if you want to read JSONs or JsonRecordSetWriter if you want to write JSONs. JsonRecordSetWriter should be configured to output only one line per object as the StreamrPublish processor only processes and publishes one entry at a time. Go to [https://github.com/streamr-dev/streamr-nifi](https://github.com/streamr-dev/streamr-nifi) for more information on how to get started and how to use the Streamr processors.  #8203; Read the [full article](https://medium.com/streamrblog/apache-nifi-streamr-processors-5822da9b1820).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Labs has created publish and subscribe processors for Apache NiFi. Connect your streams and integrate real-time data to multiple systems at scale without the typical data flow challenges.</TITLE>
    <DATE>2019-06-12 14:35:12</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>You will soon be able to crowdsell your Firefox browsing data on the Streamr Marketplace. Sign up today to beta test the plugin from Ebrahim and his team:</TITLE>
    <DATE>2019-06-13 12:18:52</DATE>
    <TEXT> #8203; https://preview.redd.it/fhfu689o84431.png?width=1435 amp;format=png amp;auto=webp amp;s=5e58d4dfc3f34db1fb463a9aa779aef964adce35 As you may know, our community member Ebrahim and his team have been working very hard in the past few months to build a [fantastic Streamr integration](https://medium.com/streamrblog/crowdselling-your-personal-data-through-firefox-c4f8bf9b8a96) that allows users to sell their browsing data with a simple installation of a browser plugin (available in both Firefox and Chrome versions). What used to be a helpless situation, where your only choice was giving up full control of your data to use a service, now has a solution; this project aims to give back at least part of that control. The plugin allows users to collect the data themselves and decide to participate in revenue sharing by contributing it to an aggregated Streamr Community Product. If you want to see the journey this project took, [**this thread**](https://forum.streamr.dev/d/39-firefox-extension/19) might be an interesting read. Also [**this blog post**](https://medium.com/streamrblog/crowdselling-your-personal-data-through-firefox-c4f8bf9b8a96) offers a quick overview on the impact this project can have and a short video presentation of the plugin from its team (note that some additional features have been added since then). What makes this project even more interesting is that it gives users total control on the level of privacy they feel comfortable setting for their data sharing, like time delay before data events are sent to a stream, in-text masking for sensitive info, rotating generated user ID based on user preference (generate a new random ID every day, hour and for each new data event).  #8203; https://preview.redd.it/dlmkktqq84431.png?width=800 amp;format=png amp;auto=webp amp;s=d253978ced9a80016a124d2ac443c246ff140a03 If you are interested in joining a private beta testing of the plugin, please follow this [**link to signup**](https://docs.google.com/forms/d/e/1FAIpQLScXHTPRskdFqz2QwKTuoBhPIEs-5ZwCl3jbG61GAKkI3e_OoA/viewform?usp=sf_link). The plan is to kick-off toward the last week of June, we will be emailing everyone who signed up with additional details on plugin installation. Few quick notes: * This is early beta testing, so expect some bugs and slightly rough UI * All data will follow your level of privacy setting and anonymization before being sent to the shared stream, I have personally tested it * There will be 6 modules to capture different types of data (such as Amazon, Facebook, Twitter, Search Engines, Youtube) but you are free to choose to test any of them. Only Search Engine module will be enabled by default to increase user privacy, so you can actively decide to enable other modules. * The aggregated stream will be for testing purpose only so it will be free and not monetized during beta * As consequence of point above, there is no reward mechanism for users during beta phase, but I know a lot of you are super excited to test out the product  #128247; * Join and be part of something revolutionary!  #8203; [Read it on the dev forum!](https://forum.streamr.dev/d/50-call-for-beta-testers-firefox-and-chromium-plugin-private-beta)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-14 15:31:10</DATE>
    <TEXT>We worked with Bosch on the TIOTA pilot, but I think they are working with a number of other Blockchain based projects too.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #8220;In the modern world, machines need to talk to each other  amp; they need to do that over an open source protocol. Otherwise, what's the point in having smart cities if those IoT devices aren't having conversations with them? #8221; - Shiv speaks Streamr with Startups Magazine</TITLE>
    <DATE>2019-06-17 15:33:26</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr #8217;s community hero for July is  #129345;  #8230; Marco Giretti! Thanks to Marco for your excellent feedback testing the Streamr Core beta.</TITLE>
    <DATE>2019-06-18 12:40:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-18 12:41:37</DATE>
    <TEXT>Check out the release notes and see what's new ahead of the full release: [https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c](https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A community-built plugin for you to crowdsell your Firefox browser data in real-time will begin beta testing next week. Sign up here to be the first to try the data union!</TITLE>
    <DATE>2019-06-19 14:52:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>You will soon be able to crowdsell your Firefox browsing data on the Streamr Marketplace. Sign up today to beta test the plugin from Ebrahim and his team:</TITLE>
    <DATE>2019-06-19 15:45:00</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>You will soon be able to crowdsell your Firefox and Chrome browsing data on the Streamr Marketplace. Sign up today to beta test the plugin:</TITLE>
    <DATE>2019-06-19 18:22:21</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-20 06:13:24</DATE>
    <TEXT>Hard to give exact figures as much of this will depend on the demand for it and how many data sets you #8217;re streaming. Likely not a lot relatively speaking. It may be possible for users to collectively pool their earnings for a common cause though, in which case it can build to quite a lot. This is a step towards an alternate model that sits between shutting out all data and handing it all over. I think that #8217;s a better approach and offers a healthier relationship for personal data and services. It seems there is a growing demand for this kind of model but we #8217;ll see.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #127925;  #128225;  #128722; The Spotify Community Products demo has a lot of music data flowing through it. Launch a canvas with Streamr Core and see what 1,500 songs the community listens to each week:</TITLE>
    <DATE>2019-06-20 13:29:15</DATE>
    <TEXT> #8203; [Streamr Core Editor Showing Spotify Data #160;Union](https://preview.redd.it/ma412ujrji531.png?width=1914 amp;format=png amp;auto=webp amp;s=84a516ba679a1d2c6a85c7ee4b4ed7ba7f76e957) Designed by a member of our extended developer community, the demo is pretty simple. The idea is that by authorising a third party app to read their Spotify data, people can send the title of the tracks they play, along with album and artist information, to create a real-time data product that, for example, [economists](https://www.bbc.co.uk/news/business-43960998) from the Bank of England might be interested in purchasing. You can try it yourself if you like. All you need is a Spotify account.  #8203; [Weilei Yu, Head of DevRel, manning Streamr #8217;s stall at RadicalxChange](https://preview.redd.it/u9rwhnpsji531.jpg?width=1280 amp;format=pjpg amp;auto=webp amp;s=7716ae0c5fda6c108c4770bce39df3919b950a51) Simply scan the QR code below (or click this link [https://spotify.streamr.dev](https://spotify.streamr.dev/)) and after clicking the HTML link that pops up, you will be redirected to Spotify #8217;s interface. Spotify will then ask your permission to authorise a third party app. Press accept and that #8217;s it! You #8217;re now part of one of the world #8217;s first data unions.  #8203; https://preview.redd.it/huxtatxtji531.png?width=300 amp;format=png amp;auto=webp amp;s=f9ca7e2165517621df1afc84bc2496117a7bfb0b To test it, just play a song on Spotify. You can view a [live demo page here](https://spotify.streamr.dev/data), which will show you all data being contributed by users around the world in real-time. If you have a registered [Streamr](https://www.streamr.com/) account, which is free, you can see the canvas shown at [following link](https://www.streamr.com/canvas/editor/-HZBN3-0TVu9ec3L7zmtLwFgw6lJ0TTA-DBnkxtyb1Rg). (Please note: this is a very basic test at the moment and although this particular data set is publicly viewable, it won #8217;t be put on sale). So where does this go from here? Well, Streamr core devs are currently working on the user interface (UI) for Community Products and ensuring that the backend, including the process for payments, functions smoothly. A first workable version of Community Products should be released during Q3. You can read [more about the general architecture](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) and timelines here. [Read the full article about crowd selling through data unions](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has partnered with Elk to provide developers of IoT devices with an easy way to aggregate and monetise sensor data. Let's open up those data silos and fuel smarter cities!</TITLE>
    <DATE>2019-06-20 15:38:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Monoplasma is an off chain scaling solution built for one-to-many payments. Clone the repo and experiment with it yourself from the Streamr GitHub</TITLE>
    <DATE>2019-06-21 09:02:41</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-21 09:03:55</DATE>
    <TEXT>[Monoplasma](https://github.com/streamr-dev/monoplasma) is a special-purpose off-chain scaling solution for one-to-many payments. It #8217;s a good fit for any system where you repeatedly need to:  #8203; 1. Distribute value to a large and dynamic set of Ethereum addresses. 2. Allow recipients to accumulate value over time. 3. Withdraw tokens at their preferred moment. We built Monoplasma because we needed it for the upcoming [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) feature on the [Streamr Marketplace](https://marketplace.streamr.com/). Community Products will allow end users who produce data through a connected gadget, [like a Fitbit](https://medium.com/streamrblog/personal-fitbit-data-sell-streamr-marketplace-blockchain-ethereum-3b32c215660c), or a Tesla, to pool and sell data collectively. And when that conglomerated data product sells, the revenue will then be automatically distributed out to potentially hundreds of thousands of data contributors. The Monoplasma framework is reusable and un-opinionated, and as such, it #8217;s a piece of software that might help others too. That #8217;s why we wanted to make it standalone as a (hopefully useful) contribution to the Ethereum scaling space. Monoplasma lives in [this repo](https://github.com/streamr-dev/monoplasma) within the [Streamr Github](https://github.com/streamr-dev). It ships with an interactive revenue sharing demo which showcases the functionalities of the framework. Feel free to clone the repo and play around with it yourself (as others already have done!). Instructions for running the demo are in the readme. You can also watch the demo as Henri presented it live at [ETHDenver](https://www.ethdenver.com/) in the video below. [Check out the full post](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128739; Lower insurance, better roads, less congestion: re-read the use cases for how the Streamr Network can be used to address the real-time data limitations in the transport industry:</TITLE>
    <DATE>2019-06-24 13:53:19</DATE>
    <TEXT> #8203; [Recent pilot built as part of the TIOTA mobility challenge showing vehicle data in action](https://reddit.com/link/c4o8g0/video/u2pqofad7b631/player) The Streamr Partnerships team has identified three use cases, which have been subsequently validated with experts in the transport and smart cities sectors. They involve gathering various types of realtime data from vehicles. This data is then streamed via the [Network](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca), our realtime data transportation layer and made available for sale to data consumers on the Streamr Marketplace. The Network will be decentralized in stages over the coming years, creating a vastly more scalable, low latency, P2P infrastructure for the IoT-Smart City data economy. Because it will benefit from the utilisation of sunk cost computing hardware, it will operate at a much lower price to the user than solutions currently provided by incumbents. Streamr #8217;s soon to be released Community Product feature for the Marketplace will also further the development of data products in a revolutionary way. For the first time, data buyers will be able to bulk subscribe to streams of data originating from many multiple and independently owned data producing gadgets such as home sensors, wearable IoT devices and vehicles and directly remunerate those owners. For the data buyer this will facilitate purchasing multiple and disparately owned data products in a  #8220;one click #8221; fashion. For the individual and independent data producers of a gadget, the Community Product will bring them the opportunity to bundle their data with others to create more value for their individual stream and be directly paid for their own data. This pattern of individual and independent data streams bundled by gadget type, geographic location or other descriptors, into one product and then made available for sale, will form the bedrock of the new Smart City data economy. # Road Surface #160;Quality Potholes and poor road surface quality on highway networks causes damage to vehicles and can lead to severe accidents. According to [a 2016 AAA study](https://www.oregon.aaa.com/2016/02/pothole-damage-costs-u-s-drivers-3-billion-annually/), the damage to vehicles caused by potholes cost US insurers $3bn per year. The UK government will spend over  #163;200m in 2018 trying to [combat the problem](https://www.gov.uk/government/news/pothole-fund-boosted-to-repair-roads-after-winter-damage). So how will Streamr make a difference? Through Streamr it is possible to stream realtime road surface data from road users #8217; vehicles. By making this data available, it may be possible to better understand the causes of potholes, predict when they will appear, and create more innovations around the material used for constructing the pavement. The Streamr Network can connect directly with sensors in vehicles or via the OEM database to gather anonymized data concerning: road surface imagery, acceleration, suspension and location. Using this data, a  #8220;road surface quality #8221; Community Product will then be created in the Streamr Marketplace, which interested parties could subscribe to and run analytics on via the Streamr Editor tool. They can also download the data themselves. One example of an interested subscriber might be an AI company. An AI company can label the data and curate it into a refined product suitable for contractors to use in their maintenance regimes. Whenever a party subscribes to the product, the road users  amp; OEMs would be rewarded in cryptographic tokens called DATA. Like other cryptographic tokens, DATA can be exchanged for fiat currency or other crypto tokens like Bitcoin. By allowing data purchasers to combine road surface data products from multiple data producers, Streamr takes this data offering to a new level. For example, on any particular day, every single road user on the M25 motorway that encircles London could be streaming their realtime data to the Streamr Marketplace where it could be sorted into an  #8220;M25 Road Surface Quality #8221; community product. Every road user and the various OEMs participating in the product would be rewarded with a share of the data tokens whenever a third party subscribes/ purchases to the product. Having these more sophisticated data products available could lead to greater innovations in road standards and developments in the materials used for constructing the road surface. # Improved driver behaviour leading to lower insurance premiums Vehicle insurance companies base their quotes on limited data. This sometimes leads to misaligned premium pricing. Streamr is working on solutions to overcome this challenge by enabling road users to stream realtime data of their driving behaviours. Streamr is currently designing a pilot that would involve gathering various in-vehicle datasets, which could be used to determine how well a road user is driving their vehicle and the level of risk created during a trip. The pilot would aim to gather data on: * Lane discipline * Speed limit discipline * Abrupt braking * Vehicle occupancy By making this data available to insurance companies, participants could be rewarded with lower premiums for better driving behaviour. # Traffic models Streamr has [partnered with decentralized location data market Fysical](https://www.econotimes.com/Joint-venture-of-blockchain-backed-data-companies-Streamr-and-Fysical-to-trace-real-time-aggregate-location-for-shaping-traffic-models-1430107) to use realtime aggregated location to more accurately inform and shape traffic models. The collaboration will leverage data from mobile apps on 10 million phones to enable the development of new and improved infrastructure in an industry that has largely relied on limited information. Through its collaboration with Fysical, Streamr demonstrates a significant use case of how data can be collected, modeled, and used to improve the delivery of transport planning services within the infrastructure industry. Streamr has procured [Bold Native Advisors Ltd](http://boldnativeadvisors.com/) (Native), a global multidisciplinary consulting firm, to build a traffic model of the nation of Georgia using aggregated location data obtained from apps on iOS and Android phones. The pilot aims to build a new demand matrix for a traffic model, with Georgia selected due to the fact that in 2017, Native built a model in Georgia on behalf of the World Bank, using traditional datasets and roadside interviews. Through its data Marketplace, Streamr is rectifying the existing roadblocks to efficiently develop traffic models. Among other features, the Marketplace enables national transport authorities to use vast amounts of traffic-related data that was previously difficult to access. Together Streamr, Fysical and Bold Native are pioneering a real-world use case that highlights the benefits of recent historic information in the development of infrastructure. Check out the [full post](https://medium.com/streamrblog/unlocking-the-benefits-of-vehicle-data-solving-data-limitations-in-the-transport-industry-a3e7b7f78355) from last year.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Chief Fisheries Analyst, Bradley Soule reflects on the recent panel discussion about the best practices for the verification of seafood supply chain data with Streamr's Head of Partnerships, Ben Sheppard at SeaWeb Seafood Summit.</TITLE>
    <DATE>2019-06-25 12:51:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A note on team news, Carl Rodrigues has moved on from working with Streamr full time, but he will still be involved in projects on an ad hoc basis. I hope you'll join me in thanking him for his work with Streamr and wishing him the best in his next challenge</TITLE>
    <DATE>2019-06-26 14:11:26</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Have you tried the updated Streamr API explorer? Use it to test any Streamr Restful API endpoint live in the browser, authenticate with your API key, generate SDK code in multiple languages,  amp; experiment with code examples</TITLE>
    <DATE>2019-06-26 15:21:16</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Have you tried the updated Streamr API explorer? Use it to test any Streamr Restful API endpoint live in the browser, authenticate with your API key, generate SDK code in multiple languages,  amp; experiment with code examples</TITLE>
    <DATE>2019-06-26 15:57:56</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-06-27 13:23:58</DATE>
    <TEXT>The transmissions were replaced with the packet switch newsletter for more regular updates.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Raw real-time data needs to be aggregated, filtered, or combined to extract value for consumption by Dapps and smart contracts. That's where the Engine comes in. The backbone of Streamr's platform.</TITLE>
    <DATE>2019-06-27 14:31:50</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Rob Holmes, Streamr's Key Account Leader, discusses the recent TIoTA pilot, and why a more collaborative environment for data exchange is needed for technical advancements like autonomous vehicles at CognitionX.</TITLE>
    <DATE>2019-06-28 12:54:41</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Rob Holmes, Streamr's Key Account Leader, discusses the recent TIoTA pilot, and why a more collaborative environment for data exchange is needed to power machine learning for technical advancements like autonomous vehicles at CognitionX.</TITLE>
    <DATE>2019-06-28 12:55:43</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can creating an incentivised data sharing economy lead to outcomes like more sustainable fishing practices? Streamr #8217;s Head of Partnerships, Ben Sheppard, shares his thoughts on return from SeaWeb Seafood Summit.</TITLE>
    <DATE>2019-07-01 15:01:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>There #8217;s been a lot of questions on partnerships status recently, so please consider this a general update (shared on Telegram last week)</TITLE>
    <DATE>2019-07-02 08:32:51</DATE>
    <TEXT>The Smart Citizen, Nokia and OSISoft relationships are currently are on hold. Although initial attempts to produce a suitable use case proved unsuccessful, that does not mean these relationships may not bear fruits in the future. Finding pilots that deliver mutual benefits takes time and it doesn #8217;t always happen in a straight line. It is often about timing. The team speaks periodically with all partners and continues to update them as the stack develops. As we make progress towards becoming decentralised, based on feedback from a number of major enterprises, it #8217;s anticipated that new pilots and opportunities will emerge with the organisations like the ones mentioned above. Streamr #8217;s relationship with HPE continues to progress fruitfully but is strictly under NDA. It is therefore not possible to divulge any specific information at this time. There is work in progress with other enterprises but the relationships and names of those organisations are also currently under NDAs. This is because enterprises are generally conservative and also because the work with them is commercially sensitive. As soon as they are ready to announce, we will share them here.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128161; Is there a feature you'd like to see built or changed in our tech stack? Please share your thoughts on the Streamr dev community forum:</TITLE>
    <DATE>2019-07-02 14:03:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr integration templates are now available for large-scale data processing and unified analytics engine, Apache Spark.</TITLE>
    <DATE>2019-07-03 16:28:37</DATE>
    <TEXT> #8203; https://preview.redd.it/mtztnq6574831.png?width=2870 amp;format=png amp;auto=webp amp;s=801c171acaf322f5b0509c35c920c751c76bf78f Decentralization of real-time data is a huge part of the Streamr project. As the global production of data grows every day, especially with the wider adoption of IoT, more peer-to-peer solutions for big data might become necessary to increase throughput. Until mature P2P MapReduce style solutions are available, we must to fall back to centralized solutions when training ML models etc for the foreseeable future. This is why we have created [Streamr integration templates](https://github.com/streamr-dev/streamr-spark-integrations) for one of the most used Big Data processing tools today, [Apache Spark](https://spark.apache.org/). When building data architectures that need to be capable of handling terabytes or even petabytes of data, there are often multiple different tools required. It #8217;s very important that the tools perform fast and scale horizontally. You might need tools such as [Apache NiFi](https://nifi.apache.org/) to handle the transfer of data inside the architecture effortlessly, and tools such as [Apache Cassandra](http://cassandra.apache.org/) and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) to store the data for later use in ways that allow fast recovery. https://preview.redd.it/b47u68z774831.png?width=2048 amp;format=png amp;auto=webp amp;s=e4bd21b5e3a375d4ed931b0f8b326591f327f35c You might also need tools to do computation on the data in various different points of the architecture. For example, when using machine learning you need to be able to train ML models using massive amounts of stored data, and then be able to use the trained models in real time for new arriving data sets. What if you also need windowed anomaly detection on top of the machine learning? You could learn and set up a different tool for each of these cases #8202; #8212; #8202;or you could just use [Apache Spark](https://spark.apache.org/). Apache Spark is an open source unified analytics engine for large-scale data processing. Originally designed to be a faster alternative for Hadoop MapReduce, it claims to be up to 100x faster. Spark #8217;s Resilient Distributed Datasets (RDD) are the main cause of this performance increase. RDDs make it possible to do distributed computation in memory, which is the main cause of the performance increase when compared to MapReduce. In reality, because fetching all of the data in memory is unlikely, such an increase is unrealistic in most cases. However, RDDs overflow all of the data that doesn #8217;t fit in memory to disk fault tolerantly, so you don #8217;t have to worry about memory management. This also means that performance decreases based on the amount of data in disk. On top of doing computation over large data sets from HDFS, Cassandra etc, Spark also provides libraries for Spark SQL (Structured data), Spark Streaming, Spark MLlib (Machine Learning) and Spark GraphX (graph processing). Importantly for Streamr, Spark SQL also has a Structured Streaming functionality. With Structured Streaming you are able to do real-time computations on your JSON formatted Streamr data in micro batches or larger windows. Decentralization of real-time data is a huge part of the Streamr project. Streamr integration templates are provided for Java and Scala. For Java there is only a Spark Streaming direct integration template using Sparks CustomReceiver abstraction. Currently, there are no examples provided for direct publishing of data because Streamr #8217;s Java client does not work outside of Spark #8217;s executor. If you wish to publish data directly from Spark to Streamr you need to make direct calls to [Streamr #8217;s Data API](https://www.streamr.com/docs/streamr-api). For Scala there are more examples for integrations with Streamr. Direct integration examples, similar to the Java template, are provided for Structured Streaming and Spark Streaming in Scala. Here is the example code for a Streamr subscription custom receiver for Spark in Scala: [Full example in the Medium post \(linked at the bottom\)](https://preview.redd.it/t1ejlmlg74831.png?width=1182 amp;format=png amp;auto=webp amp;s=a1529b365a23009fbd8fd2d3b9efd15e549c0599) There is also a Node.js script that allows you to pull historical and real-time data from Streamr to file systems, and a Scala template on how to process the data of the created JSON files with Spark SQL. An example of h[ow to use Apache NiFi to pull data from Streamr](https://medium.com/streamrblog/apache-nifi-streamr-processors-5822da9b1820) and push it via Kafka to Spark to be analysed is also provided. You can also do the process in reverse and publish the analysed data back to Streamr. It would also be possible to use Spark #8217;s Python version PySpark to pull data to Spark directly from NiFi with [this](https://community.hortonworks.com/articles/171893/hdf-31-executing-apache-spark-via-executesparkinte-1.html) out-of-the-box NiFi processor. Once Streamr #8217;s Java client is in a more stable state, we may publish Streamr #8217;s custom Spark receivers and sinks to Maven. This would make integrating Streamr to Apache Spark a lot easier as you would only need to declare the Maven library as a dependency in Spark #8217;s start up script without any workarounds. For now, the required steps on how to start using Streamr data with Apache Spark in Java or Scala are documented in the [integrations #8217; repository](https://github.com/streamr-dev/streamr-spark-integrations). One glaring hole in Spark #8217;s streaming libraries is that the data processing isn #8217;t actually done in real-time for single events in a stream. Instead, the Spark Streaming and Structured Streaming do their computation in micro batches. This means that you will often see multiple outputs, because the micro batches tend to contain more than one data point. This is where [Apache Flink](https://flink.apache.org/) comes in as the better candidate for real-time data processing. Apache Flink does its real-time processing in events, so each arriving data set gets processed exactly once in its own window. We have also done an integration to Apache Flink which will published soon. *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community run* [*dev forum here*](http://forum.streamr.dev/)*.* *See the original post from* *Santeri Juslenius* *on* [*Medium*](https://medium.com/streamrblog/integrating-streamr-with-apache-spark-4c049f2688a1)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr integration templates are now available for large-scale data processing and unified analytics engine, Apache Spark.</TITLE>
    <DATE>2019-07-03 16:29:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-07-03 16:30:15</DATE>
    <TEXT>Thanks for the feedback. I appreciate the frustration but there are things happening, some of which can #8217;t be communicated yet as mentioned in the post. There are several articles that tell the bigger picture of the Streamr story, incentives, and use cases - most of which can be found on the [strategy](https://medium.com/streamrblog/strategy/home) or [partnership](https://medium.com/streamrblog/partnerships/home) pages of the blog. Broader implementation strategies is something that will be communicated more on, though it does require the tech to be ready for it to be put into action, which is progressing on schedule (and [ahead of schedule](https://medium.com/streamrblog/dev-update-may-2019-83992de3975d) in some places), but needs further patience before it is ready to reach the goals set out in the whitepaper. I hope you'll see some of this progress with Community Products launching later in the year. Please let us know if you have any other suggestions regarding communication. There is also the [Community Fund](https://forum.streamr.dev/t/community-fund) open to project ideas.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128225;  #128722;  #128202; The Streamr Marketplace is your storefront to the world of real-time data. Subscribe to streams, create and sell your own to earn DATA, and bring it all to life with Streamr Core #8217;s processing tools</TITLE>
    <DATE>2019-07-04 11:21:23</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A new fund proposal from longtime community contributor Gang Liu, the Surf team's Firefox  amp; Chrome plugin private beta advancing well, and progress made for Core, the Network, and Community Products - read the latest in the Streamr dev update!</TITLE>
    <DATE>2019-07-04 14:56:41</DATE>
    <TEXT> #8203; https://preview.redd.it/4f3dkytpva831.png?width=3200 amp;format=png amp;auto=webp amp;s=18acf3b4b6fde67de8b71a2204c4f4ba35564d24 Welcome to Streamr Core Developer #8217;s Update for June. Before diving into our internal updates, I wanted to highlight two exciting occurrences that happened last month. One of the early Streamr community projects  #8212; a web browser plugin that allows users to share their browsing and search engine data  #8212; [launched a private beta](https://forum.streamr.dev/d/50-call-for-beta-testers-firefox-and-chromium-plugin-private-beta) to let users try out their application. I have been really impressed with their privacy setting functionality, which has been built from the ground up to give users maximum control. Currently, they offer the plugin for both Firefox and Chrome platforms. Over 40 users signed up to provide their feedback. It was great to see interest from people both within and outside of the crypto community. [Search Engine module from Firefox plugin](https://preview.redd.it/p7wmdpnrva831.png?width=800 amp;format=png amp;auto=webp amp;s=d00fe21ee1d5b3d0974811273dded04510d130e5) The second exciting occurrence is regarding a new [Community Fund project proposal](https://forum.streamr.dev/d/51-create-a-mobile-app-to-gather-usage-metrics-across-apps/9), started by our longtime community member [Gang Liu](https://medium.com/u/ee1c939660c5). It was in response to [this recent news about Facebook #8217;s study app](https://techcrunch.com/2019/06/11/study-from-facebook/?guccounter=1 amp;guce_referrer_us=aHR0cHM6Ly9tZWRpdW0uY29tL3IvP3VybD1odHRwcyUzQSUyRiUyRnRlY2hjcnVuY2guY29tJTJGMjAxOSUyRjA2JTJGMTElMkZzdHVkeS1mcm9tLWZhY2Vib29rJTJG amp;guce_referrer_cs=yzZyHlqftFMq30DBHvD2qg). If they are planning to harness even more data for their proprietary usage, then let #8217;s aim to build a more open version to give any small/medium startup the opportunity to access the same quality data. This is what we mean by creating new  #8220;Data Commons #8221; with our upcoming [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9).  #8203; https://preview.redd.it/qmvgrd2vva831.png?width=1120 amp;format=png amp;auto=webp amp;s=1c2b6ecffcd78a1c5de0bcfdba81b7f7102e1131 *Study App mock interface from Facebook, source:* [*TechCrunch*](https://techcrunch.com/2019/06/11/study-from-facebook/) Coming back to Streamr #8217;s internal dev update, the team has been focusing on the following tasks: * Add the last features to [Streamr Core](https://medium.com/streamrblog/streamr-core-beta-launch-notes-14b0d2740a9c) and fix the few remaining bugs * Work on the [ #8220;Corea #8221; milestone](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca) release for Streamr Network (around Q4) * Finalise the specs and implementation for [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) For the Core app, we have gathered valuable feedback from our recent private beta testing. The aim is to complete the remaining tasks by Q3 and to roll out the stable official version by September, with a new landing page too. Our developers are also working on a new revamped technical documentation section, with the goal of offering a much better onboarding experience for all developers interested in testing our stack. Just a reminder that the current Core version is still in Beta. On the Network front, we have been conducting some end-to-end tests for Broker node, to verify that the software works as expected from a black-box perspective. We have recently completed the end-to-end symmetric encryption implementation for the Network and we are now moving on to the key exchange mechanism. **Important notes** for the upcoming Corea release: at this milestone the Network will leverage the new p2p protocol we have developed, but it won #8217;t be decentralized yet, as stated in our [Network roadmap](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca) published last year  gt;*In the Corea network, all nodes will still be run by us, as the necessary encryption and incentive mechanisms are not yet in place.*  #8203; [Streamr Network](https://preview.redd.it/dinvg0myva831.png?width=1400 amp;format=png amp;auto=webp amp;s=689b18aeffd370beacb1ad255f0a84a5b2f1f363) However, interested community members will have the opportunity to run a network node for a public testnet we are planning to launch in conjunction. Regarding the earlier-than-planned rollout of encryption with the Corea milestone, the first version will only offer a symmetric key exchange. Encryption is necessary in the p2p network to guarantee data confidentiality, in addition to anti-tempering. There are still many components that need to be ironed out, such as key exchange mechanism, key revocation mechanism and integration with our Editor. The key exchange (and later revocation) will make our encryption usable with minimum user intervention and potentially place Streamr ahead of other p2p projects on the encryption side. *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community run* [*dev forum here*](http://forum.streamr.dev/)*.* As always, thanks for reading. Here #8217;s our regular list of updates: ## Network * Continuing work on MQTT adpater * Metrics scripts for node and the tracker * Started working on mock API for broker * Storage node configuration * Update broker to use the new network API * Created and ran end-to-end testing * Finished end-to-end symmetric encryption and started working on the key exchange mechanism ## Ethereum * Writing tests for Community Products * Working on admin cut feature for paid streams ## API  amp; Clients * v31 of protocol deployed to production * Latest version of java client to Maven Central (1.1.0) * v31 JS client integration of our protocol in the p2p network ## Core App (Engine, Editor, Marketplace) * Worked on stream list paging layout * Debugging Ethereum login and web3 fixes * Stream-specific inactivity period tracking * Fixed module search styling, tweaked marketplace product search, continuing on map zoom level and centre persisting * Continuing working on landing page * Designing new Developer Docs and creating new content for it ## DevOps * Cassandra monitoring up and running * Artifact versioning for E amp;E * Data-api metrics *Read the* [original post](https://medium.com/streamrblog/dev-update-june-2019-d77482af8bdb) *from Head of Developer Outreach, Weilei Yu.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Crowdselling data via Community Products incentivises individuals to provide insights into previously unexplored areas. Here is just one use case from Head of Partnerships, Ben Sheppard.</TITLE>
    <DATE>2019-07-05 14:47:26</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A new project from Gang Liu to build an app that collects mobile app usage data and sends it to the Streamr Marketplace alongside user cohort info has been approved by the Community Fund. Read more  amp; suggest your own proposal on the dev forum!</TITLE>
    <DATE>2019-07-08 15:16:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>What if there was an alternative model to the murky harvesting of personal data in exchange for free online services? Matthew Fontana shares his argument for individuals reclaiming the value of their data:</TITLE>
    <DATE>2019-07-09 14:36:17</DATE>
    <TEXT> #8203; https://preview.redd.it/kp1wazfsga931.png?width=1600 amp;format=png amp;auto=webp amp;s=68785c8901395bb1201243990c5cb88ad0b45622 Your next Smart TV purchase will know how to spy on you and has [buyers for your data](https://www.theverge.com/2019/1/7/18172397/airplay-2-homekit-vizio-tv-bill-baxter-interview-vergecast-ces-2019). You bought a product from the store but as soon as you switch it on, and for its lifetime, you are the product. If you feel a little queasy, please read on. (It gets worse). While data monetisation is not entirely new, it still feels weird to talk about. I #8217;m used to the concept of owning the things that I buy, but I don #8217;t seem to have ownership of the data generated from the things that I #8217;ve bought. To add to the murkiness, I #8217;m not sure what data I #8217;m giving away, why I #8217;m giving it away, and to whom I #8217;m giving it to. And yes, I admit, *I agreed* to it all. Monetisation of consumer data has been in plain sight for years. From the websites that offer us free content while they track us, to our supermarket rewards programs. It #8217;s why Facebook is free and always will be. The dark side to this tropical oasis of free services is the mostly unregulated and unfair harvesting of our data for purposes unbeknown to us. Have you ever received a refund from a company that made too much money from your data? Probably not! These data parasites have burrowed into our lives and we #8217;re going to have a hard time getting rid of them.  #8203; [If you can #8217;t figure out how this thing you #8217;ve bought can be so cheap, you #8217;re probably the product.](https://preview.redd.it/63v9hq4vga931.png?width=1120 amp;format=png amp;auto=webp amp;s=efb1b3a98dfac5bdc0cbf6c69f8864776cb4ca56) # How did we get here? The data that you generate by interacting with the world around you has value to the right buyer. Your location, your heart rate, how fast you drive and everything in between is valuable to advertisers, insurance companies and probably your crazy ex. Tech companies especially have become exceptionally good at extracting value from that data by applying machine learning and AI algorithms to it. Like water to wine, data turns into understanding, insight for profit, and competitive advantage. It #8217;s the recent activation of this value chain that makes data such a hot topic today.  gt;Like water to wine, data turns into understanding, insight for profit, and competitive advantage. # Power from the people Sophisticated algorithms that have been trained on huge amounts of personal data have superpowers that can predict and influence our future behaviour. They can predict things about you, like [if you #8217;re pregnant](https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#5e6e38f06668), or when you #8217;d be most vulnerable to a particular type of advertisement. Perhaps your data trail reveals your marriage is on the rocks  #8212; your partner might start getting advertisements for divorce lawyers and dating services. Is that fair play or is our free will under attack? Foursquare used your location data to predict how many iPhones would be sold on launch weekend with a [striking degree of accuracy](https://www.wired.com/2015/09/foursquare-claims-foot-traffic-can-predict-weekend-iphone-sales/), even before the official numbers were released to investors. They figured out that the foot traffic of their users queuing outside Apple stores would correlate with how many phones would be sold.  #8203; https://preview.redd.it/4e35qzzxga931.jpg?width=1120 amp;format=pjpg amp;auto=webp amp;s=6273d68e5ca545fca574ab73e3389388c21fbbda So your data is valuable but your wallet is empty #8230; This is because you are dealing with the Asshole Business Model (ABM)**.** Today, the only way to  #8216;profit #8217; from your data is by consuming discounted services and products. This happens to be the wildly successful business model of the greatest technology companies of our generation, and it #8217;s spreading like wildfire. # Disrupting the Asshole Business Model Open source projects such as [Brave](http://brave.com/) and [Streamr](http://streamr.com/) are attempting to reverse the **ABM** through fundamental decentralisation. Brave is shaking the box at the browser level  #8212; making it much harder for companies to turn a profit by tracking you. Streamr is building decentralised data highways for your data to safely flow through as well as ways for your data to self organise and be sold autonomously without middlemen.  #8203; [How decentralising data could return value back to the individual. #crowdselling](https://preview.redd.it/16iqie40ha931.jpg?width=1120 amp;format=pjpg amp;auto=webp amp;s=3b9d66f275a0bd34f7925fc7b4ab03535c024434) There is an important point to make about how data is generally utilised: the value of your individual data is quite different to the value of everyone #8217;s combined data. I can run simulations on large datasets  #8212; I can #8217;t do that with a single data point. This is why your data needs a way of self organising into larger datasets  #8212; so that it can claim its true market value from fairly contributing to the larger, more valuable data pool. If you #8217;re skeptical as to why something like this hasn #8217;t been built already it #8217;s because much of our existing tech stack needs to be rebuilt from the ground up. Fundamental decentralisation is largely an unsolved problem on many levels. Tying this back to your smart (spy) TV purchase, when the technology is ready, you #8217;ll be able to purchase a TV that sends you micropayments from the data you choose to sell. Your TV watching habits would be anonymised and self organised with others #8217; data. It would be sold on a decentralised marketplace and the data would flow with end-to-end encryption from seller to buyer. # Why should you care about your data? Besides getting a micropayment or not, your data is going to play a huge role in your digital life as we enter the  #8216;Internet of Things #8217; era. Your shoes will be conversing with your doctor and your washing machine will be negotiating with your neighbour #8217;s solar panels. In other words, there is going to be an enormous amount of data chatter happening for and about you. You best be engaged in the discussion or someone/something else is going to be making the decisions for you. They might not have your best interests at heart.  gt;There is going to be an enormous amount of data chatter happening for and about you so you best be engaged or someone/something else is going to be making the decisions for you. # What comes next? Data will transform our lives in unimaginably wonderful ways but there is a battle to be fought first that will decide how much of the value goes to individuals versus how much will be siloed in Silicon Valley. Ignorance is bliss but wealth inequality is at an all time high today  #8212; protecting out data rights would take us a huge step forward towards a fairer distribution of wealth. For now, think twice about your growing data footprint and give the middle finger to services that request your data and offer nothing back. Make no mistake, a data revolution is brewing. A revolution that gives individuals control of their data and puts an end to freeloading corporate consumption of your personal data for selfish gain. *Read Mathew's original post here:* [https://medium.com/streamrblog/get-ready-the-data-revolution-is-brewing-cfa969f6f1e2](https://medium.com/streamrblog/get-ready-the-data-revolution-is-brewing-cfa969f6f1e2)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is pleased to team-up with BLOCKROCKET, a community connecting projects providing blockchain based tech to potential adopters</TITLE>
    <DATE>2019-07-10 12:01:04</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Monoplasma is an important component for Community Products, allowing multiple individuals to be paid simultaneously every time their data is purchased on the Marketplace. Read how it might also be deployed by the wider dApps space</TITLE>
    <DATE>2019-07-11 13:03:42</DATE>
    <TEXT> #8203; https://preview.redd.it/uq8qs8ut9o931.png?width=1600 amp;format=png amp;auto=webp amp;s=f038bf77120b0b4ef6fe75b00c542a0d52e238fa Monoplasma is a special-purpose off-chain scaling solution for one-to-many payments. It #8217;s a good fit for any system where you repeatedly need to: 1. Distribute value to a large and dynamic set of Ethereum addresses. 2. Allow recipients to accumulate value over time. 3. Withdraw tokens at their preferred moment. We built Monoplasma because we needed it for the upcoming [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) feature on the [Streamr Marketplace](https://marketplace.streamr.com/). Community Products will allow end users who produce data through a connected gadget, [like a Fitbit](https://medium.com/streamrblog/personal-fitbit-data-sell-streamr-marketplace-blockchain-ethereum-3b32c215660c), or a Tesla, to pool and sell data collectively. And when that conglomerated data product sells, the revenue will then be automatically distributed out to potentially hundreds of thousands of data contributors. The Monoplasma framework is reusable and un-opinionated, and as such, it #8217;s a piece of software that might help others too. That #8217;s why we wanted to make it standalone as a (hopefully useful) contribution to the Ethereum scaling space. # What problem does it solve? Here are some possible use cases for Monoplasma, where this type of one-to-many payment pattern is repeated: * Revenue sharing * Dividend distribution * Staking rewards * Repeated airdrops * Community rewards * Pension/benefit payments * Loyalty reward schemes To outline something more specific I #8217;ll pick an example way outside of our own use case of data monetisation and smart devices. Imagine a decentralized Airbnb service where ownership shares in apartments are tokenised. An individual apartment could have hundreds, even thousands, of owners, each owning a fraction of it. When a guest stays in said apartment, the fee needs to be split among the owners. Ideally, this would happen on-chain within the payment transaction, but looping through each recipient, calculating their share, and sending tokens to them in a smart contract costs more and more gas as the number of recipients grows. This approach works fine for a handful of people, but it is simply not feasible beyond, say, 100 recipients  #8212; the transaction will simply become too large to even be included in a single block! # Design principles We looked at the various scaling solutions out there for Ethereum, but none of them met our requirements, which were: * It must distribute individual lump sum payments among a large group of recipients. * Recipients must be able to accumulate value for as long as they want, and withdraw at will by making a transaction on-chain. * Recipients must be able to join the pool without making a transaction or placing a stake. This is different from many other payment channels, and in fact crucial for us. In our use case, the people are mainstream users and we need to minimise obstacles for joining, such as having to buy ETH on day one. * And finally, the system should be trustless and transparent. So we decided to roll out our own, and try to keep things as simple as possible. We were [inspired by the different Plasma variants](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa#a312) and payment channel stuff, but we basically threw out everything we didn #8217;t need, to find the simplest possible mechanism that does what we want (and only that!), while maintaining good security properties and trustlessness. I won #8217;t dive into the endless depths of how Monoplasma compares to every other side-channel/side-chain/payment channel solution out there. I #8217;ll focus on the major functional difference with the rest of the family. This is the one thing to remember after reading this article: *We sacrificed generality in favour of special-purpose.* This is not a generic payment system where everyone can arbitrarily transact with each other. The system is unidirectional: there are no transfers between accounts in the side channel.  #8203; https://preview.redd.it/bgisq1bv9o931.png?width=1120 amp;format=png amp;auto=webp amp;s=5f1d33747fcd8bd71e57d4c41a98c0b947d949ef In other words, tokens can #8217;t be spent on the side channel. But why is that a good thing? Since tokens can #8217;t be spent there, there can be no double spending! And as you might know, preventing double spending is arguably the hardest problem in designing any distributed system that transfers value. Not having to deal with this simplifies things immensely, which is exactly what we wanted. # Positive balance off-chain, negative balance on-chain Since there #8217;s no side channel spending, balances on the side channel are your cumulative lifetime earnings. In other words, your account balance can only increase monotonically. Hence the name: Monoplasma. So what about withdrawals then? Withdrawals are tracked on-chain instead of off-chain. The side channel records your cumulative earnings (credits) and the smart contract keeps track of your cumulative withdrawals (debits).  #8203; https://preview.redd.it/impzgw10ao931.png?width=1120 amp;format=png amp;auto=webp amp;s=1d2604e964d730d27fd8d7846414183b39931fad Your withdrawable balance is the difference. To withdraw, all you need to do is prove your positive balance to the smart contract. You do this by providing a simple [Merkle proof](https://medium.com/crypto-0-nite/merkle-proofs-explained-6dd429623dc5). Let #8217;s look at an example. The Monoplasma smart contract holds all the non-withdrawn tokens of the side channel. If we can prove we have earned 100 tokens, and the Monoplasma smart contract has seen us withdraw only 30 so far, it will allow us to withdraw 70 more.  #8203; https://preview.redd.it/nsz9ebs3ao931.png?width=1120 amp;format=png amp;auto=webp amp;s=a106f9bfbdfaee4de82c27a21f59a9f271c2d2e8 The smart contract updates its internal record keeping to reflect that we now have withdrawn 100. If we try to withdraw again with a proof of earnings of 100 (or less), it won #8217;t give us any further tokens. In other words, Monoplasma relies on the secure consensus mechanism of Ethereum to prevent people from withdrawing more than their provable share. # Other similarities and differences I #8217;ll briefly highlight some other aspects of Monoplasma: * There #8217;s an Operator/Validator model. The Operator controls the off-chain state, and in Monoplasma they are actually the only party who can cheat. The job of the Validators is to check that they don #8217;t. * Both the Operator and Validators maintain the off-chain state in a Merkle tree, and the Operator periodically publishes the root hash to the on-chain smart contract to enable withdrawals from that state. This is all familiar from other Plasma variants. * One difference, however, is that there #8217;s no challenge period/exit game, as there can #8217;t be double spending. Instead, there #8217;s a non-interactive freeze period to allow time for Validators to exit people #8217;s funds using the last validated honest block in case the Operator cheats. So earned funds do become withdrawable with a few days of latency, which is fine for many use cases, including ours. Although somewhat simplified, the below table summarizes some of the similarities and differences between Monoplasma and other typical implementations in the same space:  #8203; https://preview.redd.it/q71zpg06ao931.png?width=1120 amp;format=png amp;auto=webp amp;s=dec5ff8e87b0767550762b6ba2caeac3bd55bd42 # As a developer, how can I try it out? Monoplasma lives in [this repo](https://github.com/streamr-dev/monoplasma) within the [Streamr Github](https://github.com/streamr-dev). It ships with an interactive revenue sharing demo which showcases the functionalities of the framework. Feel free to clone the repo and play around with it yourself (as others already have done!). Instructions for running the demo are in the readme. You can also watch the demo as I presented it live at [ETHDenver](https://www.ethdenver.com/) in the video below: [https://www.youtube.com/watch?v=t7vOoLBFkUA](https://www.youtube.com/watch?v=t7vOoLBFkUA) We hope Monoplasma will be useful to other #buidlers out there! It will be reasonably well maintained for the time being, because we will be needing it ourselves for at least as long as similar scale and cost can #8217;t be achieved on-chain, which is probably several years away from now. Kudos to Juuso Takalainen of Streamr who #8217;s done almost all of the heavy lifting, and to [Kelvin Fichter](https://www.linkedin.com/in/kelv-in) of OmiseGO (and recently [plasma.group](https://plasma.group/)) for support and putting us on the right track. A shout-out also to some prior art we came across including [RicMoo](https://blog.ricmoo.com/merkle-air-drops-e6406945584d) #8217;s airdrop utility and the [OUTPACE](https://github.com/AdExNetwork/adex-protocol/blob/master/OUTPACE.md)protocol by the folks at [AdEx](https://www.adex.network/). You can follow the Streamr community #8217;s progress on [Peepeth](https://peepeth.com/streamr) and [Twitter](https://twitter.com/streamr)and also join the [community on Telegram](https://t.me/streamrdata) and our [developer forum](http://forum.streamrdev.com/). Good luck!  #8203; *Read the* [original post](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa) *from Henri Pihkala on Medium.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>To widen the reach of the Streamr tech stack, the guys at Labs have created an integration with IBM #8217;s Node-RED. Read about it and watch the developer tutorial</TITLE>
    <DATE>2019-07-12 16:32:28</DATE>
    <TEXT> #8203; https://preview.redd.it/k50bm7gl4w931.png?width=2554 amp;format=png amp;auto=webp amp;s=2510da7dc5ba30c25dffb8b3399976aa7cdec566 At [Streamr](https://streamr.com/) Labs we are constantly looking for ways to further the adoption of the Streamr stack. As we are trying to solve real world problems around utilising decentralized technologies, we occasionally delve back to centralized world and find interesting cross-sections of tech. Lately we have been prototyping how to wrap the Streamr Node.js client for Node-RED. We have created Streamr publish and subscribe nodes for Node-RED that utilise our existing Streamr javascript client for publishing and subscribing to our data [API](https://medium.com/streamrblog/the-new-api-explorer-for-streamr-developers-e6af28c2c808).  #8203; https://preview.redd.it/ovyy0k2n4w931.png?width=1400 amp;format=png amp;auto=webp amp;s=de916b78b1eeb3f44870042b7f4cf367bfb5bba1 ## What is Node-RED and why create an integration? Node-RED is a flow based programming tool for internet of things originally built by IBM. If you haven #8217;t yet checked it out, [take a look here](https://nodered.org/). One might find similarities between our Editor and Node-RED #8217;s editor, but they are fundamentally quite different. Without going too deep into these differences, we have modules on canvases and they have nodes on flows. What excites us about Node-RED is the vast number of existing libraries and integrations already build for it. There are modules for everything including how to integrate home devices into automatic networks, to modules that integrate to third party AI and ML endpoints. And now there is a module to integrate to Streamr #8217;s data API. You can find our Streamr node for Node-RED from Streamr #8217;s Github repository: [https://github.com/streamr-dev/node-red-streamr-client-node](https://github.com/streamr-dev/node-red-streamr-client-node) And also from: [https://flows.nodered.org/node/node-red-contrib-streamr](https://flows.nodered.org/node/node-red-contrib-streamr) An example flow which uses tram data from Helsinki (our classic Streamr demo) can be found here: [https://flows.nodered.org/flow/b67885ec48a9a5ae213519c9a6c825c9](https://flows.nodered.org/flow/b67885ec48a9a5ae213519c9a6c825c9)  #8203; [Helsinki tram data being pushed from Streamr and into a Node-RED interface](https://preview.redd.it/tmfv56wp4w931.png?width=1400 amp;format=png amp;auto=webp amp;s=98977f62614c7084792f5371a099cd3cb0868bc5) You can reach out to the community on our new [community developer forum](https://streamr.dev/) to let us know how to improve this integration and what else you #8217;d like to see us integrate. In the meantime here is a short tutorial using the tram demo data to help you get started: [https://www.youtube.com/watch?v=f55D34KirF0](https://www.youtube.com/watch?v=f55D34KirF0)  #8203; *Read the* [original post](https://medium.com/streamrblog/streamr-node-red-integration-tutorial-b0b410496354) *from Head of Technology at Streamr Labs, Jarno Marttila.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>To widen the reach of the Streamr tech stack, the guys at Labs have created an integration with IBM #8217;s Node-RED. Read about it and watch the developer tutorial</TITLE>
    <DATE>2019-07-15 07:21:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128499; Streamr Community Fund admin nominations!</TITLE>
    <DATE>2019-07-15 18:17:10</DATE>
    <TEXT> #8203; https://preview.redd.it/xqw7z76cfga31.jpg?width=1280 amp;format=pjpg amp;auto=webp amp;s=da3d295771e5729d54f2e2d53cd437e8c91849ff It's been six month's since the Streamr [Community Fund Launched](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674) and now it's time for you to nominate \*two new admins, as Remy and Sason's first terms come to an end. The past 6 months saw two prominent projects accepted for funding, including the [Firefox and Chrome plugin](https://forum.streamr.dev/d/39-firefox-extension) and [Streamr Probe](https://forum.streamr.dev/d/51-create-a-mobile-app-to-gather-usage-metrics-across-apps), both launching this year. Thank you to Remy, JD, Sason, and Viktor for your contributions. You can nominate yourself to become a community administrator here. Just comment on this thread and introduce why you #8217;d make a good candidate, any ideas for growing the project, and share your Telegram handle. After a week (22/07/19), we #8217;ll add the eight candidates with the most thumbs up to a polling bot and share it on the [Streamr Telegram](https://t.me/streamrdata) for a final week of voting to choose two new admins. Only candidates and votes from those currently in the Telegram group as of today (15/07/19) will be counted on the polling bot. At the end of the voting week (29/07/19), the two candidates with the most votes are chosen as fund administrators for the next six months. Elections will then take place again every three months as the other two admins' terms ends. Good luck and don't forget to vote for your favorite! \**Correction 16/07/19 to clarify that it is two admins who's term is ending, not all: This first 6 month term coming to an end had two Streamr elected admins (@remykoning and @s\_sason) for 6 months and two community elected admins (@Viwe12 and @MrNerd2) for 9 months. It is therefore @remykoning and @s\_sason, the initial Streamr elected admins, who are at the end of their first term and are open for re election if they wish to apply. @Viwe12 and @MrNerd2 have another three months left on their opening nine month term.* *Admin elections will then take place every three months for two admins at a time, each serving six month terms.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr team met in Helsinki with Implement Consulting last month to discuss the strategy for marketing the Streamr Network, "the backbone of this project". Take a look behind the scenes of the workshop with Head of Partnerships, Ben Sheppard!</TITLE>
    <DATE>2019-07-16 14:47:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr Community Hero for July is  #129345; ... Ebrahim Khalilzadeh! A big thank you to Ebrahim and his team for their exciting plugin that will provide a way for users to offer their Firefox and Chrome browsing data as a Community Product</TITLE>
    <DATE>2019-07-17 13:43:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-07-17 13:44:45</DATE>
    <TEXT>Read more: [https://medium.com/streamrblog/crowdselling-your-personal-data-through-firefox-c4f8bf9b8a96](https://medium.com/streamrblog/crowdselling-your-personal-data-through-firefox-c4f8bf9b8a96)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can unlocking data silos to create a decentralized real-time data economy benefit the automotive and manufacturing industries? Streamr #8217;s Head of Partnerships, Ben Sheppard shares his thoughts.</TITLE>
    <DATE>2019-07-18 11:56:38</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A RuuviTag is an open source sensor that records temperature, pressure, motion,  amp; humidity. This info can be sent to the Streamr Marketplace in real-time for users to subscribe to. Soon you #8217;ll be able to crowdsell this data as a Community Product!</TITLE>
    <DATE>2019-07-19 13:26:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is backing our partner Elk #8217;s kickstarter to produce development boards for the decentralized web. It's great to see another gateway open to Ethereum and de-siloed IoT data.</TITLE>
    <DATE>2019-07-29 11:26:47</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is backing our partner Elk #8217;s kickstarter to produce development boards for the decentralized web. It's great to see another gateway open to Ethereum and de-siloed IoT data.</TITLE>
    <DATE>2019-07-29 11:27:16</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-07-29 11:55:57</DATE>
    <TEXT>It's worth reposting older content when things are a bit quieter in terms of fresh updates as not everyone will have seen it first time. New website was delayed but should be live in the next few weeks unless there is another delay. You can read more about the network development in [this post](https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b) from last month and the general stack in the latest [dev update](https://medium.com/streamrblog/dev-update-june-2019-d77482af8bdb). As with development, marketing work is not always immediately obvious, but it is taking place constantly, such as event planning and the recent workshop the team held in Helsinki for the go to market strategy for the network: [https://www.reddit.com/r/streamr/comments/cdxzus/the\_streamr\_team\_met\_in\_helsinki\_with\_implement/?utm\_source=share amp;utm\_medium=web2x](https://www.reddit.com/r/streamr/comments/cdxzus/the_streamr_team_met_in_helsinki_with_implement/?utm_source=share amp;utm_medium=web2x)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-07-29 12:12:30</DATE>
    <TEXT>Regarding the website, Matt also gave this update on Telegram last week:  #8220;For everyone wondering about the website update, i can fill in a few details. It is coming soon, but it has been delayed a bit. The core reason is that we always prioritise development work on the apps  amp; services over that of the public site, and we have limited bandwidth in our front end team. There are only 5 front end devs, and currently they are working on: * beginning implementation of Community Products front end * getting the Core app ready for its upcoming hard launch * working on updates for the new Docs * working on the public site update This is roughly in priority order. The site update is probably 80% done, but sometimes the final 20% can take longer than expected, especially when summer holidays crash into it. So we're almost there, and i'll be as happy as everyone else to see the site update go live. #8221;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-07-29 15:59:27</DATE>
    <TEXT>There is a lot in development (events, user acquisition campaigns, sponsorships, partner collaborations) around marketing that you will see as the rest of this year #8217;s product launches roll out. It #8217;s been said before that 2019 is a different time from 2017/18 where anticipation generated huge followings or a surge in new users. In the current climate, the best chance of marketing success in terms of user adoption or interest is when you have something solid to put into people #8217;s hands. The tech is ahead of schedule so this continues to be the direction of motion but there is still time before it is ready to market on a larger scale than today. This is why the current focus and content leans towards the technical and specialised in nature to attract those who believe in the end goal and can build and integrate the tech ahead of more general user acquisition (a niche within a niche). This summer has been quiet in terms of new public updates but I #8217;m confident you will see the results of the behind the scenes work later this year.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr goal is to ensure that the world #8217;s real-time data is owned and controlled by those who produce it. This is achieved by a p2p pub/sub Network, decentralized Marketplace, and open source tools for machines and people to trade in a data ecosystem.</TITLE>
    <DATE>2019-07-30 11:36:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr goal is to ensure that the world #8217;s real-time data is owned and controlled by those who produce it. This is achieved by a p2p pub/sub Network, decentralized Marketplace, and open source tools for machines and people to trade in a data ecosystem.</TITLE>
    <DATE>2019-07-30 11:36:49</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Did you know it #8217;s possible to connect data to the Streamr Marketplace in only 5 minutes? Take another look at this tutorial mini-series for creating your own real-time data products.</TITLE>
    <DATE>2019-07-31 14:48:50</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Did you know it #8217;s possible to connect data to the Streamr Marketplace in only 5 minutes? Take another look at this tutorial mini-series for creating your own real-time data products.</TITLE>
    <DATE>2019-07-31 14:53:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Did you know it #8217;s possible to connect data to the Streamr Marketplace in only 5 minutes? Take another look at this tutorial mini-series for creating your own real-time data products.</TITLE>
    <DATE>2019-07-31 18:33:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The partnership team regularly educates enterprises about the value in data sharing economies and how the Streamr stack can commercialise user data in real-time. Ben Sheppard gives an insight into those conversations.</TITLE>
    <DATE>2019-08-01 16:24:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-02 11:07:24</DATE>
    <TEXT>Hi there, so the Gartner analyst Ben spoke with predicts it will be five years before we see data marketplaces "take off". By this i understood it will be several years before they are likely to break out beyond early adopters to greater use/attention. The Streamr marketplace is live now and Network not far behind it, but for adoption at an enterprise level, that's the estimated timeframe (though a lot can change in 5 years of tech of course!). If you have any specific questions i can pass them on to Ben.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr will be out in force at the Chainlink Hackernode at Web3 Summit in Berlin (20-21/08). Here #8217;s what to expect</TITLE>
    <DATE>2019-08-02 13:36:36</DATE>
    <TEXT> #8203; https://preview.redd.it/tvfhnereg1e31.png?width=2001 amp;format=png amp;auto=webp amp;s=897b1896704c006573c8c32d52d318e82a337eee \- Co-hosting the amberdata  amp; Chainlink Hackathon \- Henri Pihkala will join a Panel on Data Providers \- Jonathan Wolff demos Ethereum canvases created with the Streamr Core App Hope to see you there! [https://web3summit.com](https://web3summit.com)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Your data is valuable but there is no way for you to extract this value. In a few months, this will change with a plugin developed by the Streamr community for Firefox, Google Chrome, and Microsoft Edge users to monetise their browser data</TITLE>
    <DATE>2019-08-05 16:28:52</DATE>
    <TEXT># Join a data union with the Surf-Streamr browser plugin [Canvas view of real-time search data from participating community members](https://preview.redd.it/t4m3k3khone31.png?width=1917 amp;format=png amp;auto=webp amp;s=e655cfa73216126cba028e1b3c62b3d106bc229d) Only a handful of corporations really profit from all the data that is collected when users browse the internet. Giants like Facebook and Google compete for your personal data so they can use that information to target you with advertising. Their advertising businesses are so dominant that many of the world #8217;s largest consumer-facing companies rely heavily on their services. Personal data has real value. The problem is that, until now, there has been no technical way for ordinary people to exploit this value for themselves. So we remain at the mercy of the freemium model: pay to use email, search engines and social media  #8212; not with money but with your personal information and privacy. We all know who is winning that game. In a few months, we won #8217;t have to accept this. There is an alternative. # Surf-Streamr Surf-Streamr is a new plugin available for Mozilla Firefox, Google Chrome and Microsoft Edge (Chromium version), which allows you to monetize the data you already generate whilst using your browser. What data you share with the outside world is fully controlled by you. That data is sent to a  #8220;data union #8221; where it is aggregated and sold on your behalf (by the team who have built this plugin) on the Streamr Marketplace. This data union is a firehose that contains all the information that everyone with a Surf-Streamr plugin has permissioned to be shared. The more people that join the data union, the greater the value that everyone can extract from buyers. The data union backend is powered by a platform created by the open-source project Streamr. Organisations that are interested in using that information can subscribe to the data firehose through the Streamr Marketplace, and that payment will be automatically distributed to all the users who participate in the data union. In essence, the buyer is directly paying you for your information. And you are in full control of what you share. [https://www.youtube.com/watch?v=r6mmiMGdyfY](https://www.youtube.com/watch?v=r6mmiMGdyfY) *An extended video demonstration by the plugin #8217;s co-creator Ebrahim Khalilzadeh* The Surf-Streamr plugin contains a few sections that users can navigate. A profile section so you can enter payment details. Advanced filters so you can select the data you feel comfortable sharing in a way that suits you. And dedicated data modules so users can enable specific settings for a service. # Modules Say you want to share your Facebook data. Within the dedicated Facebook module you can select exactly the kind of data you #8217;d like to share. For example, you can enable the module to share your  #8220;User Likes #8221; data but disable sharing your  #8220;User Friends #8221; data. That #8217;s what will take place, every time you visit Facebook within your browser. [Facebook module within the Surf-Streamr browser plugin](https://preview.redd.it/pon3pmeomne31.jpg?width=1920 amp;format=pjpg amp;auto=webp amp;s=e08bc48d33ae05d9fd3dbd25ed5fdb23b8fd5f8e) # Pro-Privacy The plugin also contains a five-point slider so you can set a privacy level that suits you. Each level contains its own variations. And each module can be set differently so you can have full control over what is sent and how it is sent. In the pictures below, you can compare and contrast the output of the lowest and highest privacy settings. [Lowest privacy level](https://preview.redd.it/8910hgp4nne31.png?width=1920 amp;format=png amp;auto=webp amp;s=6e3643305d23462610cf17a28b19354bad9e03ee)  #8203; [Highest privacy level](https://preview.redd.it/jjybfm48nne31.png?width=1920 amp;format=png amp;auto=webp amp;s=8a22a59627e15141b2510e8d06b7e34f3aff6ce7) # Advanced Filters Advanced Filters are there for you to mask specific URLs that you do not want to share. You can specifically exclude them by adding them to the list. Think of login URLs from frequently used websites. You can also mask a specific piece of text that you don #8217;t want to share. You are in full control when sharing your data. # Marketplace This marketplace should not be confused with the Streamr Marketplace. The marketplace within the Surf-Streamr plugin allows you to search for newly released modules from websites and services, so you can incorporate specific privacy settings when visiting those websites. The modules that you add will be visible within the Module category for you to adjust.  #8203; [Marketplace category within the Surf-Streamr plugin](https://preview.redd.it/bbz5lk7xnne31.png?width=1920 amp;format=png amp;auto=webp amp;s=be52b41d9b2d12f9c013e9a5914b2aff55796907) # Streamr Marketplace The Streamr Marketplace is an open-source platform created by Streamr. Streamr is a project that uses the Ethereum blockchain and works with an ERC20 token called DATAcoin. Streamr is creating an open-source platform for the free and fair exchange of the world #8217;s real-time data and their blockchain-backed data Marketplace and powerful tools are created to put your data back where it belongs  #8212; with you. https://preview.redd.it/qdzfsqb5one31.png?width=1920 amp;format=png amp;auto=webp amp;s=55914b362b958f1be57dc8853e4d10f9dfd669a9 # Beta Testing The Surf-Streamr plugin has been in private beta testing for a few weeks at the time of writing. We are currently testing the plugin with about 40+ community members who are interested in participating in order to find bugs and inconsistencies. The next stage is to ensure integration with Streamr platform once the plugin is completed in a few months, so it can be tested as a fully-fledged data union; a massive real-time data set, drawing upon the aggregated and permissioned data from all users of the plugin. These raw user-generated data sets liberate data, some of which is currently not shared by companies such as Google, and make it available for all to purchase. Data unions like this are therefore real game-changers to the data economy. *Thank you to Remy for the blog post and to the other community beta testers for assisting Ebrahim and team with development. Check out the original blog on* [Medium](https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-browser-plugin-d9050d2d9332)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Your data is valuable but there is no way for you to extract this value. In a few months, this will change with a plugin developed by the Streamr community for Firefox, Google Chrome, and Microsoft Edge users to monetise their browser data</TITLE>
    <DATE>2019-08-05 16:29:55</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can Smart Cities and mobility benefit from a de-siloed real-time data ecosystem? Martin Moraveck, Solutions Architect at Streamr, presents his thoughts.</TITLE>
    <DATE>2019-08-06 13:40:07</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How can Smart Cities and mobility benefit from a de-siloed real-time data ecosystem? Martin Moraveck, Solutions Architect at Streamr, presents his thoughts.</TITLE>
    <DATE>2019-08-06 13:40:28</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Decentralized real-time data economies can transform the way we understand the world. Hear how Streamr and WWF are working to incentivise fisherfolk to become data points for sustainable fishing practices.</TITLE>
    <DATE>2019-08-07 15:54:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-08 10:18:51</DATE>
    <TEXT>Thanks! Ben is over there now working on it and we should have a video out in the next few weeks that explains how it works in more detail. In the meantime, here's a link to his [video playlists](https://www.youtube.com/playlist?list=PLz4GDOpDozwWRZcISoaFVdaMgL83x1d6V) where he talks more about the concept.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Network testnet deployed, Community Products server reaches MVP stage, new Marketplace functionality, and dev docs coming soon. See the latest progress in the Streamr dev update!</TITLE>
    <DATE>2019-08-08 12:05:00</DATE>
    <TEXT># Dev Update July 2019  #8203; [Elk Development Board](https://preview.redd.it/8vae6up1t7f31.jpg?width=1797 amp;format=pjpg amp;auto=webp amp;s=24916faff0005874f49e335522c19af5bedc26a4) Welcome to Streamr #8217;s core dev team update for July 2019. You #8217;re supposed to take a break in the summer months  #8212; not us. As we approach some crucial shipping deadlines, we have seen progress on multiple fronts including Network, Community Products and the Core app. Before jumping into our internal progress, we would like to mention that one of our IoT partners, [Elk](https://elk.cc/), has just launched a [Kickstarter campaign](https://www.kickstarter.com/projects/233173198/elk-the-dev-board-for-the-decentralized-world). Amazingly, they reached their funding goal in less than two days. They are [building a development board](https://blog.hackster.io/elk-is-a-development-board-for-the-blockchain-and-the-decentralized-web-f42c857dfed9) for blockchain and the decentralized web. Check out this awesome video on punch bag payouts which details one of the many possible use cases when merging automated crypto payments and IoT. And if you #8217;re a fan, like we are, give them your support :) [\[PITCH\] Elkrem - The smart contract that makes you fit !](https://www.youtube.com/watch?v=1VIbe3a_5D4) *It combines the simplicity of Arduino along with native support for decentralized networks. With only a few lines of code you can build IoT that interfaces with Ethereum, IPFS, Whisper, and more!* https://preview.redd.it/h0hi61n7t7f31.png?width=700 amp;format=png amp;auto=webp amp;s=0c3da76cb36b4d83aeefb2efd66f8f96a7e0d2e6 Users will be able to stream data from the board easily to [Streamr Marketplace](https://www.streamr.com/marketplace/) via one of our SDKs, like the [Javascript library,](https://github.com/streamr-dev/streamr-client-javascript) which allows authentication via Ethereum account. Returning to our core dev monthly update, on the Network front we have finalised the deployment of the testnet. We are planning to mirror data from production servers and gather vital metrics to verify the current performance level. What comes next is running comprehensive stress testing with many node instances. By September we should be able to get some initial results from the testnet (which we hope to share) and continue further improvement from there. Expect a big reveal in October! On the [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) front, the initial version of the server has reached the MVP stage and is ready for internal testing. The admin fee feature has been completed too. This allows Community Product administrators to specify the percentage of overall product revenue they #8217;d like to take as their fee. Of course, admins are at liberty to set their fee to zero. We have also started working on the Marketplace side to integrate new functionality and make it compatible with Community Products. In the coming months, we will also be releasing additional technical specs regarding Community Products, in order to allow existing and new community-driven projects to be built on top with greater ease. [Refreshed Streamr Developer Documentation](https://preview.redd.it/tkmtpt3bt7f31.png?width=827 amp;format=png amp;auto=webp amp;s=46b5a828a5eb849eef88ceba426f2ff66299e95f) Additionally, we are overhauling the current landing page to improve conversion rates on our website. The new technical documentation section is almost ready too, with expanded content and a new design, offering a much better flow and user experience to developers interested in building on top of our solutions. Finally, we are incredibly close to launching the stable version of the Core app. The beta version of the Core app has been open to the public for a few months now and we want to thank everyone who participated in our beta testing trial or provided feedback along the way. *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community-run* [*dev forum here*](http://forum.streamr.dev/)*.* As always, thanks for reading. Here #8217;s our regular list of updates: ## Network * Mock REST API for testnet and added new metrics to network. * Working on key exchange mechanism (still WIP): allows publishers and subscribers to exchange a symmetric key (used to encrypt and decrypt data) using asymmetric cryptography * Created fork of cloud-broker to push real-time data to testnet * Updating broker for new network version, also looking at testnet on AWS * Metrics from Broker nodes, done and merged, started on experiments for testnet * Setup mock API on Hetzner machine and Broker nodes reporting * Added tracker reporting to Streamr * Work on testnet optimisations: micro-batching on Cassandra * Bridged the real-time data in production to the test network and implemented some metrics reporting of the amount of data pushed to the test network ## Community Products * Managed to get tests running on local environment. Next step is push it to Docker * Implemented admin fee cut feature * From truffle to regular mocha for code testing * Looking now at persistence Community Products server ## Core app (Engine, Editor, Marketplace) * Doing design section of new page. Still some small responsive style fixes remaining * Docs review and editing, data price graph on landing page * Merged about us page, updates for top nav and mobile nav, updated partners section (more logos and structural changes) and timeline component * Worked on components of the  #8220;learn #8221; pages * Fixed the editor bug logged by user Frederic * Responsive views for tablets and mobile. Localisation support for the page * Sharing a session between tabs to avoid login each time * Fixed Editor bugs on autosave functionality * New routes for new product editor ## Labs * Apache Flink integration * Working on the airplane traffic streams * Demo for Community Products * Working with Kafka connect, starting Kafka streams *See the original post from Wei on* [*Medium*](https://medium.com/streamrblog/dev-update-july-2019-dc411ce52bd5)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Read about the advantages of IoT powered by decentralized data and how Elk and Streamr are working together to make this a reality.</TITLE>
    <DATE>2019-08-09 14:43:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Read about the advantages of IoT powered by decentralized data and how Elk and Streamr are working together to make this a reality</TITLE>
    <DATE>2019-08-09 14:43:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Read about the advantages of IoT powered by decentralized data and how Elk and Streamr are working together to make this a reality.</TITLE>
    <DATE>2019-08-09 18:09:24</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Request for literature recommendations - harmonic analysis of full works by the classical/baroque masters</TITLE>
    <DATE>2019-08-13 12:35:23</DATE>
    <TEXT>Hi there, can anyone recommend literature or websites that provide Roman numeral analysis of the masters? For example, this resource outlines the harmony of a full [Bach suite](https://www.teoria.com/en/tutorials/forms/suite/03-sarabande.php). Especially one that covers the Beethoven sonatas or symphonies would be great. It #8217;s quite hard to find analysis of complete works. If it also includes comments on form or motivic development, all the better. Thank you! (It #8217;s beneficial to do your own but it is time consuming if you want a quick reference on a certain aspect).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Request for literature recommendations - harmonic analysis of full works by the baroque/classical masters</TITLE>
    <DATE>2019-08-13 12:47:54</DATE>
    <TEXT>Hi there, can anyone recommend literature or websites that provide Roman numeral analysis of the masters? For example, this resource outlines the harmony of a full [Bach suite](https://www.teoria.com/en/tutorials/forms/suite/03-sarabande.php). Especially one that covers the Beethoven sonatas or symphonies would be great. It #8217;s quite hard to find analysis of complete works. If it also includes comments on form or motivic development, all the better. Thank you! (Of course it #8217;s beneficial to do your own but it is time consuming if you want a quick reference on a certain aspect).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr's Head of Comms, Shiv Malik explores the politics of personal data, and how crowdselling in a real-time data union can be the technical solution that gives back ownership</TITLE>
    <DATE>2019-08-13 16:16:20</DATE>
    <TEXT># How to Crowdsell Your Information Through a Data Union [The opening of the RadicalxChange Conference in Detroit in late March](https://preview.redd.it/n8uf9af8q8g31.jpg?width=2048 amp;format=pjpg amp;auto=webp amp;s=686b8e58c3deac502c92e4fe950653e9f055eaad) The way personal data is managed and profited from is a topic that is undoubtedly moving up the political agenda. In February, California #8217;s Governor proposed doling out the proceeds of the information industry with a  #8216;data dividend #8217;. US Senator Elizabeth Warren has suggested that the government not only break up tech giants but hold their CEOs criminally liable when breaches of data occur. These ideas, which might have seemed well outside the Overton window just a few years ago, appear to be gaining traction in an international legislative milieu, already fired up by the introduction of Europe #8217;s GDPR. There is activity within academia and at grassroots level too. Change.org recently hosted a petition to ensure car owners [get to keep the data](https://yourcaryourdata.org/) generated from their vehicles. And a fortnight ago, in Detroit, [academics](http://glenweyl.com/), [activists](https://www.baratunde.com/), [lawyers](https://cyber.harvard.edu/people/pdefilippi), [poets](http://www.kresgeartsindetroit.org/portfolio-posts/natasha-t-miller) and [developers](https://twitter.com/VitalikButerin?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) from well-funded, open source, technology projects in the crypto space, (including [Streamr](https://streamr.com/)) gathered for the inaugural RadicalxChange conference to push the agenda forward. There, American engineer and entrepreneur [James Felton Keith](https://medium.com/u/f07231ad51a8?source=post_page-----ec032289a51c----------------------) spoke alongside European MEP, Paul Tang, to detail their respective efforts to create data unions. These data unions are ostensibly political activist organisations that leverage existing laws to force businesses currently profiting off data generated in the workplace into fairer settlements with employees. Keith passionately believes that if workers are generating extra value in the digital sphere for business owners, they should be getting a cut of the extra revenue. And, as he made clear at the event, he will sue you to make it so.  #8203; [Data Union activists Paul Tang, MEP \(left\) and James Felton Keith \(right\). Photo courtesy of RadicalxChange](https://preview.redd.it/djvind0cq8g31.jpg?width=2039 amp;format=pjpg amp;auto=webp amp;s=9d44832a5ab93e0a9eb2ef09cf1087fe48e70771) Yet for all the political activism that #8217;s taking place, political and legal solutions will always fall behind the power of software. Why? Because they lack software #8217;s greatest asset; the ability to scale at near zero marginal cost. When it comes down to it, in a system in which trillions of packets of data, relating to billions of people, are being transferred, bought and sold in microseconds every single day, you simply can #8217;t employ enough people to police or negotiate fair settlement. However this sort of complexity is just the kind of thing that machines are good at. And that leaves society with a simpler strategy to realising data justice  #8212; use tech to fight tech. It #8217;s a concept readily envisioned by, amongst others, author and technologist Jaron Lanier, and academic and RadicalxChange conference co-organiser [Glen Weyl](https://medium.com/u/efb5397d83d0?source=post_page-----ec032289a51c----------------------). What the world needs is software versions of a data union  #8212; digital platforms that could easily scale what Keith and Tang are achieving through politics and law  #8212; digital platforms that will completely reshape power differences in the data economy to favour users, not the data giants. Software must be the medium, Lanier argues, through which the spoils of the Information Age can be fairly distributed. So why haven #8217;t technologists already developed software that can represent people controlling, owning and profiting from their personal data? It #8217;s a good question. The simplest response is that until the last few years, the three most basic elements needed to make up a market for personal data haven #8217;t really been devised, let alone developed. So what are those three elements? 1. People need to be able to **transfer ownership** of their data to someone else. If, for example, you own your house but couldn #8217;t ever transfer the title to anyone else, then the notion of  #8216;ownership #8217; becomes more than a little meaningless. 2. That asset needs to be **discoverable**. To use the house example again, if no one could ever know your house was for sale, then again, ownership loses its power. That #8217;s exactly what [marketplaces](https://marketplace.streamr.com/) are for. But in the case of individually generated data, there #8217;s a twist. This type of data is only really valuable in aggregate form. So individuals need to be able to bundle/unionise/aggregate that data so they can sell it as a crowd. They need their data to be discoverable *en masse*. 3. Data owners also need to **get paid** for their data. This turns out to be the last and perhaps the most technologically complex piece of the puzzle because if your data is only worth 20 cents, the regular fiat banking system isn #8217;t going help solve this problem. And that #8217;s where micro-payments facilitated by cryptocurrencies come in. Followers of the Streamr project will already know that alongside building a decentralised Network for real-time data, we #8217;ve also been working on a system for selling personal data called [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9). It #8217;s now possible to say that Streamr has developed solutions to all three of those elements outlined above. In February, we debuted our payments system [Monoplasma](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa), which can pay millions of people with a few clicks and near-zero transaction cost to the operator or purchaser of the data. (You can [read about it here](https://www.coindesk.com/ethereum-scaling-tech-monoplasma-wants-to-let-dapps-broadcast-crypto) and watch Streamr #8217;s co-founder Henri Pihkala demo in [this video](https://www.youtube.com/watch?v=t7vOoLBFkUA)). At RadicalxChange, we managed to present a crude demo of how we #8217;re solving parts one and two in the simplest way we knew; by getting attendees to create a data union right there at the conference. Designed by a member of our extended developer community, the demo is pretty simple. The idea is that by authorising a third party app to read their Spotify data, people can send the title of the tracks they play, along with album and artist information, to create a real-time data product that, for example, [economists](https://www.bbc.co.uk/news/business-43960998) from the Bank of England might be interested in purchasing. You can try it yourself if you like. All you need is a Spotify account. [Weilei Yu, Head of DevRel, manning Streamr #8217;s stall at RadicalxChange](https://preview.redd.it/cbyhkjkjq8g31.jpg?width=1400 amp;format=pjpg amp;auto=webp amp;s=907c6735385ad5d48d2d2c7695b0e4e44ce330ce) Simply scan the QR code below (or click this link [https://spotify.streamr.dev](https://spotify.streamr.dev/)) and after clicking the HTML link that pops up, you will be redirected to Spotify #8217;s interface. Spotify will then ask your permission to authorise a third party app. Press accept and that #8217;s it! You #8217;re now part of one of the world #8217;s first data unions. https://preview.redd.it/mx3hxu0nq8g31.png?width=300 amp;format=png amp;auto=webp amp;s=281a4e5d4c609f0a356d81a61431cdbf958be056 To test it, just play a song on Spotify. You can view a [live demo page here](https://spotify.streamr.dev/data), which will show you all data being contributed by users around the world in real-time. If you have a registered [Streamr](https://www.streamr.com/) account, which is free, you can see the canvas shown at [following link](https://www.streamr.com/canvas/editor/-HZBN3-0TVu9ec3L7zmtLwFgw6lJ0TTA-DBnkxtyb1Rg). (Please note: this is a very basic test at the moment and although this particular data set is publicly viewable, it won #8217;t be put on sale). [Streamr Core Editor Showing Spotify Data Union](https://preview.redd.it/pdnieebpq8g31.png?width=1914 amp;format=png amp;auto=webp amp;s=9db2d488f2c1dbd10ad66ca8d084749ef999f765) So where does this go from here? Well, Streamr core devs are currently working on the user interface (UI) for Community Products and ensuring that the backend, including the process for payments, functions smoothly. A first workable version of Community Products should be released during Q3. You can read [more about the general architecture](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) and timelines here. Concurrently, the core team and [Community Fund](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674) are already aiding developers within our ecosystem to design some pretty awesome applications to help the wider public aggregate and sell their data through Streamr #8217;s decentralized Marketplace. The most exciting of these is a [privacy-respecting plugin](https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-browser-plugin-d9050d2d9332) that works within the open source, Firefox browser, to help users push their search queries, Amazon data and basic social media data from Facebook and Twitter to a Community Product on the Marketplace. (App developers can of course make money by taking a cut of the sale of the products they are helping to create and administer). There are further apps being developed to allow people to sell data from their Fitbit, e-Scooters and mobile phones.  #8203; [A mock-up of a Community Product view.](https://preview.redd.it/72hytzhsq8g31.png?width=2000 amp;format=png amp;auto=webp amp;s=87de3c5d9eb2ad1798a7746e093b4fe059a77e22) If the concept of data unions really excites you, and you want us to keep you regularly informed, [drop us your email here](https://docs.google.com/forms/d/e/1FAIpQLSdR0D8xoOex7xRYVCY4_Bvuoqcrif8EJNX6fYsllhDa9Dvtcw/viewform?usp=sf_link). If you have your own ideas for an app, our [Community Fund](https://forum.streamr.dev/) might be able to give you a grant to get started. Otherwise, in a few weeks time, we hope that the team behind the Firefox plugin will be able to demonstrate their fantastic work, so stay tuned. *Read the original post from Shiv Malik on* [*Medium*](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-13 19:44:19</DATE>
    <TEXT>Thanks for the tip!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>A highlight recap of things being worked on in the last few weeks if you missed anything</TITLE>
    <DATE>2019-08-14 10:19:34</DATE>
    <TEXT>\- [User adoption strategy campaigns](https://www.youtube.com/watch?v=WRduSuy0_Sg). \- [WWF and Streamr sustainable fishing project](https://www.youtube.com/watch?v=8ir9H3WRg8c amp;feature=youtu.be). \- [Surf Streamr plugin launch on the horizon and generating interest for Community Products](https://www.reddit.com/r/ethereum/comments/cmd7ts/your_data_is_valuable_but_there_is_no_way_for_you/). \- Core app beta testing completed and launch planned. \- New website release near. \- [Network testnet deployed and dev docs coming](https://medium.com/streamrblog/dev-update-july-2019-dc411ce52bd5). \- [Streamr co-hosting Chainlink Hackernode at web3 summit (20th)](https://twitter.com/streamr/status/1157288241596370944). \- [Partnership with Elk and funding complete for their kickstarter](https://medium.com/elkrem/elk-partners-with-streamr-c2bc25ecbb92). \- [New CP project from Gang approved by Community Fund](https://forum.streamr.dev/d/51-create-a-mobile-app-to-gather-usage-metrics-across-apps).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Find the channel that fits you best! Your support and your voice are vital for adoption of the Streamr stack. Here's where to find Streamr!</TITLE>
    <DATE>2019-08-14 17:19:38</DATE>
    <TEXT> #8203; https://preview.redd.it/vk3ylsjw1gg31.png?width=1012 amp;format=png amp;auto=webp amp;s=af2c00c03582be1e0ee6c6c9c2308e87536d1912 [Twitter](https://twitter.com/streamr) [Streamr Dev Community Forum](https://forum.streamr.dev) [GitHub](https://github.com/streamr-dev/) [Medium](https://medium.com/streamrblog) [Telegram](https://t.me/streamrdata) [Telegram Announcements](https://t.me/streamrofficial) [Reddit](https://www.reddit.com/r/streamr) [Peepeth](https://peepeth.com/streamr) [LinkedIn](https://www.linkedin.com/company/streamr-network/) [YouTube](https://www.youtube.com/channel/UCGWEA61RueG-9DV53s-ZyJQ)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-14 19:05:35</DATE>
    <TEXT>Thanks a lot!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-15 13:44:23</DATE>
    <TEXT>Telegram for discussion Twitter for number of followers.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Labs has created publish and subscribe processors for Apache NiFi. Connect your streams and integrate real-time data to multiple systems at scale without the typical data flow challenges.</TITLE>
    <DATE>2019-08-15 15:32:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Next Tuesday, Streamr, Chainlink, and Amberdata are hosting a Developer Mentorship where devs can learn how to use tools from the three projects. Join the event 15:40  #8211; 18:00 at Web3 Summit!</TITLE>
    <DATE>2019-08-16 10:43:44</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core is OUT NOW! Developers and data scientists working with real-time data, this is your all in one toolkit to leverage the power of data and smart contracts with ease.</TITLE>
    <DATE>2019-08-19 11:52:42</DATE>
    <TEXT># News: Streamr launches Core app *Free-to-use app gives developers and data scientists rapid insights and easy smart contract integration without needing to establish separate backend infrastructure.* https://preview.redd.it/x8v7gc2n2eh31.png?width=3840 amp;format=png amp;auto=webp amp;s=6b6fa98a177f7c099e6acb68100261ba73dbb4a1 Developers and data scientists working with real-time data will now be able to use an innovative free toolkit to leverage the power of smart contracts with complete ease. Released today, the [Streamr](https://www.streamr.com/) project #8217;s open source app, [Core](https://www.streamr.com/core), allows organisations who lack specialist Web3 programming skills to freely experiment with getting real-world events to automatically trigger monetary payments. Core is seamlessly integrated into the project #8217;s [open data Marketplace](https://www.streamr.com/marketplace) and a dedicated backend [data streaming network](https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b), so that programmers and data science teams will be able to fetch, push and sell new data products with total convenience. Bypassing the need for specialist coding skills and complex data infrastructure, the new application is expected to save organisations days or even weeks of work when prototyping and testing new product ideas involving real-time data pipelines.  gt;* #8220;Companies involved in industries like IoT, insurance, transport, social media and logistics should all find the Core app incredibly useful, #8221; said Henri Pihkala, Streamr #8217;s Co-Founder.*  gt;  gt;*After a good number of months, with input from developers across four continents, we #8217;re all looking forward to establishing a new way of working with real-time data that enables far more social sharing, easy smart contract integration, beautifully simple visualisations and proper Web3 integration.* Potential use cases include: * **Linking Web2 data to trigger Web3 events**: Streamr Core offers built-in modules to easily interact with smart contracts, deployed on the Ethereum blockchain, without any coding involved. With these modules, users will be able to listen to on-chain events, push data to smart contracts or bridge real-world events with smart contract triggers, via Oracles. * **Gain insights into big data sets**: Data scientists can quickly check and inspect data sources in real-time by connecting, for example, public transportation data and visualising this data as a heat map. * **Ideation and prototyping**: Create MVPs within your organisation without weeks of coding and the need to set up a backend. Utilising a modular editing system similar to [IBM #8217;s Node-Red](https://medium.com/streamrblog/streamr-node-red-integration-tutorial-b0b410496354), Core #8217;s inbuilt modules allow users to easily visualise their data and then share those results through social media on a live canvas.  #8203; https://reddit.com/link/csg106/video/aslyvsc29eh31/player Web3 sign-in and identity, alongside traditional email, allows decentralized accounts to  #8216;own #8217; data streams. This means that entities such as AIs, machines, or multisignature DAOs can control and benefit from Core app outputs, creating a paradigm shift in data ownership and management. The app also offers rich developer documentation, smart guidance features on connecting and searching for modules, a stunningly designed user interface and nearly 170 modules readily available to cover most use cases. Other useful modules include aggregate functions applied to streaming data to do real-time calculations and easy drag-n-drop charts creation, all of which immediately update as events are received. Streamr Core also offers an advanced option for users to build their own custom module in Java, directly from the same UI interface, ready to be used on the spot, without any build or deployment requirements. Developers and data scientists who would like to learn more can join an upcoming webinar taking place on the **4th of September**. Further details will be announced soon on [Twitter](https://twitter.com/streamr), [Reddit](https://www.reddit.com/r/streamr/) and [Telegram](https://t.me/streamrofficial). Core app documentation can be found here: [streamr.com/docs](http://streamr.com/docs) If you #8217;re attending **Web3 Summit** or **EthBerlinZwei** in Berlin this week Streamr Core devs will be available to give live demos on site. We will also be at a series of events. * **Tuesday**: developer workshop with Amber Data and Chainlink, diving into the details of the Core app. * **Wednesday**: developer Jonathan Wolff will present Ethereum canvases created with Core. Both of those sessions take place at the Chainlink Hackernode at Web3. * **Friday  #8212; Sunday**: you will be able to find the Streamr team in the EthBerlin exhibition area showcasing Core. # More on the Streamr Stack With Core, there #8217;s no need to set up a backend. Users simply log into the Core app where they can then create and manage data streams, and use visual programming to manipulate and visualise real-time data flows with a few clicks. The underlying component powering the Core app is the Streamr Network, which is a peer-to-peer, decentralized, publish/subscribe messaging network. The [Streamr](https://streamr.com/) Network is a set of open protocols for real-time data distribution and exchange in a decentralized fashion. In addition to the Network, the Streamr stack includes a [Marketplace](https://www.streamr.com/marketplace) for data sharing and monetisation, and canvases for data processing and analytics as part of the Core web app. Users can interact with Streamr today using the Core user interface, [SDKs](https://github.com/streamr-dev), or directly via the [API](https://www.streamr.com/docs/streamr-api). To learn more visit [www.streamr.com](http://www.streamr.com/). *You can follow Streamr on* [*Twitter*](http://streamrinc/)*,* [*Reddit*](https://www.reddit.com/r/streamr/) *and* [*Telegram*](https://t.me/streamrofficial)*. For* ***media enquiries*** *contact* [*marlene@streamr.com*](mailto:marlene@streamr.com) *cc* [*media@streamr.com*](mailto:media@streamr.com)*. For* ***developer enquiries*** *contact* [*weilei.yu@streamr.com*](mailto:weilei.yu@streamr.com)*.* *See the original post on* [Medium](https://medium.com/streamrblog/streamr-core-app-smart-contract-integration-real-time-data-visualizations-9180e343c7a)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core is OUT NOW! Developers and data scientists working with real-time data, this is your all in one toolkit to leverage the power of data and smart contracts with ease.</TITLE>
    <DATE>2019-08-19 12:21:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>"For us, smart contracts are one of the subscribers that data can have. In addition to the things we're doing with Chainlink ... we also offer tooling for boosting that side of things. In the Core App you can connect data to smart contracts." - Henri Pihkala today at Web3 Summit.</TITLE>
    <DATE>2019-08-19 16:16:32</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Calling devs! Streamr Core is a real-time data toolkit, giving you all the basic tools needed to integrate, process and visualise your data streams. Try the app.</TITLE>
    <DATE>2019-08-20 14:24:38</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-20 14:25:53</DATE>
    <TEXT>Sign up and try the app! [https://www.streamr.com/core](https://www.streamr.com/core)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr Community Hero for August is  #129345; ... Darko Horvat! A big thank you for your support in the community channels over the past year. A Streamr goodie pack is heading your way!</TITLE>
    <DATE>2019-08-21 14:33:33</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr's Head of Comms, Shiv Malik is giving a talk today on Ethereum governance and Round Robin system design at #ethberlinzwei, titled 'A Defense of Szabo's Law'.</TITLE>
    <DATE>2019-08-22 14:28:03</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #10071; #65039; There is a fake website posing as a Streamr Community Product (selldata . org). While imitation is the sincerest form of flattery, this is a scam. Do NOT download CP plugins unless endorsed by official Streamr accounts.</TITLE>
    <DATE>2019-08-26 07:18:27</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Create your own smart fridge and send the data in real-time to the Streamr Marketplace with this tutorial. See the cognitive IoT architecture constructed using RuuviTags, Raspberry Pi, Node-RED, Streamr, and IBM Cloud Watson Studio</TITLE>
    <DATE>2019-08-26 11:53:02</DATE>
    <TEXT># How to build your own smart fridge *Streamr in a cognitive IoT architecture with RuuviTags, Node-RED, IBM Cloud Watson Studio* [Learn how to create your own smart fridge using a RuuviTag, Raspberry Pi and the instructions below. \*Red Bull not included.](https://preview.redd.it/nyvtao0y5si31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=22b6bf4f184d57433d4d821b46532129c728424f) The most important devices in IoT systems are sensors and actuators. Sensors collect data from the real world and actuators interact with it based on the data collected  #8212; this is why data analysis is central to IoT systems. A cognitive IoT architecture is a specific type of IoT implementation. Utilising machine learning models trained in a cloud or cluster, a cognitive IoT system makes the majority of its real-time decisions at the edge. This way, decisions are made quickly each time a data set arrives from the sensors. Further analysis of the data and tweaking of the edge model can be done in the cloud later. The architecture outlined below is what might be implemented to create a  #8216;smart #8217; fridge: Two [RuuviTag](https://ruuvi.com/) sensors were placed in the fridge door at Streamr #8217;s office. The data was published via BlueTooth LTE. A Raspberry Pi was set up to collect it. A Node-RED flow in the Raspberry Pi then aggregated and filtered the data to detect events such as the door being opened. The raw data with some aggregations was then sent as a stream in Streamr. A Node-RED instance in IBM Cloud then subscribed to that stream. The data was then further analysed in the cloud. Streamr is used to connect IoT sensor data, collected by a gateway device, to IBM Cloud. This makes connecting the real-time data from the edge to the cloud very simple. An additional benefit of using Streamr is the mechanism for data monetisation. Streamr could be used to sell data collected by user-owned sensors. [The Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) system being developed by Streamr could be employed in this case to create monetised [data union](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c) firehoses. ## An ideal cognitive IoT-architecture with the used technologies https://preview.redd.it/pcvgep926si31.png?width=791 amp;format=png amp;auto=webp amp;s=1aacbd2e85cee1768d902b1ac52147b1114ed778 ## The implemented architecture https://preview.redd.it/4q58xvb96si31.png?width=791 amp;format=png amp;auto=webp amp;s=6b100875f4134da486f1ccad3f288aa1acca8c4a # Step-by-step set up In the above images, we see the role of the edge more clearly. However, an actual machine learning model isn #8217;t used. Instead, more classical statistics are used to implement anomaly detection. The cloud service that was used was IBM Cloud, which provides a free starter kit that enables access to Node-RED for easy cloud integrations. (Watch [this](https://www.youtube.com/watch?v=Tk0sHowF3I0) video to learn more about the starter kit). Streamr [has a library readily available in the Node-RED palette](https://medium.com/streamrblog/streamr-node-red-integration-tutorial-b0b410496354), so no extra configuration outside of creating and connecting to a stream is required. The library to publish and subscribe to Streamr is found in Node-RED by going to: Manage pallette - gt; Install - gt; Search for: node-red-contrib-streamr - gt; Install After installing the library, you should create a stream in [Streamr #8217;s editor](http://www.streamr.com/). You need your [Streamr API](https://www.streamr.com/docs/getting-started#get-api-keys) key and the created stream #8217;s ID to connect to it in Node-RED. Then you should drag either the Streamr sub node for subscribing to a stream or the Streamr pub node for publishing data to the canvas. By double-clicking the node, you open up the properties where you can copy and paste your API key and stream ID. Node-RED lowers the threshold for new IoT developers significantly. It is really useful even for more experienced IoT developers because it simplifies data flows. And as Node-RED is built in JavaScript, its dynamic nature also allows developers to inject JS functions into the flow to create data transformations and aggregations. Here, Node-RED is also used to connect the RuuviTag sensors to the Raspberry Pi. Connecting the RuuviTags to Node-RED is pretty easy, as demonstrated [here](https://github.com/ojousima/node-red). If you are using a Mac to connect the RuuviTags you might need to reconfigure the Noble library that is used to establish Bluetooth connections in Node-RED. (More info for Mac users is [here](https://github.com/Timeular/noble-mac)). If you are using a Raspberry Pi as the edge gateway device, you need to downgrade/upgrade your Node.js version to 8.x. This has to be done for the Noble library to work.  #8203; [Node-RED flow in the edge \(Github for import\): https:\/\/github.com\/juslesan\/officefridge\/blob\/master\/flows\/edge\_flow.json ](https://preview.redd.it/49h4hsud6si31.png?width=1600 amp;format=png amp;auto=webp amp;s=3519eed1970582e4069e7df3669f4350aa20c750) The data fields sent by the RuuviTags via Bluetooth * deviceUuid * humidity * temperature * pressure * battery * detectedAt * accelerationX * accelerationY * accelerationZ The three acceleration axes are aggregated to a single value representing the total acceleration of the sensor. Then the edge flow calculates a [moving z-score](https://developer.ibm.com/tv/moving-zscore-based-anomaly-detector-on-the-iot-edge-using-nodered/) anomaly detection algorithm for the g-force calculated from the sensors in the fridge door. This way one can make fairly certain assumptions about when the fridge door is opened or closed based on z-score. The current window for the algorithm is 60 data points, which should mean that the window corresponds to one minute because the RuuviTags send data once per second. The z-score threshold for anomaly detection can be changed by sending an API request to the Node-RED instance in the cloud. The Edge flow gets the threshold value for the anomaly detection from the cloud in one-minute intervals. It #8217;s probably not necessary to have the interval set so high because new models may only be processed in IBM Cloud once per hour. However, as manual API requests are currently the only real way to set the threshold, a one-minute interval is probably good. There is also a filter function for events with timestamps that are three seconds apart. This way we can filter away data that can be assumed to be corresponding to the same event. You can import the flow from [Github](https://github.com/juslesan/officefridge/tree/master/flows) if you wish to analyse the flow and its functions further.  #8203; [The node-red flow in IBM bluemix \(Github for import\): https:\/\/github.com\/juslesan\/officefridge\/blob\/master\/flows\/cloud\_flow.json ](https://preview.redd.it/b1xy5pjj6si31.png?width=1600 amp;format=png amp;auto=webp amp;s=a4a92eae12787b3f8a868c392bf299b6e2fcd562) Temperature and humidity data is aggregated to means and z-scores and then pushed to a database, so that more analysis can be done on the data later on. The get functions are for updating and getting the edge model. The update function can be called by: {node-red-instance-base-url}/updateGZscore?new=0.5 The value for new has to be between 0 and 1. The model update can be called from the Jupyter Notebook that has been set up as the data analysis platform in IBM #8217;s Watson Studio. Doing machine learning in batches using the Notebook instances with Apache Spark is a great way to update the edge models when they are required.  #8203; [Jupyter notebook with spark in IBM Watson Studio](https://preview.redd.it/5nu5bv6o6si31.png?width=1600 amp;format=png amp;auto=webp amp;s=bd9ca83478270e8f8a14d3387c86a8a176953019) In the picture above you can see the basics of how to use Notebook with Apache Spark and Python in IBM #8217;s Watson Studio. Watson Studio also supports Notebook with Scala. (Check out [this](https://www.youtube.com/watch?v=h5r_4wurwDk) video to find out how to launch Jupyter Notebook instances in Watson Studio). You can also see how a Cloudant connection can be established to the Notebook in the picture above. You can create read-only credentials easily in IBM Bluemix #8217;s console for Cloudant. Just go to the permissions tab in your Cloudant database #8217;s console and click on generate API key. This gives you a username and password for the database.  #8220;Cloudant.host #8221; is simply the base url of your Cloudant database. After the Cloudant connection is set up you can easily make SQL queries with Spark to analyze or visualize the data. As you are able to set up Jupyter Notebook to run, for example, every hour in the cloud, it is possible to update the models in the Raspberry Pi at those intervals. For example, you could train a logistical regression model in the cloud. Then the new model is updated to the edge by an HTTP request. However, setting up a Raspberry Pi to receive external HTTP requests takes a little bit more work because you need to set up routers and take care of security. Creating external connections to edge gateway devices is not a recommended security practice. Instead, the gateway should be responsible for making the GET or POST requests to the cloud. This is why there is no REST API in the Raspberry Pi when used as an edge gateway. Instead, it gets its model from the Node-RED instance in the cloud and the model updates after data analyses are first updated to the cloud.  #8203; [Example of Notebook with Spark. Graph displayed by pixiedust](https://preview.redd.it/tynu1g4s6si31.png?width=1600 amp;format=png amp;auto=webp amp;s=547917d9239c873c0f5ce452faadd53b1ff99a09) IBM has connected Apache Spark and Notebook for fast data exploration. The Pixiedust library is used to visualise the data. By doing some exploring, I was able to decide on which model to use to filter events where a door being opened is likely. Logistical regression and anomaly detection were the leading candidates. I decided to use sliding z-score anomaly detection for this use case. Anomaly detection is a safer method because the default total acceleration of an immobile RuuviTag might change based on which side it #8217;s laying on. Violent door openings did quite often flip the RuuviTags around. The meanTotalAcceleration displayed in the graph is calculated at the same time with the sliding z-score, so it is calculated with the same window of data. This also means that the mean is also sliding. All the spikes in the graph can easily be found by the z-score anomaly detection. However, it could be useful to implement a logistical regression model that would be used to estimate whether or not a door has been opened without z-score as multiple door openings in the same window might not register as anomalies. [React app displaying door events. Demo: https:\/\/sujuwa-fridge.herokuapp.com\/ ](https://preview.redd.it/f9uq70vu6si31.png?width=1454 amp;format=png amp;auto=webp amp;s=3c031f24c4659d5819bc6153a811a3b65a8e96e6) More technical documentation can be found in the code [repository](https://github.com/juslesan/officefridge) of the example application. # Results As the application has been running for two months now we have some interesting data to share from the 1,700,000+ RuuviTag data points stored in Streamr. The free version of IBM Cloud #8217;s Cloudant can only store up to 500 MBs so Streamr #8217;s historical data capabilities are of great use here. Starting with the g-force of the RuuviTags, the average g-force of all data points is at 1.05 G. This is roughly equivalent to the Earth #8217;s gravitational pull. However, for the anomaly detected door events the average g-force of the recorded events is below that, at 0.937 G. This is most likely because the RuuviTags were set up to move as much as possible, which caused them to jump around a bit. Effectively the RuuviTags could be in near-weightless states at the peaks of the jumps. You can see this visualised in the acceleration graph earlier in the blog. Moreover, the lowest point of total acceleration recorded was 0.071 G. On the other end, the highest acceleration recorded was at 1.7 G. Perhaps someone was having a bad moment at 13:39 on May 29th. We can also analyse the recorded temperature, humidity and pressure fields of the RuuviTags. By looking at the highest temperature recorded at 13.66** #176;**C, we can assume that the fridge door was left open for quite a while on June 4th between 6 and 7 PM. The application has also confirmed that the office fridge can reach freezing temperatures, as the minimum temperature during the two months was -0.94** #176;**C. The RuuviTags measured the lowest pressure in the fridge on the 5th of July at 14:49. This happened to correspond to the weather event shown below. [Weather in Helsinki during the lowest measured pressure in the fridge](https://preview.redd.it/ekau8boz6si31.png?width=1400 amp;format=png amp;auto=webp amp;s=53630b524e789a9331bf857aa8ff564e86ab1021) Overall the pressure did not change much in the fridge, and the minimum and maximum measurements weren #8217;t that far from each other. Still, it #8217;s interesting to see a clear low-pressure-area move through Helsinki when the lowest amount of pressure was measured. Lastly, unsurprisingly the humidity of the fridge correlated with the temperature of the fridge. These two values went pretty much hand in hand throughout the recorded two months. # Conclusion Hopefully, the implemented architecture and historical data analysis gave you an idea about how to use Streamr in IoT and other data-driven applications. All of the data you publish to Streamr can be published on the [Marketplace](https://marketplace.streamr.com/) where you can share it for free or sell it to other users interested in your data. You can find the two data streams from this experiment (one for continuous RuuviTag data and the other for anomaly detected data points) [here](https://www.streamr.com/marketplace/products/4279e72eab50485a965c5f873d7aa4ee912900f9e209479f8d1868590eecb608). *See the original post from* [*Santeri Juslenius*](https://medium.com/@santeri.juslenius?source=post_page-----d834bb6dc779----------------------) *on* [*Medium*](https://medium.com/streamrblog/streamr-build-smart-fridge-iot-ruuvitag-node-red-ibm-cloud-fridge-d834bb6dc779)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Create your own smart fridge and send the data in real-time to the Streamr Marketplace with this tutorial. See the cognitive IoT architecture constructed using RuuviTags, Raspberry Pi, Node-RED, Streamr, and IBM Cloud Watson Studio</TITLE>
    <DATE>2019-08-26 11:53:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-08-27 11:33:34</DATE>
    <TEXT>It turns out this was a community member who made a demo site to promote the plugin and forgot to set a password. Still, better to be safe than sorry regarding third party plugins. We #8217;re working on a page for approved third party plugins. You will also be able to see them on the marketplace and check the smart contract address.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Join Streamr devs for an  #8216;Introduction to Streamr Core #8217; September 4th, 13:00 GMT+1</TITLE>
    <DATE>2019-08-28 10:09:24</DATE>
    <TEXT> #8203; https://preview.redd.it/ejo26cx4z5j31.png?width=1200 amp;format=png amp;auto=webp amp;s=548a851c273a07eab9a0fd1c062f67ce4937c462 In this Webinar, you will learn how to: 1. Create streams using Streamr Core. 2. Learn Web3 sign in and basic smart contracting functions. 3. Get an introduction to Core's basic modules including tables and graphs. Places are limited so register here not to miss out! [https://zoom.us/webinar/register/WN\_StRinVSYRNaLAo0saDUdXg](https://zoom.us/webinar/register/WN_StRinVSYRNaLAo0saDUdXg)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Join Streamr devs for an  #8216;Introduction to Streamr Core #8217; September 4th, 13:00 GMT+1</TITLE>
    <DATE>2019-08-29 05:52:43</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core's Web3 sign-in, identity, and payment processes can create a paradigm shift in data ownership and management for DAOs and AIs. Thoughts from Berlin Blockchain Week</TITLE>
    <DATE>2019-08-29 11:50:26</DATE>
    <TEXT># DAO #8217;s And The Data Economy  #8203; [Streamr Co-founder, Henri Pihkala, just after Core #8217;s launch at the Web3 Summit in Berlin 2019, on a panel hosted by Chainlink](https://preview.redd.it/zd7dq57rldj31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=3b3b0bdc10a90d8aa88ccf48dcbab7c451de5a7f) What happens if you create Web3 sign in and payments process for a data marketplace? Traditional data marketplaces, such as Thompson Reuters and Dun and Bradstreet require purchasers to have credit cards, business address etc. But with Ethereum addresses acting as both payment facilitation mechanism and user identity, you quickly realise that AIs, DAOs (Decentralized Autonomous Organisations), and machines can all start owning, controlling and selling real-time data. That #8217;s a huge deal because instead of being [interesting experiments in consensus decision making](https://humanitydao.org/), or curious investment vehicles, suddenly DAOs can get serious. They can generate revenue and (effectively) own hefty assets in the data economy without needing to create complex legal structures beforehand. For those familiar with the [Streamr white paper](https://www.streamr.com/whitepaper) this was always the development intention, but the fact that it #8217;s so easy to do in Streamr #8217;s [new Core app](https://medium.com/streamrblog/streamr-core-app-smart-contract-integration-real-time-data-visualizations-9180e343c7a), (released last week) means that Core #8217;s Web3 sign-in and identity management is likely to create a paradigm shift in data ownership and management. [Streamr](http://www.streamr.com/) team members are currently in talks with various DAO projects to chart a way in which DAOs could operate innovative new data products, called digital data unions, which control real underlying assets potentially worth tens or hundreds of millions of dollars. Imagine an [Aragon](https://aragon.org/) or [Colony](https://colony.io/) DAO controlling massive user data sets from Twitter [or Spotify](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c)? We #8217;re a few steps away from this becoming a reality.  #8203; [The Core app interface that allows DAOs to own and control their data](https://reddit.com/link/cwzt7s/video/tbrnhylvldj31/player) ## Launching Core at EthBerlin While participating at the Web3 Summit and EthBerlin events, we were able to talk to numerous projects working in the Web3 ecosystem, ranging from decentralized finance (DeFi) by Kyber, to scaling solutions via plasma by Matic, payment channels by Adex and decentralized computation by DappNode. While presenting our Streamr ecosystem and long-term vision, they were all very interested in our roadmap. This was especially true when we presented the idea for our [upcoming Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) platform and how it will help us open the data ecosystem without any extra effort from users #8217; perspective. It was interesting to note that while most current blockchain projects were primarily targeting crypto ecosystem users, still a very limited pool, the real audience for Streamr comes from the mature Web2 user base, especially since Streamr Community product users can leverage the EU #8217;s new data portability rules (see GDPR article 20). This means Community Products acts as a dApp that isn #8217;t limited by the blockchain community #8217;s growth. On the other hand, many potential solutions developed in the Web3 ecosystem could further progress Streamr #8217;s vision. For example, DappNode sells hardware equipment intended to power various decentralized applications running on top of it. This could enable, potentially, existing owners of this hardware to easily run Streamr Network broker nodes around the globe. We also showed-off Streamr #8217;s Core app to scores of Web3 developers over Berlin Blockchain Week. Many devs were impressed with the nearly 170 built-in modules which easily interact with Ethereum smart contracts and need no separate coding. They give anyone (including DAOs!) the ability to listen to on-chain events or freely experiment with connecting real world events to smart contract triggers via Oracles.  #8203; [The Streamr stand at EthBerlinZwei](https://preview.redd.it/b4e5dwrzldj31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=7f7502aeb12472ed53568a16c40769e6a9feddc6) Utilising a modular editing system similar to IBM #8217;s Node-Red, users can easily visualise their data, then share those results through social media on a live canvas. The app also offers rich [developer documentation](https://www.streamr.com/docs), smart guidance features on connecting and searching for modules, a stunningly designed user interface. Streamr Core also offers an advanced option for users to build their own custom module in Java, directly from the same UI interface, ready to be used on the spot, without any build or deployment requirements. It really is an exciting new toolkit for anyone working at the intersection of Web3 and data productization! *Developers and data scientists who would like to learn more can join an upcoming webinar introduction to Streamr Core taking place 4 September 13:00 GMT+1. Places are limited so* [*register here*](https://zoom.us/webinar/register/WN_StRinVSYRNaLAo0saDUdXg) *not to miss out!* *See the original post from Weilei Yu on* [*Medium.*](https://medium.com/streamrblog/dao-data-economy-ethereum-oracles-39e1e3ab160c)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core's Web3 sign-in, identity, and payment processes can create a paradigm shift in data ownership and management for DAOs and AIs. Thoughts from Berlin Blockchain Week</TITLE>
    <DATE>2019-08-30 10:54:58</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Devs and entrepreneurs - submit your proposal for a real-time data stream, product, or integration using the Streamr tech stack and get backing from the Community Fund!</TITLE>
    <DATE>2019-08-30 12:22:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core's Web3 sign-in, identity, and payment processes can create a paradigm shift in data ownership and management for DAOs and AIs. Thoughts from Berlin Blockchain Week</TITLE>
    <DATE>2019-08-30 14:24:08</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core's Web3 sign-in, identity, and payment processes can create a paradigm shift in data ownership and management for DAOs and AIs. Thoughts from Berlin Blockchain Week</TITLE>
    <DATE>2019-09-02 09:43:13</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core's Web3 sign-in, identity, and payment processes can create a paradigm shift in data ownership and management for DAOs and AIs. Thoughts from Berlin Blockchain Week</TITLE>
    <DATE>2019-09-02 10:33:20</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Devs and entrepreneurs - submit your proposal for a real-time data stream, product, or integration using the Streamr tech stack and get backing from the Community Fund!</TITLE>
    <DATE>2019-09-02 11:55:40</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Create your own smart fridge and send the data in real-time to the Streamr Marketplace with this tutorial. See the cognitive IoT architecture constructed using RuuviTags, Raspberry Pi, Node-RED, Streamr, and IBM Cloud Watson Studio</TITLE>
    <DATE>2019-09-02 12:24:30</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>In this series of videos, Ben Sheppard, Head of Partnerships, talks about building incentivised data economies to improve the way information is traded. Take a look</TITLE>
    <DATE>2019-09-02 15:12:53</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has launched its Core app and invites you to a webinar introduction and Q amp;A tomorrow at 13:00 (GMT+1)</TITLE>
    <DATE>2019-09-03 13:29:07</DATE>
    <TEXT> #8203; https://preview.redd.it/awjtj9k2sdk31.png?width=2400 amp;format=png amp;auto=webp amp;s=ebd934aee75685f5f084161e6438e873edea9430 [Streamr Core](https://medium.com/streamrblog/streamr-core-app-smart-contract-integration-real-time-data-visualizations-9180e343c7a) is a free-to-use open source toolkit for developers and data scientists to create, integrate, process and visualise real-time data streams for fast prototyping and insights. Core has smart contract integration, visual analytics tools, web3 sign in, payment, and identity management, all integrated with an open data Marketplace and back-end streaming network to bypasses the need for specialist coding skills or complex infrastructure. The 1 hour webinar introduction to the app is on September 4th, 13:00 GMT+1. In the webinar, you will learn how to: 1. Create streams using Streamr Core. 2. Learn Web3 sign in and basic smart contracting functions. 3. Get an introduction to Core's basic modules including tables and graphs.Places are limited so register here not to miss out! [https://zoom.us/webinar/register/WN\_StRinVSYRNaLAo0saDUdXg](https://slack-redir.net/link?url=https%3A%2F%2Fzoom.us%2Fwebinar%2Fregister%2FWN_StRinVSYRNaLAo0saDUdXg amp;v=3) If you would like to submit a question for the devs in advance, please comment on this thread: [https://forum.streamr.dev/d/62-join-streamr-devs-for-an-introduction-to-streamr-core](https://slack-redir.net/link?url=https%3A%2F%2Fforum.streamr.dev%2Fd%2F62-join-streamr-devs-for-an-introduction-to-streamr-core amp;v=3)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has launched its Core app and invites you to a webinar introduction and Q amp;A tomorrow at 13:00 (GMT+1)</TITLE>
    <DATE>2019-09-03 19:35:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>An introduction to Streamr and how to build Ethereum workflows in this talk given at the Chainlink Hackernode last month at ETHBerlin</TITLE>
    <DATE>2019-09-04 13:59:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to turn your fridge into a real-time data point and offer the info on the Streamr Marketplace in this tutorial demonstrating a cognitive IoT architecture.</TITLE>
    <DATE>2019-09-05 15:32:41</DATE>
    <TEXT>[https://medium.com/streamrblog/streamr-build-smart-fridge-iot-ruuvitag-node-red-ibm-cloud-fridge-d834bb6dc779](https://medium.com/streamrblog/streamr-build-smart-fridge-iot-ruuvitag-node-red-ibm-cloud-fridge-d834bb6dc779)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-05 15:32:59</DATE>
    <TEXT>When the Community Products feature is added you can bundle  amp; sell aggregated data like this together with others to extract its value. [https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Join community dev Ebrahim Khalilzadeh who is leading the team building a plugin to monetise your browser data, and Streamr #8217;s Head of Comms Shiv Malik for a Community Products AMA on 12th Sept 15:30 CEST</TITLE>
    <DATE>2019-09-06 14:19:13</DATE>
    <TEXT> #8203; [Questions will be answered here on the 12th](https://reddit.com/link/d0ha2f/video/1q5lhrr3czk31/player) Please add your questions on the Surf-Streamr plugin or Community Products on this thread. **Surf-Streams browser plugin** [Facebook module within the Surf-Streamr browser plugin](https://preview.redd.it/t36y7wllezk31.jpg?width=1920 amp;format=pjpg amp;auto=webp amp;s=f3cdfc770754258b3c53c89e9e0a19a0e06bc9a9) Ebrahim Khalilzadeh is a community developer leading the Surf Streamr plugin for users to monetise their browser data by sending it to a data union for crowdselling on the Streamr Marketplace. The Surf-Streamr plugin will be available for Mozilla Firefox, Google Chrome and Microsoft Edge (Chromium version). It comes with advance privacy settings and will utilises the Community Products feature coming to the Marketplace. [Read more about the plugin and watch the demo.](https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-browser-plugin-d9050d2d9332) **Community Products** [The functional flow of a Community Product](https://preview.redd.it/aophx5ihezk31.png?width=2000 amp;format=png amp;auto=webp amp;s=f46a5a4399aa2629cf6fecfbc434d1b1a965abd6) Community Products is an upcoming feature for the Marketplace where users push the data they create into a data union and receive automatic payment every time that product sells. This is a unique feature for the real-time data industry, where information from multiple users is aggregated into a product large enough to extract its value and interest data buyers. [Read more about Community Products.](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) See you here on the 12th!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Full movement Roman numeral analysis of classical/baroque compositions</TITLE>
    <DATE>2019-09-07 12:46:59</DATE>
    <TEXT>I'm looking for harmonic analysis of full works by the master composers in roman numerals (if it includes motive/theme development or form analysis too, all the better). Can anyone share any links, recommend textbooks, online journals, or other learning resources with analysis of full movements? This is what I have found so far: \- [Analysis of Beethoven's "Waldstein" sonata](https://www.teoria.com/en/articles/waldstein/) \- [Full Bach suite with harmonic analysis for each movement](https://www.teoria.com/en/tutorials/forms/suite/) \- [Harmonic analysis of Bach "Air" from orchestral suite no.3](https://www.youtube.com/watch?v=NGmQixzNdu0) \- [Several harmonic analysis videos of Bach, Chopin, Tchaikowsky, Mozart](https://www.youtube.com/user/MusicTheoryTCC/videos) \- [Several harmonic analysis videos of Beethoven, Bach, Schubert, and other romantic composers](https://www.youtube.com/watch?v=OyvSrpQ2eWY amp;list=PL613D1A6B3C4BBDF2) \- [Several harmonic analysis videos of Beethoven, Mozart, Bach](https://www.youtube.com/user/Drewsical/videos) \- [Analysis videos of form, a little harmony/key relationships, and motive/theme development in the Beethoven piano sonatas](https://www.youtube.com/watch?v=AC5iQAbMrGg amp;list=PLmwnQkTVb4ocLtDLBgvx4IH9byoIkxMDk) \- [Here is an early version of my own composition for piano annotated with harmonic intentions](https://musescore.com/user/2501446/scores/5702805/s/ridPc6) I have several textbooks, such as Walter Piston's 'Harmony', but they do not contain analysis of full movements. (Roman numerals do not give a complete picture of a composition but I find them a useful reference to understand phrase structures, modulations, sequences, theme development and other aspects of form in the common practice). Thanks in advance!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Full movement Roman numeral harmonic analysis of classical/baroque compositions</TITLE>
    <DATE>2019-09-07 12:57:55</DATE>
    <TEXT>I'm looking for harmonic analysis of full works by the master composers in roman numerals (if it includes motive/theme development or form analysis too, all the better). Can anyone share any links, recommend textbooks, online journals, or other learning resources with analysis of full movements? This is what I have found so far: \- [Analysis of Beethoven's "Waldstein" sonata](https://www.teoria.com/en/articles/waldstein/) \- [Full Bach suite with harmonic analysis for each movement](https://www.teoria.com/en/tutorials/forms/suite/) \- [Harmonic analysis of Bach "Air" from orchestral suite no.3](https://www.youtube.com/watch?v=NGmQixzNdu0) \- [Several harmonic analysis videos of Bach, Chopin, Tchaikowsky, Mozart](https://www.youtube.com/user/MusicTheoryTCC/videos) \- [Several harmonic analysis videos of Beethoven, Bach, Schubert, and other romantic composers](https://www.youtube.com/watch?v=OyvSrpQ2eWY amp;list=PL613D1A6B3C4BBDF2) \- [Several harmonic analysis videos of Beethoven, Mozart, Bach](https://www.youtube.com/user/Drewsical/videos) \- [Analysis videos of form, a little harmony/key relationships, and motive/theme development in the Beethoven piano sonatas](https://www.youtube.com/watch?v=AC5iQAbMrGg amp;list=PLmwnQkTVb4ocLtDLBgvx4IH9byoIkxMDk) \- [Here is an early version of my own composition for piano annotated with harmonic intentions](https://musescore.com/user/2501446/scores/5702805/s/ridPc6) I have several textbooks, such as Walter Piston's 'Harmony', but they do not contain analysis of full movements. (Roman numerals do not give a complete picture of a composition but I find them a useful reference to understand phrase structures, modulations, sequences, theme development and other aspects of form in the common practice). Thanks in advance!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Full movement Roman numeral analysis of classical/baroque compositions</TITLE>
    <DATE>2019-09-07 13:15:58</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Full movement Roman numeral analysis of classical/baroque compositions</TITLE>
    <DATE>2019-09-07 15:16:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 10:31:34</DATE>
    <TEXT>For sure it's better to do yourself but it is time consuming if you want a quick reference from the masters when you're stuck in your own composition. For example I decided to write [this movement](https://musescore.com/user/2501446/scores/5702805/s/ridPc6) in a binary dance form and found these [annotated suits](https://www.teoria.com/en/tutorials/forms/suite/) helpful to compare several of Bach's modulation routes towards the dominant for a return to the home key. Thanks for the channel recommendation. I liked his analysis approach video (though it's made me hungrier for professional analysis of entire movements) and i'll watch the others.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 10:32:09</DATE>
    <TEXT>That's great, thank you! Any other resources i find, i'll share here.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 10:36:06</DATE>
    <TEXT>Do you have an example of a DAW that can do this? I know Logic shows pop style chords but i don't know of one that could annotate a whole work in terms of classical function or key relationship.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 10:53:05</DATE>
    <TEXT>I use RN analysed works as a quick reference from the masters when stuck in my own composition. For example I decided to write [this movement](https://musescore.com/user/2501446/scores/5702805/s/ridPc6) (my RN intentions are still in the score) in a binary dance form and found these [annotated suits](https://www.teoria.com/en/tutorials/forms/suite/) helpful to compare several of Bach's modulation routes towards the dominant for a return to the home key. I also add RN when composing as a visual guide to where i've been and where i'm going in terms of modulation routes, phrase structures and cadences, or to pick out exposition material for development (this may not be good composition practice but i haven't heard otherwise yet). I understand it's better to do analysis yourself but it is time consuming and there must be professional material out there already. Can you please recommend any resources that you consider to be a true analysis of a whole movement?</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 10:54:40</DATE>
    <TEXT> gt;I use RN analysed works as a quick reference from the masters when stuck in my own composition. I understand it's better to do analysis yourself but it is time consuming and there must be professional material out there already.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-08 20:43:04</DATE>
    <TEXT>I use RN analysed works as a quick reference from the masters when stuck in my own composition. I understand it's better to do analysis yourself but it is time consuming and there must be professional material out there already. Yes it may be possible for an algorithm to do the basics of it if someone can make a tool.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Q amp;A with Henri and Ben regarding Streamr and TX - Transcript from Telegram 09/09/19</TITLE>
    <DATE>2019-09-09 16:17:31</DATE>
    <TEXT>Streamr CEO/CTO, Henri Pihkala, and Head of Partnerships, Ben Sheppard gave an impromptu Q amp;A for the Telegram community today regarding questions on TX from Ben's [video](https://twitter.com/tx_shep/status/1170946938163912704). This is a highlights transcript of the conversation: \--- **Henri**: We were planning to talk about Streamr  amp; TX in a much more planned and coordinated way a bit later, but the video Ben posted on Twitter today stirred up some questions in the community, and we want to take this opportunity to clarify things. Ben actually made that video to answer questions about his current job description on LinkedIn, and it was admittedly confusing when viewed out of that context. So both me and Ben are here now to answer any questions. We'll also continue with our original plan to talk about this properly in the near future, in form of a proper announcement, blog post, live AMAs, and such. Give us a few moments to find your questions and start writing answers! So what is TX? TX is an arms-length company to spearhead efforts to deploy Streamr technology in the enterprise sector. Streamr technology is now approaching a stage in which more and more organizations are showing interest in what we are building. Often they need help in applying the technology - they want a solution for a particular problem - and we (the Streamr project) can't really offer them that, as our focus is on developing the technology. So we need a separate vehicle, an entity that can help these organizations to start using Streamr technology, and the vehicle for that is TX. Adoption of the tech is the shared goal of both the project and TX. Ben, do you wanna expand on that? **Ben**: Thanks @hpihkala. I would add that TX is also helping to educate enterprises on the importance of data economies and how it can benefit their organizations. Data economies, in many verticals is still quite a new topic and will take some time for executives to get their heads around. TX aims to accelerate process by providing consulting advice on the subject and then offer pilots using the Streamr stack. We're already having a great deal of success with this approach. \--- ***Nico****: This is logically what happens when you contract with external consultants, instead of internal employees ( which seems reserved to dev, maybe the team can clarify on this )* *The question is: who owns the work being done by those consultants?* **Henri:** While TX is an arms-length company and has its own leadership, team, and finances, it is a subsidiary of the company responsible for developing the project (Streamr Network AG). This means two things: 1) TX has a very strong interest in delivering solutions to its customers which support the token economy and create demand for it. Even when customers want private deployments of the technology, TX will seek to involve the token in these deals. 2) Ben hasn't run off and set up something of his own, but instead he is deeper than ever in Streamr stuff and creating more-formal-than-before structures to support the ways in which enterprises want to work, and to boost the adoption of the tech and token economy. **Ben**: Perhaps also worth highlighting, I was previously a sub-contractor to the Streamr project. I had been running my own consulting firm around SE Asia. Last Friday I officially joined the organisation as an employee. Meaning I am firmly committed to ensuring the longevity of the project and adoption of it's tech stack. \--- ***Pocket\_Change****:* *Who #8217;s funding TX and how much equity will I own ?* **Henri:** TX is funded by income it manages to generate, not from the project funds. Actually, this is an absolutely crucial point with very important consequences to the project as well: The goal is to enable the technology and ecosystem to survive in the long term. However, the project funding \*will\* run out at some point (still several years away though). By successfully building a sustainable business to fuel the maintenance and further development of the project, effectively an \*infinite lifespan\* can be achieved. \--- ***Sason****: What's this entity that owns streamr and Tx, does it won the ICO funds? Also, how much is left from that fund?* **Henri**: Streamr Network AG is the top-level legal entity. This entity is also responsible for delivering the project, and it is the custodian of the ICO funds. From the CHF 30 million raised, a bit less than two thirds remains. That's pretty well in line with estimated project duration (5 years), where we are now (2 years down the road). Also in terms of tech development, I think more than one third of the work is done. Many projects screwed up their funding in the market crash. We played it safe, and looking back now, that was definitely the right move. One of the goals of TX is actually to decrease project spend. Instead of doing a lot of partnerships work (i.e. the project paying for talking to companies) we can in some instances turn it around (companies pay us for talking to them). This is very good for the project. **Shiv**: I can add that internally we have made a really comprehensive start on token governance in the Network. We have a dedicated PhD researcher on this and co-founder Nikke, is leading this effort himself. It's exciting, meaty stuff and as you may or may not know, I"ve personally been involved with the players in the Ethereum scene concerned with governance (here's my talk from ETHBerlin two weeks ago [https://www.youtube.com/watch?v=mN2uhS5pYp0 amp;feature=youtu.be amp;t=27391](https://www.youtube.com/watch?v=mN2uhS5pYp0 amp;feature=youtu.be amp;t=27391) ). This means we've been able to draw upon their learning and mistakes for when we come out with our Network governance /token incentivisation model. To be successful, project/protocol governance and token incentivisation have got to go hand-in-hand, as we've seen from other projects. \--- ***Nico****: Are those subsidiaries also non-profit companies ?* **Henri**: Technically, all the entities in the setup are just ordinary companies - that's actually the most future-proof and practical way to wrap a crypto project. But Streamr Network AG and TX apply very different modes of operation: Streamr Network AG develops software as commissioned by the project backers, funded via ICO for a pre-set roadmap. So the project entity has no sources of income, and is not really "selling" anything. TX is set up differently. It is geared for offering services (whereas Streamr doesn't sell anything), and sustaining its operations via income. It's like a financially sustainable version of a partnerships team - one that can keep going forever instead of draining the project budget, freeing up precious resources for other areas like R amp;D and marketing, which are at the heart of the project. In fact, \*endless\* resources could easily be spent on fooling around with these slow-moving enterprises. That's a space with a lot of talk but not much action. There's been plenty of leads, but it is not trivial how to select and pursue the ones that actually lead to adoption, solved problems, and innovation of new products and services. Having a financial restriction (=sustainability) in place seems to rank the leads in a useful way, and guides the tam towards the more serious collaborations with practical outcomes. \--- ***Sason****: Thanks! again in all this we still never address the tokeneconomics...* **Henri**: It is indeed a tricky one. Someone asked about equity in relation to TX. Obviously, attaching ownership properties to the DATA token (just to provide an example, not saying we'd do it exactly like that even if it were possible) would immediately make it a security and get it kicked out of all the exchanges. We're following the security token scene carefully, but the space is still too immature. We're very interested in what can be done a bit further down the road in terms of tokenizing the equity of the related legal entities. We're also working on governance structures to migrate control over the project to a DAO (or similar) setup in the longer term (say, towards the end of the project). But this and the equity tokenization are longer term things with a lot of regulative friction at the moment. As for token economics in the technical product itself (as opposed to the above discussion on legal claims contained by the token), we're making steady progress through the roadmap, and will get there as planned. \--- ***Nico****: Thank you henri* *One more if I may:* *Streamr has a wp and we could invest based on it on ico or afterwards* *Those new companies have absolutely and legally nothing related to Data token, specifically* *1st concern: what garantees about those entities regarding Data, as they seem the ones which should sustain the whole ship in the future ?* *2nd: we all know what happens when one subsidiary has to sustain the mother company. One day or another ( precisely when it runs out of money, and as you say cannot bring money anyway ) the temptation is big to  #171; #160;abandon #160; #187; the main ship and focus on profitable set-ups.* **Henri**: Hey, sorry but your questions were a bit unclear. I'll try to answer, but please clarify if my answers are not precise. As the top entity, the mothership, holds the share of tokens allocated to the company in the beginning, any and all subsidiaries are equally incentivized to contribute to the success of the token economy. As for the second question, I'm at all sure if "we all know what happens when one subsidiary has to sustain the mother company". No one has really done it yet, so it's hard to say what will happen. The TX approach does sound a lot better compared to e.g. Ethereum and ConsenSys or Ethereum and Parity/Web3, in which the spin-off companies were completely detached from both the core devs and the associated token (ETH) economy, and in the latter case even leading to the creation of a competitor. Oh yeah actually, speaking of competitors, one interesting thing I wanted to mention about TX is this: Note that as Streamr is free and open source software, anyone (each one of you!) could start a software business that builds solutions on top of Streamr technology. In fact we hope that many such businesses are started, as that would be the best possible outcome for adoption of the technology, and also playing into the token economy. So interestingly, we want a lot of competition for TX. If TX is successful in getting customers for Streamr technology, then you can be too! So in a sense, TX is a propotype, an exploratory tool, a model, an example. We hope you copy it. Please sell Streamr-based solutions to your customers, we don't mind, we love it, because our main goals are on a much higher level than generating income: our goal is adoption, changing how data is shared and sold, and supporting the token economy. \--- **Nico**: *Isn #8217;t it frustrating that all those big potential partnerships didn #8217;t really achieve anything ?* *Is that because they do things by themselves finally ( in your opinion ) ?* **Henri**: When the blockchain and crypto hype was at its peak, everyone and their mother wanted to partner with you to show their audiences how pioneering they were. Talk is cheap, and announcing partnerships is a short term victory for everyone, but turns out it doesn't carry much long term benefit unless the other party is serious about it. Now we're seeing (luckily!) that people actually want to do real things and solve real problems instead of just making noise about intentions. The downside is - well - that they \*don't\* want to make much noise about it, so there's not that much that can be publicly flashed before things are much further along. \--- **Henri**: I have to go now, but there definitely will be much more on how we see TX positioning itself into the long term vision. This outburst of information wasn't supposed to happen just yet, but rather a bit later in a slightly more controlled manner, but as always, please ask whenever something's on your mind! I guess the key takeaways from this are: \- TX is financially independent, and will help \*save\* project funds instead of spending them. \- TX is providing services that the project can't provide, thereby filling a void in supply of services to enable adoption of the technology and the token. \- TX has managed to land a handful of very interesting projects, which shows that there is traction and demand for what we are building in various industries. As the technology improves, this will only get more and more feasible going forward. Streamr and/or TX will communicate more about these use cases when possible. \- Even though TX has a close relationship to Streamr due to its origin, it's nothing special at all, and you too can create businesses around Streamr technology. In fact if you do, we'll support you. Really have to run now, catch you all later! Thanks everyone for being here, you keep us going every day.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core launch and webinar introduction recording, Berlin Blockchain Week reactions, Community Products and Network development progress - check out the latest dev update!</TITLE>
    <DATE>2019-09-09 16:23:44</DATE>
    <TEXT># Dev Update August 2019 [Santeri Juslenius drilling into Core #8217;s potential at EthBerlin Zwei in August](https://preview.redd.it/m1m8yebfgll31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=50e7c4a4bbecf3e35c414e9974f611fb70559831) Welcome to Streamr #8217;s core dev team update for August 2019. If you #8217;ve been keeping on top of the Streamr project #8217;s development, we officially launched the [Core app](https://www.streamr.com/core/) a few weeks ago. We [showed-off Core](https://medium.com/streamrblog/streamr-core-app-smart-contract-integration-real-time-data-visualizations-9180e343c7a) to scores of Web3 developers over Berlin Blockchain Week. Devs were impressed with the easy drag-and-drop UX, revamped design and the nearly 170 built-in modules, which conveniently allow users to [interact with Ethereum smart contracts](https://medium.com/streamrblog/dao-data-economy-ethereum-oracles-39e1e3ab160c) without any frontend coding. Our developers Tim Oxley and Jonathan Wolff hosted an online webinar walking through the basic aspects of Core: how to use most popular modules, connect real-time data streams, create event triggers, deploy smart contracts on Ethereum blockchain and how to trigger them automatically. If you missed it, feel free to watch the recording [here.](https://youtu.be/KZ35QPJmY-4) [Streamr #8217;s Juha Haavisto at EthBerlin blockchain week presenting Core](https://preview.redd.it/x2kmapcmgll31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=63fc87d3ce88dcc8ce0baf6e9740cad9f08219a5) In August, we also entered into a more advanced development stage for the highly anticipated [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9). We have successfully deployed it in Streamr #8217;s dev environment, which means it is ready for real testing and integration. Our blockchain architect Juuso, who is leading the effort on this front, also reached out to Ebrahim in order to assist him with the first Community Product integration ever. If you haven #8217;t followed previous announcements regarding the browser plugin that Ebrahim and his team are building, I highly recommend you check out this [project proposal post](https://forum.streamr.dev/d/39-firefox-extension). It is an understatement to say that the combination of ethically crowdsourced data, cross-border revenue sharing via [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) and an open data [Marketplace](https://marketplace.streamr.com/) initiative will trigger a paradigm shift in the current state-of-internet. What an exciting few months ahead! Of course, we keep pushing forward on the [Streamr Network](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca) side too. During August we are continuing to work on the key exchange mechanism to deliver a true end-to-end encryption for the Streamr Network. Most code has been submitted and is under internal review. The dedicated Network developers also tried to make the Network testnet run stable under the current production data load. This meant creating a bridge to transfer data over in parallel. Streamr devs are also working on the logic needed to tackle duplicate data detection and message re-sending if there are gaps at the receiving end. A final note. We don #8217;t do boasting at Streamr. It #8217;s a Finnish thing  #8212; you stay humble. But what rapidly became clear during EthBerlin was how amazed other teams were with how much we #8217;d managed to ship and produce, and how far our ecosystem had already expanded, given that our basic team is only 30 or so members. It #8217;s a lovely feeling to know you #8217;re on the right path. *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community-run* [*dev forum here*](http://forum.streamr.dev/)*.* As always, thanks for reading. Here #8217;s our regular list of updates: # Network * Testnet seems to be running stable under current production load. Nodes in Germany and Finland * Encryption key exchange mechanisms are in PR. Big changes, it will take time to review the code base * Preparing for a geographically distributed set of nodes to be added on AWS * Starting to test existing components (Core, SDKs) against new network. * Preparing AWS infrastructure for agile geographically distributed performance testing # Community Products * Working on Community Products creation flow for product administrators * Community Products server state persistence/playback on restarts is almost finalised * Working on server deployment flow and detailed documentation for users * Completed the setup for Community Products in dev server environment * Juuso working on fixing tests (based on manual testing, the CP server itself is working). * Deployment work continues on staging  amp; production environment * Marketplace frontend work to integrate Community Products has started * Lots of devops work, fitting the pieces together in different environments (dev, staging, production) * Devs are now in touch with Ebrahim on Telegram for the browser plugin # Core app (Engine, Editor) * Finalized Core app for official launch * Developer docs sprint completed and deployed to production * Working on displaying module help content in docs and canvas editor * Working on canvas embeds and sharing, the last major missing feature * Improving example canvases and streams for the initial onboarding tours * Working on ability to view public canvases without forcing user login * Many other small fixes and improvements on an ongoing basis # Labs * Setting up a Cisco router to run a network node * Researching machine learning on canvases. *See the original post on* [*Medium*](https://medium.com/streamrblog/dev-update-august-2019-core-app-data-visualization-data-unions-472d5d5fec51) *from Wei.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-11 08:55:16</DATE>
    <TEXT>In terms of utility, the token is currently used for purchases on the Marketplace but its functionality will expand as the tech develops. For example, nodes on the Network will offer their idle bandwidth to earn tokens and data providers will receive tokens every time their data is crowdsold on the Marketplace in Community Products. There is some more info in the FAQ: [https://www.streamr.com/faq#datacoin](https://www.streamr.com/faq#datacoin) Please note that this community is primarily about discussions related to the tech and not token price or purchase speculation.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-11 09:00:19</DATE>
    <TEXT>Streamr is still currently uses Ethereum, but the project is blockchain agnostic and can move to an alternative should the need arise. This subject is also discussed in the FAQ tech section: [https://www.streamr.com/faq#tech](https://www.streamr.com/faq#tech)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>What does a Devops Lead do? Please meet Mikhael Santos and find out.</TITLE>
    <DATE>2019-09-11 10:29:39</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-11 10:51:40</DATE>
    <TEXT>Please post your questions about this plugin and Community Products ahead of tomorrow #8217;s AMA (15:30 CEST) with plugin creator Ebrahim Khalilzadeh and Streamr's Head of Comms Shiv Malik: [https://www.reddit.com/r/streamr/comments/d0ha2f/join\_community\_dev\_ebrahim\_khalilzadeh\_who\_is/?utm\_source=share amp;utm\_medium=web2x](https://www.reddit.com/r/streamr/comments/d0ha2f/join_community_dev_ebrahim_khalilzadeh_who_is/?utm_source=share amp;utm_medium=web2x)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-11 10:54:29</DATE>
    <TEXT>Please post your questions about this plugin and Community Products ahead of tomorrow #8217;s AMA (15:30 CEST) with plugin creator Ebrahim Khalilzadeh and Streamr's Head of Comms Shiv Malik: [https://www.reddit.com/r/streamr/comments/d0ha2f/join\_community\_dev\_ebrahim\_khalilzadeh\_who\_is/?utm\_source=share amp;utm\_medium=web2x](https://www.reddit.com/r/streamr/comments/d0ha2f/join_community_dev_ebrahim_khalilzadeh_who_is/?utm_source=share amp;utm_medium=web2x)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #128225;  #128202;  #9939; Streamr Core is a free-to-use toolkit to create, integrate, process and visualise real-time data streams for fast prototyping and insights. Learn how to get started creating streams and using smart contracts in this webinar intro!</TITLE>
    <DATE>2019-09-11 14:45:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-12 11:26:17</DATE>
    <TEXT>Good to hear your interest in the project. I've asked Henri if he can get back to you on this when he has a moment</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Cars are predicted to become the second most important IoT device after the mobile phone. What if your car could sell the data it produces and purchase the data it needs, p2p, in real-time?</TITLE>
    <DATE>2019-09-12 15:49:12</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-16 07:26:49</DATE>
    <TEXT>This was spoken about in Telegram a few months ago. Ben, Head of Partnerships, had this to say on it at he time:  #8220; [...] The update from HPE and Continental is a really fantastic development within the mobility industry. Seeing two major enterprises taking significant steps around data marketplaces can only be good news for projects such as ours. Let me also add that it is not unusual for organisations of HPE size to have a relationships with multiple partners working with similar technologies. We continue to have a very strong relationship with HPE which has developed since we first announced we were partnering with them in May last year. That work to explore multiple use cases involving the Streamr technology stack continues. #8221; [...]  #8220;it is exciting to now see HPE and Continental use Crossbar as a demonstrator. I believe in the future we will see multiple marketplaces serving the automotive industry. #8221;</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Canvas zooming is a feature coming soon to the Core app. Try it today to generate fast prototypes and insights from data streams</TITLE>
    <DATE>2019-09-17 15:15:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-17 15:15:30</DATE>
    <TEXT>Try the app! [https://www.streamr.com/core](https://www.streamr.com/core)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is mentioned in this report by GSMA on page 22 of the section about data hubs advancing public and private interests</TITLE>
    <DATE>2019-09-18 13:24:59</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has a new home! The new website contains updated use cases, dev documents, development timelines and live statistics on the token and Network. Check it out!</TITLE>
    <DATE>2019-09-19 09:58:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>New site is up. Join us as the Streamr Network starts to grow. You can learn more and see its current data points per second live on the Network page</TITLE>
    <DATE>2019-09-20 12:33:55</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-20 12:34:07</DATE>
    <TEXT>Take a look: [streamr.network/learn/network](https://streamr.network/learn/network)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>This month's community hero is  #129345; ... Bonjovo! Your support in the Streamr Telegram is much appreciated. Join the conversation there and follow the latest developments!</TITLE>
    <DATE>2019-09-23 12:38:35</DATE>
    <TEXT> #8203; https://preview.redd.it/ej374dpj9co31.png?width=1012 amp;format=png amp;auto=webp amp;s=e3b362c0c73a7e540e5359550b58c2d380fe39ea</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>It #8217;s happening! The decentralized version of Streamr #8217;s p2p Network for real-time messaging is launching at Devcon 5! To celebrate, we #8217;re throwing a pier-to-pier boat party. Places are limited so sign up quick! All aboard in Osaka!  #9973; #65039;</TITLE>
    <DATE>2019-09-23 14:46:35</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr has 5 tickets for you to join the first demo of Community Products featuring the Swash plugin at Mozfest in London this October 26/27th. Please reply to this message to claim yours</TITLE>
    <DATE>2019-09-24 08:41:45</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-24 08:42:30</DATE>
    <TEXT>Working link: https://www.eventbrite.com/e/streamrs-pier-to-pier-party-tickets-73397177935</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-24 08:43:07</DATE>
    <TEXT>Try this one, sorry. https://www.eventbrite.com/e/streamrs-pier-to-pier-party-tickets-73397177935</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-24 08:43:24</DATE>
    <TEXT>This should work now https://www.eventbrite.com/e/streamrs-pier-to-pier-party-tickets-73397177935</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Community dev Gang Liu has combined Fitbit data on the Streamr Marketplace with Machine Learning to predict a user's real-time movement activity with 91% accuracy</TITLE>
    <DATE>2019-09-24 13:13:55</DATE>
    <TEXT># Integrate the Fitbit stream with Machine Learning to predict user activity # What #8217;s Streamr Marketplace? [Streamr](https://streamr.network/) Marketplace is a global data exchange platform offering easy integration APIs. In the Marketplace, any application can easily setup a stream to exchange/monetize data such as weather condition, traffic status and personal fitness for example. Their explainer video is [here](https://www.youtube.com/watch?v=nbWIPVE8zFE) # How to use the data on Streamr Marketplace Our previous blog post introduced [how to publish personal fitness](https://medium.com/streamrblog/personal-fitbit-data-sell-streamr-marketplace-blockchain-ethereum-3b32c215660c) data (including daily steps, heart rate, consumed calories and etc) on the Streamr Marketplace. While building an app to aggregate data was useful, the real question is what the data can be used for. This blog showcases how one could leverage Machine Learning models (ML) to predict what kind of activities data producers (users) were doing at various time of the day. Specifically, we first subscribed to the published fitness data from Marketplace. Then we used our trained ML model to predict the activity users were taking part of. Using the model we can predict in real-time whether the a user was walking, working or playing a sport (in this case we trained the model to detect basketball patterns).  #8203; https://preview.redd.it/ad1yro9ikjo31.png?width=555 amp;format=png amp;auto=webp amp;s=7fc092f6a6fb5b448824ec490a66c72fe4527a22 # How to preprocess the recorded data? Preprocessing data is a key step in ML modeling. Correct preprocessing can speed up the convergence of machine learning models and improve the final model accuracy. The preprocessing of data includes a lot of steps, including data filtering, data outlier processing and data normalization. The data measured by the device mainly includes: time, number of steps and heart rate. Because the time format of the test is: hour + minute + second, such the data is not conducive to modeling operations, so the time is planned uniformly. If the number of minutes is greater than 30, add one to the number of hours; if the number of minutes is less than 30, the number of minutes is ignored, and the number of hours is not added. The above operation is similar to the rounding operation of minutes. Because there are no outliers or missing values when testing the data. If an abnormal value occurs, the removal operation is generally performed. If a missing value occurs, it is usually done with 0 or inserting the mean. Normalizing the data is the second step. In the objective function of the machine learning algorithm (such as the RBF kernel of the SVM, support-vector machine, or the regularization of the l1 and l2 of the linear model), the basis of the objective function in many learning algorithms is to assume that all features are zero mean and have variances on the same order. If the variance of a feature is orders of magnitude larger than the variance of other features, it will dominate the learning algorithm, increasing the system weight of certain features, and skew the final results. After the data is normalized, the optimization process of the optimal solution will become smooth, and it is easier to correctly converge to the optimal solution. Generally speaking, StandardScaler and MinMaxScaler are the start-of-the-art methods for normalizing the data. The StandardScaler first passes the data by subtracting the mean, then divides the result by the variance (or standard deviation). This data normalization method is processed to conform to the standard normal distribution, that is, the mean is 0 and the standard deviation is 1. MinMaxScaler scales features to specific intervals, scales features to a given minimum and maximum value, or converts the maximum absolute value of each feature to a unit size. Such a method is a linear transformation of the original data, normalizing the data to \[0,1\]. Since the data used in this article is positive and does not follow a normal distribution, the method of normalization of MinMaxScaler data is finally adopted. Among many machine learning algorithms, SVM algorithm has obvious superior performance, and a solid theoretical foundation. It has a good application in the fields of handwritten digit recognition and face recognition. The biggest difference between SVM and other learning methods is that the SVM algorithm is based on the minimization of structural risk, rather than minimizing the empirical risk in traditional algorithms. Besides, the SVM can make a compromise between model complexity and system generalization ability. Moreover, in the SVM-based training and prediction process of classification model, the classification effect only depends on a few support vectors, so it has certain processing power for the classification of some high-dimensional samples. Therefore, this experiment used the SVM algorithm which is good at solving small sample classification problems. The essence of the SVM algorithm is to find the appropriate classification hyperplane in the solution space, and obtain the largest sample classification interval through the best classification hyperplane. The following figure below is the schematic diagram of the SVM algorithm. The SVM algorithm used in this blog is called by calling the scikit-learn algorithm package. In Python, scikit-learn is a widely used library for implementing machine learning algorithms. Here is the partial code: **clf = OneVsRestClassifier(SVC(C=15,gamma=0.28,probability=True))** In the modeling process, two-tenths of the data are randomly selected to form a test set, which is used to test the generalization ability of the model. The remaining nine tenth is the training set. The model is completely separated from the test set, making the results of the verification system more objective, accurate and persuasive. The modeling process steps for the classification system are as follows: 1. Data partitioning: the data set is randomly divided into two parts, 80% of the data set is used as a training dataset, and 20% of the data set is used as a test dataset. 2. Data processing: standardize the data to minimize the difference in eigenvalues between individuals. 3. Model establishment: the machine learning algorithm is used to systematically model the training dataset, and then the test dataset data separated above is used to verify the accuracy of the classification model. 4. Model analysis: adjust the key parameters in the classification algorithm and repeat step 3 until the system reaches a high precision and stable state. 5. Results: repeat the above operation, select different test set sample modeling each time, obtain the accuracy of each test result, and finally calculate the average accuracy of the classification system. # Final Results Walking activity is represented by 0, sleeping is represented by 1, working is represented by 2, and playing basketball is represented by 3. The following is the final accuracy of all the data modeling obtained: 92.65% 93.55% 89.43% 92.11% 91.04% 91.40% 91.04% 92.11% 90.14% 89.07% 91.04% 91.76% 91.22% 93.37% 89.94% 91.94% 93.73% 92.29% 93.19% 90.68% By calculating the average accuracy of these 20 replicate experiments, the final classification accuracy is: 91.59%. In summary, the classification accuracy obtained in this experiment was high, indicating that these features are highly correlated with human activities and deserve further analysis. However, due to the limited amount of dataset and fitness data sellers, future operators should increase the type of sampling population, increase the total amount of data, and find other information besides heart rate (such as PPG signals). # Source codes All the source codes can be found via the [Github](https://github.com/KB00100100/Streamr_Fitbit_ML/tree/master). *See the original post from Gang Liu on* [*Medium*](https://medium.com/streamrblog/integrate-the-fitbit-stream-with-ml-to-pre)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr is building the open source infrastructure that will power the world #8217;s data economy and give you a way to monetise and control your information. Get ready for the launch of Community Products in Q1-2 2020! #PersonalDataDay</TITLE>
    <DATE>2019-09-25 11:40:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-09-26 13:08:36</DATE>
    <TEXT>\*correction, this launch of the Network will include encrypted messaging and p2p architectures but it will still have some centralised components. FULL decentralisation will be achieved in later milestones. Apologies for any confusion. Here are the goals of this release: [https://medium.com/p/5f6ed2162bfe](https://medium.com/p/5f6ed2162bfe)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea, the next generation version of the Streamr Network is launching at Devcon 5. Henri Pihkala writes on the goals of this milestone and challenges of testing it</TITLE>
    <DATE>2019-09-26 13:44:52</DATE>
    <TEXT># How do you test a testnet? Building peer-to-peer networks is hard. Figuring out how to test them can be even harder.  #8203; [Cumulative density functions of latency in networks of varying sizes. Nodes can have a limited number of connections \(here, only four\), resulting in messages travelling more hops in larger networks.](https://preview.redd.it/ap29q5wjzxo31.png?width=2000 amp;format=png amp;auto=webp amp;s=a7b243243e55a9ad1c54605edd0292bb85a8c317) As the launch of the Corea milestone  #8212; the biggest Streamr Network upgrade so far  #8212; is drawing nearer, I thought it might be good to give an overview of the goals for this release, as well as how we #8217;ve gone about building and testing the new network. # Design Principles For the Streamr Network, we #8217;ve always had certain goals in mind. When we [started working on Corea](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca) in August 2018, we applied this set of goals to help us make the right decisions about what would (and what would not) be included in the milestone. * **Scalability:** The Network should scale without limit, meaning that the Network will continue providing good quality of service no matter how many nodes are added to it. Also, the load of the data publisher should stay constant regardless of the number of subscribers. * **Decentralization:** The Network should not have hotspots or require central points of failure to operate. * **Low and predictable latency:** The event propagation delay should stay low in all situations. Also, the relative path penalty should be small if compared to having direct connections to the publishers. * **Optimisation for small payloads (telemetry, IoT):** The network should be optimised for transmitting large numbers of small messages, in contrast to transmitting small numbers of large messages. * **Bandwidth efficiency:** The number of unnecessary messages transmitted in the Network should be low. A node should only receive a small number of duplicates of each message. * **Message completeness:** A node should receive all the messages of a stream it is subscribed to with a high probability, provide means to detect missed messages, and a mechanism to have them resent. * **Robustness:** The Network should be reliable in real-life use. The implementation should be kept simple, and easy to debug. Diving into existing literature and having these goals in mind quickly led us onto a certain track and certain decisions. For example, we realised the beneficial properties of random-topology networks, and selected that as a starting point for per-stream topologies in the Corea network. In contrast, in the [2017 white paper](https://streamr.network/whitepaper), we envisioned the Streamr Network to be more like a  #8220;decentralized [Kafka](https://kafka.apache.org/) #8221;, with a tree-like topology that had a predetermined replication factor and a predetermined number of partitions spanning all streams in the network. However, the random-topology-per-stream approach provides much better robustness and churn tolerance while providing decent performance and leaving the door open for various dynamic performance optimisations in later milestones. Should the rigid, tree-like topology originally envisioned turn out to be the best one, it should be discovered and converged on dynamically by the Network, not hard-coded in advance. [Tracker topology tester demo](https://www.youtube.com/watch?v=TtPx4_3VGO8) \- A tool we built to inspect and validate the random topologies created by the tracker as the number of nodes increases in a peer set for a stream. In addition to the normal data-brokering behaviour in the network nodes, we introduced two other roles into the Network: storage nodes and trackers. Storage nodes are responsible for long-term data archival and subscribe to large subsets of data passing through the Network. Trackers govern the formation of the topology for a particular stream by  #8220;introducing #8221; nodes that are interested in a given stream to each other. The network can have any number of normal nodes, storage nodes, and trackers to avoid single points of failure.  gt;All the planned Streamr Network milestones are intended to be drop-in replacements for the previous generation Obviously, Corea is not the end of the road. This milestone mostly focuses on implementing the highly distributed message brokering functionality. Most notably, the token economics are not yet included in this milestone, and will only be introduced during the next two milestones: Brubeck and Tatum. Corea also still depends on the centralized backend to serve a registry of stream permissions to nodes and client apps, whereas this information will later be on-chain and thus made available in a decentralized way. # How do you build and test a P2P network? By late 2018, we had already built a working prototype with one tracker, no storage nodes, and a small number of normal nodes, successfully brokering messages in one stream. That was a great start, but we had some important questions to answer. How would the Network function if there were thousands of nodes? And what if there were millions of streams, or millions of subscribers to them? What if the nodes went down randomly, or the Network links between the nodes broke? What would happen to related stream topologies if a tracker went down? What if the nodes became overwhelmed and couldn #8217;t handle the volume of data? To answer these questions, much of the work in making Corea production-ready has gone into validation: stress testing, scalability testing, performance testing, and ensuring functional backwards-compatibility with the previous-generation Network, Monk.  gt;By setting up long-running real-world testnets, we were able to uncover problems that build up over time All the planned Streamr Network milestones are intended to be drop-in replacements for the previous generation  #8212; meaning that the Core app, Marketplace, and all other applications using Streamr via the APIs should not need any changes to be compatible with the upgrade, even though, under the hood, the whole of the Network #8217;s architecture is changing. # Emulated Networks One of the most important tools in testing Corea at scale has been network emulation. Emulation allows us to run a number of nodes on the same physical machine (where the main constraints are the available memory and CPU), with nodes connected over an emulated network instead of a real physical over-the-internet connection. This environment allows us to precisely set bandwidths, latencies, and [jitter](https://datapath.io/resources/blog/what-is-network-jitter/), as well as introduce malfunctions to the connections. The [CORE](https://www.nrl.navy.mil/itd/ncs/products/core) network emulator became our tool of choice. It #8217;s open-source and developed by the U.S. Naval Research Laboratory. While it #8217;s not the newest or sexiest software you #8217;ve ever seen, it #8217;s very battle-tested and works on all platforms.  #8203; [An emulated network modelled after real-world internet latencies to mimic real geographically distributed networks.](https://preview.redd.it/rbxy9bowzxo31.png?width=1600 amp;format=png amp;auto=webp amp;s=b0047b3a604f19aa31e46ef7a3a15294fd0ac60a) Emulating networks allowed us to measure latencies and duplicate messages in larger-scale networks up to thousands of nodes, and helped validate that the Corea code was behaving correctly; the topologies were stable, data was reaching subscribers, and so on. However, as emulated networks were spun up, measured, and then terminated, this mode of testing didn #8217;t allow us to see how things behave over longer periods of time. Another form of testing was required to complement the emulated networks. # Real-world Testnets Firstly, by setting up long-running real-world testnets, we were able to uncover problems that build up over time, such as memory leaks. Secondly, in the emulated setup, some of the CPU cycles go into running the network emulation (as opposed to running the node), so stress testing the nodes and measuring actual CPU usage make sense only in a real environment. And finally, real testnets are subject to the unpredictability of actual internet connections, revealing how the network operates in practice under real-world conditions. Compared to emulated networks, it #8217;s not that easy to run testnets at scale. Sure, tens or even hundreds of nodes can be started quite easily using cloud services, but scaling to thousands of nodes is starting to hit the limits of what those platforms can do. Beyond a certain scale, this approach also becomes prohibitively expensive. So, with the above pros and cons in mind, our testnets were focused on validating behaviour over time under real-world conditions. Because the goal of the Corea release is to replace the previous-generation network, Monk, our longest-running experiment is an eight-node testnet to which we replicate all data published to Monk. That #8217;s currently around 6000 messages per second, as you can see in real-time on the [Network page](https://test.streamr.network/learn/network/). Although this doesn #8217;t fully represent everything that #8217;s happening in the old network (for example, it doesn #8217;t include real-world subscribers such as Core canvases and API users), it still gives us reasonable assurance that everything will work fine once we swap Monk for Corea.  #8203; [A large, generated topology with many interconnected nodes within  #8220;cities #8221; plus fewer links between the clusters  #8212; a so-called small-world network.](https://preview.redd.it/0g8x9l100yo31.png?width=1600 amp;format=png amp;auto=webp amp;s=84dc584b6920bd732cb45962c12a2a1c61b9a2a7) Interestingly, we leveraged the old Network and Streamr Core in measuring the testnets in real-time. The stack is actually quite suitable for this type of use case! Corea nodes were configured to regularly publish metrics to a stream, which we then monitored in real-time using canvases, one of which [you can view here](https://streamr.network/canvas/editor/MInag4IgTpKx2NdcUyC85AXSa5Eh8pSTu_9o5SJZkrKw). While Corea seems to be well equipped to handle the current production load, it #8217;s still early days in terms of serious adoption. We need to prepare for several orders of magnitude and more traffic in the future. At the time of writing this, we #8217;re setting up scripts to spin up a globally distributed network with nodes running in all available AWS zones, which we then plan to test under considerable load equivalent to an enterprise-level use case, with hundreds of thousands of devices publishing data onto the network. # White Paper and Next Steps The network R amp;D team will be releasing a white paper on Corea later this year, detailing the reasons behind the architectural choices we #8217;ve made and the experiments we #8217;ve run, as well as the results obtained. While the paper might be of some academic interest, the primary purpose of the paper will be to provide in-depth information about the characteristics of the Corea network to serious players such as enterprises, governments, and other large organisations, seeking to utilise the Network at scale. The next steps in the deployment of Corea are to double-check that all the pieces fit well together, and then finally do the switch from Monk to Corea. If all goes well, Streamr Core, Marketplace, and the API gateways will all point to Corea mainnet just in time for [Devcon5](https://devcon.org/). In fact, we #8217;re hosting a P2P (pier-to-pier!) [boat party](https://www.eventbrite.com/e/streamrs-pier-to-pier-party-tickets-73397177935) to celebrate the release of Corea. See you there! *Read more:* [*https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b*](https://medium.com/streamrblog/data-authenticity-integrity-streamr-network-decentralization-bc17630c670b) *Have questions? Join our* [*community developer forum*](https://forum.streamr.dev/) *and ask away!* *Check out the original post on* [*Medium*](https://medium.com/p/5f6ed2162bfe)*.*</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea, the next generation version of the Streamr Network is launching at Devcon 5. Henri Pihkala writes on the goals of this milestone and challenges of testing it</TITLE>
    <DATE>2019-09-26 13:45:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #9973; #65039; Over a 3rd of tickets gone for Streamr's pier-to-pier boat party to celebrate the Network launch at Devcon 5. Join shipmates Henri Pihkala, Dmitriy Ryajov, Dr. Sebastian B #252;rgel, and Anna Rose for a panel discussion on Trustless Messaging for Web 3!</TITLE>
    <DATE>2019-09-27 11:59:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Jaron Lanier outlines the faults he sees in the current personal data economy and how individual data monetisation can stimulate an innovation boom. Jaron's work has influenced the development of Community Products, Streamr's answer to this issue</TITLE>
    <DATE>2019-09-30 14:38:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The questions, implications, and possibilities of monetising your data in the 21st century are the topics explored at Streamr's Mozfest panel this October. Join us!</TITLE>
    <DATE>2019-10-01 14:08:36</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-01 17:06:10</DATE>
    <TEXT>Thanks a lot to the team, it #8217;s a wonderful platform and tool and helped open a world of composition to me. One question/suggestion is if there will ever be the possibility for a mobile composition app/tool? I take sheet paper with me when travelling but it would be great to have some basic functionality to write while on the go (maybe as an idea sketchbook type app with basic playback - of course full functionality likely impossible. Though composing with a digital pen on an iPad version of the full app would be appealing!) Thanks in advance.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Got a question on the Network, Community Products, Core or what #8217;s ahead for Streamr? Join Head of Comms Shiv Malik for an AMA on Telegram tomorrow at 17:00 CEST</TITLE>
    <DATE>2019-10-02 13:27:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea, the next generation version of the Streamr Network is launching at Devcon 5. Henri Pihkala writes on the goals of this milestone and challenges of testing it</TITLE>
    <DATE>2019-10-03 08:15:44</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Tl;dr  #8212; new website, new Network, more ecosystem developers  #128077;</TITLE>
    <DATE>2019-10-03 16:48:42</DATE>
    <TEXT># Dev Update September 2019 [An emulated network modelled after real-world internet latencies to mimic real geographically distributed networks.](https://preview.redd.it/y19x1gltucq31.png?width=1600 amp;format=png amp;auto=webp amp;s=b28e0de7de49c1eea9d2c06c52ce60b23483a4d6) Welcome to [Streamr #8217;s](https://streamr.network/) core dev team update for September 2019. If you have been following our project long enough, you might know we are reaching our second year anniversary!  #8203; https://preview.redd.it/8okufmg0vcq31.png?width=1282 amp;format=png amp;auto=webp amp;s=5a943ad2fbd99f3b7f5482ee6ae8fa4113cc0c0e Looking back at where we started and the progress we have made so far, it has definitely been a remarkable journey. In large part, the credit for this goes to you, our community. You #8217;ve continued to trust and support us and that has allowed us to maintain our focus and consistently deliver the steps (and more) outlined in our [roadmap](https://streamr.network/learn/network). As Gavin Wood mentioned [during a fireside chat ](https://youtu.be/YJi_hbnSjko)at Web3 Summit last month, delivering products is really hard. We are happy to announce that our next Network milestone, Corea, will be hitting mainnet very soon! [Corea milestone](https://medium.com/streamrblog/test-a-tesnet-scalability-decentralization-latency-5f6ed2162bfe), the biggest Streamr Network upgrade to date, represents a pivotal point for Streamr project. Not only because it #8217;s a big step forward toward a decentralized Network. But also because it has allowed our team to refine our internal process on design criteria selection (focus  amp; trade-off), testing, monitoring and performance optimisation that will be all vital for our next Milestones down the road. A lot of effort also went into the validation process: stress testing, scalability testing, performance testing, and ensuring functional backwards-compatibility with the previous-generation Network, Monk.  gt;All the planned Streamr Network milestones are intended to be drop-in replacements for the previous generation This will help us minimise risks of incompatibility or disruptions with current platforms that are being built on top of our Network, whether they are internal projects ([Core app](https://streamr.network/learn/core), [Marketplace](https://streamr.network/learn/marketplace)) or external ones from our community members like [Swash](https://forum.streamr.dev/d/39-browser-extension-swash), [myDiem](https://forum.streamr.dev/d/51-mydiem-create-a-mobile-app-to-gather-usage-metrics-across-apps) and many other developers pushing data through our Network.  #8203; [New Streamr Public Website](https://preview.redd.it/k6caiuf3vcq31.png?width=1881 amp;format=png amp;auto=webp amp;s=0d8e4a0db54d648ae754d7c4ae74ad0af8c9d0eb) During the month of September, we also released a completely redesigned new [public website](http://streamr.network/). Huge credit must go to our design team and developers who worked flat out on this release, which incorporated rich assets, with concise and impactful messaging, coupled with a smooth user experience. I #8217;d really recommend checking out the new  #8216;learn #8217; pages for the [Network](https://streamr.network/learn/network), [Marketplace](https://streamr.network/learn/marketplace) and [Core](https://streamr.network/learn/core). The timing couldn #8217;t have been more perfect, with the launch of Corea Network coming soon and presentation of the Marketplace based Community Products at [MozFest](https://www.mozillafestival.org/en/) toward the end of October. Shifting to the community development front, we have kicked-off the initial stage of [Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) integration with Swash team! We have been developing internal documentation to assist them in this effort and refining the Community Product creation flow as we receive new feedback. Very soon, myDiem will also start this initial pilot integration. Of course, the good news doesn #8217;t end here. In the last month, we have been in touch with few more developers and projects that are planning to integrate with our technology stacks! Our ecosystem keeps growing :) If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community-run [dev forum here](http://forum.streamr.dev/). As always, thanks for reading. Here #8217;s our regular list of updates: # Network * Setting up mainnet for for the official launch of Corea * Putting the new network components in dev environment (broker  amp; tracker) * Debugging JavaScript protocol problems * Fixed authentication for public streams * Fixed insecure random number generator that was being used in Engine and Editor * Adding TLS connections between nodes to allow for encrypted data transfer * Found a way to reduce CPU usage by switching websocket library, which could lead to 4 #8211;5x improvement in throughput * Add Community Products support to JavaScript SDK library * Improving Java library reconnection functionality * Working on putting Corea network in streamr-docker-dev # Community Products * Helped Swash team to get started with Community Products integration: deployed smart contract, set up Marketplace product * End-to-end tests debugging, watcher debugging, staging was updated * Kicking off Community Products integration with myDiem mobile app (Gang and team) * Working with Community Products stats, mainly with members graph. Hardest is to get correct test data. Will just use mock data for testing it for now * Working on products publishing flow * Working on community stats view for easy access to metrics * Testing the Community Products server deployment in our various environments * Finished work on product deploy refactoring. Added deploy spinner animation * Updated Community Products to use the new Monoplasma constructor with adminFeeFraction # Core app (Engine, Editor, Public Website) * New public website officially launched, in conjunction to new domain migration from Streamr.com - gt; Streamr.network * Completing zooming  amp; panning improvements for Editor * Fixed a couple of bugs with code editor, input connections vanishing on update * Ethereum watcher was fixed but a problem still exists to be fixed * Restructured developer docs sections according to Matt #8217;s designs, halfway through that. Completing navigation and migrate new screenshot images * Debugging and fixing WS reconnection issue that was killing GetEvents in production and did some minor Ethereum module fixes * First series of video explainers completed to improve new users on-boarding and improve conversion # Labs * [Integrating Streamr with Apache Kafka using Kafka Connect](https://medium.com/streamrblog/integrating-streamr-with-apache-kafka-using-kafka-connect-6ffb8cb7ecd6) See the original post from Weilei Yu on [Medium](https://medium.com/streamrblog/dev-update-september-2019-8d1fc236ba83).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Join Shiv Malik, Renate Samson, Valentina Pavel, and chair Naomi Colvin in the RSA Great Hall in London to tackle the question, should we sell our data? 24/10/19 18:30 - 20:00 don #8217;t miss this!</TITLE>
    <DATE>2019-10-07 14:11:36</DATE>
    <TEXT># Should we sell our data? [RSA Great Hall](https://preview.redd.it/s374dwwqm4r31.jpg?width=600 amp;format=pjpg amp;auto=webp amp;s=c999961b86212810d94538a42c6f4ae409e8f330) Digital Ethics is already tasked with answering some of the most important questions of the 21st Century. Freedom of speech, privacy, equality and even democracy itself are imperilled because of huge technological developments that the internet has enabled. Now a new question is rising up the ranks of digital ethics: if huge multinational tech corporations can sell our data why can #8217;t we do it ourselves? There are those who believe that in the interests of privacy, people should withhold all information in order to bring an end to the Silicon valley Panopticon. Others believe data should be open and free to all, and that to put a price on it is to destroy our dignity as human beings. Yet others think that claiming data as property rights and commercialising it for individuals is the only way to rebalance power between big tech and powerless individuals. So who is right? Mozfest Panel - Thursday 24th OCt @the RSA - 18:30 - 20:00. Panel line-up: Carl Miller - Research Director, Centre for the Analysis of Social Media, Demos Shiv Malik - Streamr Renate Samson - Open Data Institute Valentina Pavel - Privacy International, Mozilla Fellow Chair: Naomi Colvin - Blueprint for Free Speech Register here: [https://ti.to/streamr/should-we-sell-our-data](https://ti.to/streamr/should-we-sell-our-data) Find out more! [https://streamr.network](https://streamr.network/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-07 14:12:15</DATE>
    <TEXT>Yes the team is in contact with him :)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea - the biggest upgrade to the Streamr Network so far - is now live!  #127881; Read more about the milestone.</TITLE>
    <DATE>2019-10-08 08:33:05</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-08 08:39:52</DATE>
    <TEXT>There are always no-shows at events so there might be some free spots. However, officially we are sold out and we cannot guarantee you a place.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>All hands on deck to celebrate the launch of the Corea Network upgrade at devcon5. The Streamr pier-to-pier boat party and panel discussion sets sail around Osaka in one hour</TITLE>
    <DATE>2019-10-08 09:17:27</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-08 14:41:33</DATE>
    <TEXT>Link to the best pics: [https://twitter.com/streamr/status/1181578694298607616](https://twitter.com/streamr/status/1181578694298607616)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Upcoming webinar - Join Lead Developer Eric Andrews for an introduction to the latest version of the Streamr Network, Corea. Learn how to build a p2p network for real-time messaging and ask any questions about Corea in the follow-up Q amp;A</TITLE>
    <DATE>2019-10-09 13:47:31</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Do you work with real-time data? Learn how to use Streamr #8217;s Ethereum modules to create a stock ticker Oracle, listen  amp; get calls from smart contracts,  amp; compile  amp; deploy solidity code all with simple  #8216;drag  amp; drop #8217; programming</TITLE>
    <DATE>2019-10-10 10:41:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Do you work with real-time data? Learn how to use Streamr #8217;s Ethereum modules to create a stock ticker Oracle, listen  amp; get calls from smart contracts,  amp; compile  amp; deploy solidity code all with simple  #8216;drag  amp; drop #8217; programming</TITLE>
    <DATE>2019-10-10 10:42:18</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Do you work with real-time data? Learn how to use Streamr #8217;s Ethereum modules to create a stock ticker Oracle, listen  amp; get calls from smart contracts,  amp; compile  amp; deploy solidity code all with simple  #8216;drag  amp; drop #8217; programming</TITLE>
    <DATE>2019-10-11 04:49:35</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>If you're in London on the 24/10/19 and care about how our information is used and traded then this panel discussion on the implications and future of data monetisation is for you! Book your place!</TITLE>
    <DATE>2019-10-11 11:58:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-11 11:58:28</DATE>
    <TEXT>Register here: [https://ti.to/streamr/should-we-sell-our-data](https://ti.to/streamr/should-we-sell-our-data)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #8220;Once data is as open and permissionless for machines as the World Wide Web is for us humans, we can turn the Internet of Things from a dream into a reality." - Henri Pihkala. Learn more about the tools being built to bring this to life!</TITLE>
    <DATE>2019-10-14 14:01:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>How to unlock the value of data through enterprise adoption of the open source Streamr stack: introducing TX</TITLE>
    <DATE>2019-10-15 17:31:55</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Should you sell your personal data? If the tech giants can, why can't you? Join an evening of discussions to unpack the challenges and possible solutions to this issue at the RSA London. Get your ticket!</TITLE>
    <DATE>2019-10-16 15:43:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea - the biggest upgrade to the Streamr Network so far - is now live!  #127881; Read more about the milestone.</TITLE>
    <DATE>2019-10-17 08:48:46</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea - the biggest upgrade to the Streamr Network so far - is now live!  #127881; Read more about the milestone.</TITLE>
    <DATE>2019-10-17 09:03:06</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to create email alerts on Streamr Core based on real-time event triggers in a few minutes. Never miss changes in your real-time data!</TITLE>
    <DATE>2019-10-17 14:25:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Learn how to create email alerts on Streamr Core based on real-time event triggers in a few minutes. Never miss changes in your real-time data!</TITLE>
    <DATE>2019-10-17 14:26:11</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Last week, overlooking Osaka bay, Japan, we launched the next generation of the Streamr p2p pub/sub messaging Network. Take a look at the highlights from the event</TITLE>
    <DATE>2019-10-18 11:42:39</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr's community hero this month is  #129345; ... Jos #233; Camacho for his demonstration of live Lisbon Carris buss positions using the Core app. Great to see Core users bring data to life!</TITLE>
    <DATE>2019-10-18 14:18:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-18 14:18:49</DATE>
    <TEXT>Take a look at his demo: [https://twitter.com/josemnbcamacho/status/1183115880084975617](https://twitter.com/josemnbcamacho/status/1183115880084975617)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Today at 14:00 CEST, Lead Developer Eric Andrews will give a webinar introduction to the latest version of the Streamr Network, Corea. Join us and learn how to build a p2p network for real-time messaging!</TITLE>
    <DATE>2019-10-21 10:25:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Today at 14:00 CEST, Lead Developer Eric Andrews will give a webinar introduction to the latest version of the Streamr Network, Corea. Join us and learn how to build a p2p network for real-time messaging!</TITLE>
    <DATE>2019-10-21 10:25:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Dive into the potent issues surrounding trustless messaging and privacy for web 3 in this panel, recorded live at Streamr's p2p boat party in Osaka at Devcon</TITLE>
    <DATE>2019-10-22 14:17:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Dive into the potent issues surrounding trustless messaging and privacy for web 3 in this panel, recorded live at Streamr's p2p boat party in Osaka at Devcon</TITLE>
    <DATE>2019-10-22 14:19:19</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>This Saturday, Streamr will hold the first demonstration of data unions, with a community built plugin for users to crowdsell their own browser data at MozFest. Don #8217;t miss this if you #8217;re interested in the personal data economy!</TITLE>
    <DATE>2019-10-24 12:14:34</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-24 12:14:56</DATE>
    <TEXT>About the plugin: [https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-bro](https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-bro)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Where to find Streamr tomorrow at Mozfest. Don't miss the first demo of a Data Union using the Swash plugin!</TITLE>
    <DATE>2019-10-25 13:12:05</DATE>
    <TEXT>Tomorrow Streamr is at MozFest. We kick off with a panel on how to build data unions and how they'll shape the future of information, with Henri Pihkala, Marlene Ronstedt, Alex Craven, and Jason Lin. Then ... https://preview.redd.it/j9x3k5ohsou31.jpg?width=1920 amp;format=pjpg amp;auto=webp amp;s=3385c5ff361983d98543422a7ab620780dd14dfb Come and join the world's first data union! Henri Pihkala will use the community-built Swash plugin to show how individual can aggregate and trade their browsing data with millions of others in a data union whilst maintaining their privacy. https://preview.redd.it/s9ay3qbjsou31.jpg?width=1920 amp;format=pjpg amp;auto=webp amp;s=c249d529c8f838d90968641de180197646ff16d1 Finally, the team will be on hand for the weekend on level 4 to demonstrate data unions up close and also discuss our p2p trustless Network for realtime data transmission. Look out for the Streamr booth and see you there!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The community-built Swash App is available now for Firefox users to monetise their data. Install it in less than 30 seconds and start earning tokens as you browse the web!</TITLE>
    <DATE>2019-10-29 14:10:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-10-29 14:10:29</DATE>
    <TEXT>Try it here: [https://swashapp.io](https://swashapp.io)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The community-built Swash App is available now for Firefox users to monetise their data. Install it in less than 30 seconds and start earning tokens as you browse the web!</TITLE>
    <DATE>2019-10-29 17:13:23</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>It #8217;s time to find two new admins to manage the Streamr Community Fund. Nominate yourself in this thread before voting begins next week!</TITLE>
    <DATE>2019-10-29 17:17:47</DATE>
    <TEXT> #8203; https://preview.redd.it/l4wrk8jrkiv31.jpg?width=1280 amp;format=pjpg amp;auto=webp amp;s=4683fdb4b730680981a9474eeac6d0ac9c52c562 The recent demo of the [Swash Plugin](https://youtu.be/R92xw-vq100) for Firefox, a project funded by the [Community Fund](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674), is a timely reminder of the value offered by the Streamr dev community and the first (we hope) of many successful projects built to harness Streamr. You can nominate yourself to become a community administrator here. Just comment on this thread and say why you #8217;d make a good candidate, share any ideas for growing the project, and add your Telegram handle. At the start of next week, we #8217;ll add the eight candidates with the most thumbs up to a polling bot and share it on the Streamr Telegram for four days of voting. Only candidates and votes from those currently in the Telegram group as of today will be counted on the polling bot. On the 8th November, the two candidates with the most votes are chosen as fund administrators for the next six months. Elections will then take place again every three months as the other two admins' terms ends. Successful candidates will join Remy and Pocketchange. Thanks to our two previous admins Viktor and John Doe! Good luck and don't forget to vote for your favourite!</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE> #8234;Meet the team behind the Swash app plugin and learn about their motivations and ambitions for the world #8217;s first Data Union. Install Swash now to passively monetise your Firefox browser data!  #8236;</TITLE>
    <DATE>2019-10-31 16:35:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Should we sell our data? What are the implications and possibilities of monetising your personal information? Learn more in this heated debate hosted by Streamr!</TITLE>
    <DATE>2019-11-01 16:29:52</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Should we sell our data? What are the implications and possibilities of monetising your personal information? Learn more in this heated debate hosted by Streamr!</TITLE>
    <DATE>2019-11-01 16:30:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr Core has a built-in canvas sharing mechanism. Learn from  #8234;Weilei how to post to social media and open your live data insights to the world!</TITLE>
    <DATE>2019-11-04 16:32:54</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-05 09:41:34</DATE>
    <TEXT>The Swash team plan to open source the code on release. You can read more here https://forum.streamr.dev/d/39-browser-extension-swash</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>TX is working with enterprises to unlock the value of real-time data through the adoption of the Streamr open source stack. Read more!</TITLE>
    <DATE>2019-11-06 14:43:12</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea Network deployed, Swash team release their plugin, and Data Unions publicly demoed at MozFest. Read the latest Streamr developments from Weilei in October #8217;s dev update!</TITLE>
    <DATE>2019-11-07 14:09:18</DATE>
    <TEXT># Dev Update October 2019 *Tl;dr  #8212; Corea Network launch, Swash public release and Data Union* [Streamr at MozFest in London](https://preview.redd.it/ewf1g4n6u9x31.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=3cba71834fa02514c02a47754fd888f0b3f47d26) Welcome to [Streamr #8217;s](https://streamr.network/) core dev team update for October 2019. Let #8217;s nickname October  #8220;The release month #8221;. If you haven #8217;t been following Streamr #8217;s recent updates and are wondering why, below are some of the major events that happened in the past 30 days. The Streamr project: * Deployed our p2p Corea Network on mainnet ahead of schedule during [Devcon 5 in Osaka](https://twitter.com/streamr/status/1181578694298607616?s=20), on a boat with more than 100 attendees! * Completed our initial version of Community Products in order to integrate it with [early pilots](https://forum.streamr.dev/t/active-projects) building on top of Streamr ecosystem * Created the first-ever public [Data Union](https://twitter.com/streamr/status/1188030414067245062?s=20) pilot with [Swash](https://swashapp.io/) app and launched it to great reception at MozFest in London, hosted by The Mozilla Foundation.  #8203; [Wei and the Firefox](https://preview.redd.it/529wcudau9x31.jpg?width=3264 amp;format=pjpg amp;auto=webp amp;s=277110f2fcd7cbc4092a2f8d25323b8f27d662fe) Apparently, we were not the only ones feeling inspired by all of this, as you can see from the illustration below created by one of our MozFest pilot attendees, artist [Maggie Appleton](https://twitter.com/Mappletons/status/1188397445102931968?s=20): [Streamr Data Union illustration by Maggie Appleton](https://preview.redd.it/7vjzwhafu9x31.jpg?width=2000 amp;format=pjpg amp;auto=webp amp;s=bc3288a82d20adb04f03489e35a18f1a8cf2e671) If you are interested in learning more about the design process, architecture trade-offs and stress testing that went behind the Network development, take a look at [this comprehensive post](https://medium.com/streamrblog/test-a-tesnet-scalability-decentralization-latency-5f6ed2162bfe) by Henri. Our network core developer Eric Andrews also hosted a [webinar](https://youtu.be/iw1YkIsR908?t=77), recently, walking through the Network code repository and demoing how you could deploy it on your local machine, have multiple Broker nodes talking to each other as p2p architecture and test data exchanges between them. Regarding Swash integration and the Data Union movement, everything is progressing quite well. We were pleasantly surprised to receive very positive feedback from MozFest participants, even though we were prepared to face a very privacy-centric crowd. This could be in part thanks to the privacy control embedded by default into Swash #8217;s design. Now that it has both [Firefox](https://addons.mozilla.org/en-US/firefox/addon/swash/) and [Chrome](https://chrome.google.com/webstore/detail/swash/cmndjbecilbocjfkibfbifhngkdmjgog) plugins publicly available at their respective add-on marketplaces, we are ready to take on the next phases of adoption strategy and Network scalability testing. [Henri presenting the Data Union at MozFest](https://preview.redd.it/rhq0088ou9x31.jpg?width=1024 amp;format=pjpg amp;auto=webp amp;s=81f92bb975721f87e71331d9e0a3e2ba3393d072) As parting words for October, the more conferences I attend the stronger I feel about the following points: * Real-world users (not just the crypto inner circle) are desperately looking for new ways to change the current data abuse spirals and assuage the feeling of powerlessness from a user #8217;s perspective * There is a large mass market out there that we have a tangible chance to tap into now with apps like Swash. The Data Union use case seems to resonate well with people * My biggest realisation listening to thought leaders and policymakers on data commons, collective coordination, and creating a new data ecosystem at both MozFest (Firefox) and Decode (European Commission), is that we at Streamr, and all projects in our ecosystem, have the power to do something and make changes happen. It #8217;s not just abstract ideas anymore. It #8217;s game on! *If you #8217;re a dev interested in the Streamr stack or have some integration ideas, you can join our community-run* [*dev forum here*](http://forum.streamr.dev/)*.* As always, thanks for reading. Here #8217;s our regular list of updates: # Network * Official release of Corea Network * Public release of Network and Broker repositories on Github * Adding SSL connections between Broker nodes * Held webinar on Network introduction and local deployment demo * Fixed authentication bug and resend bug in JS client # Community Products * Working on Community Products stats feature * Fixing Community Products app secrets * Writing tests for Javascript library with CPS (Community Product Server) * Completing initial pilot integration of CPS with Swash * GetEvents module and disconnection issues fix # Core app (Engine, Editor, Public Website) * Backend bug fixed and testing Google gson library * Finishing up the zooming and panning for Editor * Completed desktop and mobile docs navigation refactors. Categorised module docs to their own pages * Modified marketplace contract to add updateProduct * Implemented docs review and technical review. Read the [original post](https://medium.com/streamrblog/dev-update-october-2019-8fdba861a61b) from Wei.</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Corea Network deployed, Swash team release their plugin, and Data Unions publicly demoed at MozFest. Read the latest Streamr developments from Weilei in October #8217;s dev update!</TITLE>
    <DATE>2019-11-07 14:10:09</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Great to see Streamr rise into the top 3 most popular Ethereum tool in the last 7 days, according to Dapp.com. The Streamr stack provides real-time data infrastructure for Web3</TITLE>
    <DATE>2019-11-08 13:00:02</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>TX and Streamr are exploring a new partnership with WWF-Philippines and Union Bank to develop an application that will incentivise fisherfolk to share verified traceability and trade data</TITLE>
    <DATE>2019-11-11 10:32:17</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>TX and Streamr are exploring a new partnership with WWF-Philippines and Union Bank to develop an application that will incentivise fisherfolk to share verified traceability and trade data</TITLE>
    <DATE>2019-11-11 10:48:58</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The world #8217;s first functioning Data Union is now live from the Swash team. But what are Data Unions, why is Streamr building them,  amp; how could apps  amp; devices use them? See the demo recorded at Mozfest 2019 https://youtu.be/_yo6iTH1FE0</TITLE>
    <DATE>2019-11-12 16:30:15</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-13 12:01:46</DATE>
    <TEXT>I #8217;m not aware of the issue, you can discuss with the Swash team directly in their telegram group t.me/swashapp_group</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>At the heart of the Streamr stack and goal is a powerful p2p messaging Network for real-time data. We think this is a crucial protocol for the success of web3/the decentralized web.</TITLE>
    <DATE>2019-11-13 19:41:25</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-13 19:41:37</DATE>
    <TEXT>Learn more! [https://streamr.network/learn/network](https://streamr.network/learn/network)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build your own real-time data stream using the Core app for fast prototyping and insights. Watch the tutorial from Matthew Fontana!</TITLE>
    <DATE>2019-11-14 16:12:51</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Build your own real-time data stream using the Core app for fast prototyping and insights. Watch the tutorial from Matthew Fontana!</TITLE>
    <DATE>2019-11-14 16:13:43</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>The Streamr community hero for this month is ...  #129345; Maggie Appleton. Thank you for this wonderful illustration outlining Data Unions and glad you liked the demo at Mozfest!</TITLE>
    <DATE>2019-11-15 14:04:07</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-15 14:04:15</DATE>
    <TEXT>Check out some of Maggie's other tech illustrations: [http://maggieappleton.com/](http://maggieappleton.com/)</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Meet the team: Miroslav Pokrovskii decided to join Streamr because building a decentralized real-time data Network was such a huge technical challenge.  #128253; Look out for one of the greatest ever shots in this series!  #128330;</TITLE>
    <DATE>2019-11-18 16:11:42</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>TX is building the future of health data monetisation using the Streamr tech stack</TITLE>
    <DATE>2019-11-20 14:08:50</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>What will it take to launch a data economy? Last week Streamr were invited to participate in a EU-level process as thought leaders in the data monetization space.</TITLE>
    <DATE>2019-11-21 14:32:39</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-22 08:37:15</DATE>
    <TEXT>I believe this is a display glitch. Swash team are working on it. Please keep in mind this is still an early release. You can reach them here t.me/swashapp_group</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>What are Data Unions and how can they help us reclaim ownership of our data? Shiv Malik presents the exciting progress from Streamr and the dev community! Stream from RadicalxChange</TITLE>
    <DATE>2019-11-22 14:25:22</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE></TITLE>
    <DATE>2019-11-25 12:27:48</DATE>
    <TEXT>Hey thanks for the great questions, i would take a look at these two talks that touch on the economics/rights regarding individual data monetisation. \- [https://www.youtube.com/watch?v=ewyql\_W0HfE amp;feature=youtu.be amp;t=2330](https://www.youtube.com/watch?v=ewyql_W0HfE amp;feature=youtu.be amp;t=2330) \- [https://youtu.be/6ni9K0JjI-k](https://youtu.be/6ni9K0JjI-k) This from the [Swash app](https://swashapp.io/#/) itself may also prove useful: "A comparison between the way Swash collects data and the way giant data collectors collect data, shows some major differences: Firstly, unlike giant data collectors, Swash asks the user to approve every data it would send to Streamr. Then, by using Swash, the user gets paid for every data he provides to sell, but giant data collectors never share revenue of selling data. Next, Swash never reveals user identity unless the user gives permission, but user identity is known for the giant data collectors. Moreover, Swash is not limited to data regarding a specific business and it can collects a combination of user's data that none of these companies have all these data together. As Swash is installed on the user side, it can collect some other interesting data that giant data collectors do not have access."</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Streamr in a nutshell: For those who #8217;ve just joined us, check out this overview of the tech and project</TITLE>
    <DATE>2019-11-26 12:05:34</DATE>
    <TEXT># New to Streamr? Welcome! Here #8217;s a quick guide to get you started https://preview.redd.it/wsyqtbjqs0141.png?width=1600 amp;format=png amp;auto=webp amp;s=412eacbb4c702534642aa5f44701ca4fa588cda0 # Contents: **What is Streamr?** **The Streamr Tech Stack** **A Brief History of Streamr** **Where to Find** **Developer resources** **Join the Community** **Reading list** # What is Streamr? [Streamr](https://streamr.network/) is an open-source project creating a platform and tools for the world #8217;s real-time data to be owned and traded by those who produce it. [Streamr explained in 2 mins](https://www.youtube.com/watch?v=nbWIPVE8zFE) Streamr will do this by building a p2p Network for transporting real-time data around the globe. On top of this sits a data Marketplace, Data Unions for aggregated trading of data produced by individuals, and the Core app with other tools for people, organisations, and machines to process and distribute data with ease. # The Streamr Tech Stack ## The Network Real-time data will travel through a decentralized pub/sub [Network](https://streamr.network/learn/network) hosted on computers around the globe. The Network is a scalable, low-latency transport layer, built for dApps, smart devices such as IoT sensors or connected cars, and data streams on the Marketplace. It will be supported by nodes who earn DATA tokens in exchange for the bandwidth and validation they provide. We think this is a [crucial protocol for Web3](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca). The Network is currently at its [Corea milestone](https://medium.com/streamrblog/test-a-tesnet-scalability-decentralization-latency-5f6ed2162bfe), which launched at Devcon 5 this year. Watch the [launch highlights](https://www.youtube.com/watch?v=F3lxhbTUpL0)!  #8203; [Nodes on the Streamr Network](https://preview.redd.it/4pc07hu0t0141.png?width=3200 amp;format=png amp;auto=webp amp;s=aa261506d91b873af1f0c8c752d8936d338ab8fa) ## Engine Processing raw data needs an Engine for it to be aggregated, filtered, or combined with other data to extract its value. The Engine is built to deal with high volumes of information and accept data from a wide variety of sources including financial exchanges, IoT devices, and social media.  #8203; [Streamr Engine](https://preview.redd.it/x6gcdlf3t0141.png?width=1400 amp;format=png amp;auto=webp amp;s=9744fa61c8d9aae274aa67249a99b09c6fe97a2e) ## Streamr Core [Streamr Core](https://medium.com/streamrblog/streamr-core-app-smart-contract-integration-real-time-data-visualizations-9180e343c7a) is a web based interactive platform to create or subscribe to data streams and visualise, analyse, and easily prototype real-time data without the need to set up back end infrastructure. Users can also combine data streams, and connect real-world events to trigger smart contracts. Core uses [web3 sign-in, payment, and identity management processes](https://medium.com/streamrblog/dao-data-economy-ethereum-oracles-39e1e3ab160c). Try the [app today](https://streamr.network/learn/core)!  #8203; [Connected modules running on a canvas](https://preview.redd.it/63pj0aw5t0141.png?width=3200 amp;format=png amp;auto=webp amp;s=7d3d85a43ec6bcee2a8194c65bb804f400d6b641) ## The Marketplace The [Marketplace](https://streamr.network/learn/marketplace) is a web-based application to trade real-time data streams. Providers publish streams and consumers subscribe and pay for access. Terms of use, the price schedule, and time-based access control are coded in Ethereum smart contracts.  #8203; [Streamr Marketplace](https://preview.redd.it/2odsxrz7t0141.png?width=3200 amp;format=png amp;auto=webp amp;s=0f6fceda0575f9f1b5e41b2c6eb4ac2e32f8da11) ## Community Products A [Community Product](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) or Data Union is a new type of product that sits within the Marketplace. It is an aggregated firehose of real-time data sent by individuals permissioning and pushing the data they generate into a single Data Union for purchase. Revenue sharing mechanics (see [Monoplasma](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa)) means that when these products sell, all contributors automatically receive payment. The first example is the [Swash plugin](https://swashapp.io/), built by community devs based in Turkey, for users to monetise [their browser data](https://medium.com/streamrblog/join-a-data-union-with-the-surf-streamr-browser-plugin-d9050d2d9332). Try the beta now and start earning in seconds! [Learn about the motivations and ambitions for the Swash app, the first functioning Data Union, from its creator](https://www.youtube.com/watch?v=R92xw-vq100) ## Monoplasma [Monoplasma](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa) is a special-purpose off-chain scaling solution for one-to-many payments. It was originally created to enable the Community Products feature on the Streamr Marketplace. It is now [openly licensed for anyone to utilise](https://github.com/streamr-dev/monoplasma).  #8203; [Monoplasma scaling solution for one-to-many payments](https://preview.redd.it/qjtup5hdt0141.png?width=1600 amp;format=png amp;auto=webp amp;s=eaccadb66b994ce05c7953b2179971fb5be440ea) # A Brief History of Streamr Streamr was started because most services on the internet, and particularly on the web, are run by tech giants like Amazon, Microsoft, Google, and Facebook. The project founders believe that creating decentralized infrastructure and a decentralized data economy are the only solutions to bring back a healthier diversity of power and money. Streamr #8217;s role and purpose in this big picture is to build the technology that covers the real-time data niche of this decentralized infrastructure, especially for the purposes of the decentralized web and internet of things. Streamr is following a predefined roadmap as set out in the 2017 [whitepaper](https://www.streamr.com/whitepaper). This process will take four-to-five years to complete. The Streamr approach is incremental decentralization: starting from a centralized but feature-complete system, and building new versions with less and less centralization over time. Founding [team members](https://streamr.network/about/) come from a financial backgrounds, being either quants, trading system developers, algorithmic traders and, in some cases, all of the above. Streamr #8217;s devs have a variety of backgrounds relating to networking, infrastructure, data systems and crypto experience. Check out the [meet the team](https://www.youtube.com/watch?v=VcBi-BKLTr0 amp;list=PLz4GDOpDozwUxATM8yWHLt5uDO953nyZ2) videos! While developing the platform, the team engages with developers and companies to explore use cases and promote adoption of the technology. There is a regularly updated [public Trello](https://trello.com/b/j24hxvjg/streamr-milestone-1) to follow the progress made towards each milestone. In October 2019, Streamr launched [TX](https://medium.com/streamrblog/launching-tx-the-spearhead-of-enterprise-adoption-of-streamr-technology-1fb857cb0ee0), a newly established company tasked with enterprise level consultancy for adoption of the Streamr tech stack.  #8203; [The Streamr Core team just before the start of their Network launch pier-to-pier boat party at DevCon5](https://preview.redd.it/c7er6hbgt0141.jpg?width=4000 amp;format=pjpg amp;auto=webp amp;s=5ebbfa1181dfe69588a1d701127c0bea57eae740) # Where can you find Streamr? Streamr HQ is in Zug, Switzerland and the TX team is based in Helsinki, Finland. Many other contributors work remotely across the globe. You can follow the project and discussions on: [Reddit](https://www.reddit.com/r/streamr/), [Twitter](https://twitter.com/streamr), [Medium](https://blog.streamr.com/), [YouTube](https://www.youtube.com/channel/UCGWEA61RueG-9DV53s-ZyJQ), [LinkedIn](https://www.linkedin.com/company/4002029/), [Peepeth](https://peepeth.com/streamr), [Blockfolio Signal](https://blockfolio.com/signal/). The best place to interact with the team and other community members directly is on [Telegram](https://t.me/streamrdata) or [Reddit](https://www.reddit.com/r/streamr/). The team post regular updates and there #8217;s usually someone there to answer any questions. As well as [English](https://t.me/streamrdata), there are currently dedicated community moderated Telegram groups for [German](https://t.me/streamrdeutsch), [Chinese](https://t.me/Data_StreamrCN), [Spanish](https://t.me/Data_StreamrCN), [Russian](https://t.me/streamrdatarus), and [Portuguese](https://t.me/StreamrBrasil) languages. There is also a minimal [Telegram announcements group](https://t.me/streamrofficial) and an unofficial [channel](https://t.me/streamrmoontalk) run by the community.  #8203; [Streamr #8217;s official and unofficial Telegram Channels](https://preview.redd.it/4jf1ofikt0141.png?width=4000 amp;format=png amp;auto=webp amp;s=6eeb909f1865d9221fb59694a13934706a80d166) # Developer Resources The [open source](https://medium.com/streamrblog/dev-update-november-2018-open-sourcing-the-marketplace-168182262ff7) nature of the project means a lot of resources are available for the Streamr developer community. The [Dev Community Forum](https://forum.streamr.dev/) is a good place to start to see some of the [integrations](https://forum.streamr.dev/t/integrations) being built, find the latest [bounty proposals](https://forum.streamr.dev/t/projects-proposal), [discussions](https://forum.streamr.dev/t/general), or propose a project and get funded to build it by the [Community Fund](https://forum.streamr.dev/t/community-fund). The updated [Streamr dev docs](https://streamr.network/docs/introduction) on the main site contain most of the information to get started. Streamr [Github](https://github.com/streamr-dev), [Javascript Streamr Client](https://github.com/streamr-dev/streamr-client), newly released MVP [Java Streamr Client](https://github.com/streamr-dev/streamr-client-java), the [API explorer](https://medium.com/streamrblog/the-new-api-explorer-for-streamr-developers-e6af28c2c808), and monthly [dev updates](https://medium.com/streamrblog/tech/home) are other useful resources. [Demo of the new API explorer](https://www.youtube.com/watch?v=5D6DPUPDPMM) ## Tutorials There are many [tutorials](https://medium.com/streamrblog/tutorial/home) saved to the Streamr Medium page, including how to: * [Connect your data to the Streamr Marketplace](https://medium.com/streamrblog/how-to-connect-data-to-streamr-in-5-minutes-1-of-3-9363afd254e6) * [Build your own smart fridge](https://medium.com/streamrblog/streamr-build-smart-fridge-iot-ruuvitag-node-red-ibm-cloud-fridge-d834bb6dc779) * [Integrate Streamr with Apache Spark](https://medium.com/streamrblog/integrating-streamr-with-apache-spark-4c049f2688a1) * [Build air quality based smart contract apps](https://medium.com/streamrblog/air-quality-data-streams-from-10-210-locations-in-68-countries-available-via-streamrs-platform-981d52b0ecb3) * [Deploy Monoplasma for one-to-many payments](https://medium.com/streamrblog/monoplasma-revenue-share-dapps-off-chain-6cb7ee8b42fa) And there is a whole [YouTube playlist](https://www.youtube.com/watch?v=EbIR-bLQs8A amp;list=PLz4GDOpDozwXrXUsBI6il_qA9OGGQxt2A) that covers the basics of using the Streamr stack. ## Project Examples For inspiration, here are some of the recent projects built by the Streamr dev community: * The [Swash app](https://swashapp.io/) for Firefox, Chrome, and Microsoft Edge users to monetize their browser data * Consolidated [air position streams from over 10,000 locations](https://medium.com/streamrblog/air-quality-data-streams-from-10-210-locations-in-68-countries-available-via-streamrs-platform-981d52b0ecb3) on the Marketplace * [Sell your Fitbit data](https://medium.com/streamrblog/personal-fitbit-data-sell-streamr-marketplace-blockchain-ethereum-3b32c215660c) * [Spotify](https://medium.com/streamrblog/crowdselling-through-a-data-union-at-radicalxchange-ec032289a51c) demo In advance of Community Products, Streamr would like to fund more real-time data stream integrations to the Marketplace. You can find or [submit a proposal](https://forum.streamr.dev/t/projects-proposal) directly or contact Head of Developer Outreach, Weilei Yu, on Telegram, the community [dev forum](http://forum.streamrdev.com/), or by email: [*weilei.yu@streamr.com*](mailto:weilei.yu@streamr.com)*.* [Wei showing a demo of the Streamr stack at Web3 Summit in Berlin 2018](https://preview.redd.it/vpjlcqxqt0141.jpg?width=2600 amp;format=pjpg amp;auto=webp amp;s=f7f42bb1a09b07276b05173c2dee0d8fd0bd72cc) # Join the Community The community is key for reaching the Streamr goal and there are a lot of ways to get involved. Here is a list of the [\#8 best ways you can engage](https://medium.com/streamrblog/8-ways-you-can-help-the-streamr-community-grow-19d1ab825daa). You can read more about Streamr from a community perspective and how to get involved in [this post](https://medium.com/streamrblog/streamr-all-hands-on-deck-440c3f3798ed) from Telegram memeber,  #8220;Pocketchange #8221;. ## Community Fund The [Community Fund ](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674)is backed by 2,000,000 in DATA tokens to help implement ideas with some resources at the community #8217;s disposal. Community Fund projects can be picked up and proposed on the [Community Dev Forum](https://forum.streamr.dev/t/community-fund). # Reading list [Whitepaper](https://www.streamr.com/whitepaper) [Explainer vid](https://youtu.be/nbWIPVE8zFE) [Website](https://www.streamr.com/) [Tutorials](https://medium.com/streamrblog/tutorial/home) [Tutorial videos for Core](https://www.youtube.com/watch?v=EbIR-bLQs8A amp;list=PLz4GDOpDozwXrXUsBI6il_qA9OGGQxt2A) [Video playlist](https://www.youtube.com/channel/UCGWEA61RueG-9DV53s-ZyJQ/playlists) [Building the Missing Protocol of Today #8217;s Internet Stack: a Decentralized Pub/Sub Network for Realtime Data](https://medium.com/streamrblog/building-the-missing-protocol-of-todays-internet-stack-a-decentralized-pub-sub-network-for-ad1f5972f3ca) [The Internet of Cars: Unlocking the Value of Vehicle Data](https://youtu.be/Xbouxnk8Yxc) [Unlocking the benefits of vehicle data: Solving data limitations in the transport industry](https://medium.com/streamrblog/unlocking-the-benefits-of-vehicle-data-solving-data-limitations-in-the-transport-industry-a3e7b7f78355) [Introducing Community Products](https://medium.com/streamrblog/community-products-crowdselling-big-data-iot-blockchain-streamr-fbaa794c7bc9) [How to Crowdsell Your Information Through a Data Union](https://medium.com/streamrblog/crowdselling-your-information-through-a-data-union-ec032289a51c) [The Streamr Community Fund!](https://medium.com/streamrblog/announcing-the-streamr-community-fund-you-take-the-wheel-86b0a74f8674) [Launching TX for Enterprise Adoption of the Streamr Stack](https://medium.com/streamrblog/launching-tx-the-spearhead-of-enterprise-adoption-of-streamr-technology-1fb857cb0ee0) Originally posted on [Medium](https://medium.com/streamrblog/streamr-quick-start-guide-for-beginners-b996a211c98a).</TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  <WRITING>
    <TITLE>Customise canvas modules with your own code to suit your real-time data requirements using Streamr Core. Watch the tutorial and try the app today!</TITLE>
    <DATE>2019-11-26 14:57:12</DATE>
    <TEXT></TEXT>
    <INFO>Reddit post</INFO>
  </WRITING>
  </INDIVIDUAL>